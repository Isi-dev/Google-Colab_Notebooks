{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **In-Context Edit for Editing Images with Prompts**\n",
        "- This notebook lets you use prompts to add objects, adjust color attributes, and apply styles to images. Changing backgrounds or removing objects has a relatively low success rate.\n",
        "- You can use the free T4 GPU to run this notebook, but it is much slower than others. A 512x912 image takes about 9 minutes to edit at 20 steps with the T4 GPU.\n",
        "- Make sure the width of your image is 512 pixels. There is no restriction on the height.\n",
        "- The denoise value can be reduced to 0.5 or less if your goal is to change the style of the image."
      ],
      "metadata": {
        "id": "8gNEkuc4oyv8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "F-3uUiopZ2hy"
      },
      "outputs": [],
      "source": [
        "# @title Setup Environment\n",
        "# !pip install --quiet torch torchvision --index-url https://download.pytorch.org/whl/cu124\n",
        "# !pip install --upgrade --quiet torch torchvision --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install --quiet torch torchvision\n",
        "%cd /content\n",
        "\n",
        "from IPython.display import clear_output\n",
        "!git clone https://github.com/Isi-dev/ComfyUI\n",
        "clear_output()\n",
        "%cd /content/ComfyUI/custom_nodes\n",
        "!git clone https://github.com/Isi-dev/ComfyUI_GGUF.git\n",
        "clear_output()\n",
        "# !git clone https://github.com/Isi-dev/ComfyUI_KJNodes.git\n",
        "# clear_output()\n",
        "%cd /content/ComfyUI/custom_nodes/ComfyUI_GGUF\n",
        "!pip install -r requirements.txt\n",
        "clear_output()\n",
        "# %cd /content/ComfyUI/custom_nodes/ComfyUI_KJNodes\n",
        "# !pip install -r requirements.txt\n",
        "# clear_output()\n",
        "%cd /content/ComfyUI\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_pip_packages():\n",
        "    packages = [\n",
        "        'torchsde',\n",
        "        'av',\n",
        "        'diffusers',\n",
        "        # 'transformers',\n",
        "        # 'xformers',\n",
        "        'accelerate',\n",
        "        # 'omegaconf',\n",
        "        # 'tqdm',\n",
        "        # 'librosa',\n",
        "        'einops',\n",
        "        'spandrel'\n",
        "    ]\n",
        "\n",
        "    for package in packages:\n",
        "        try:\n",
        "            # Run pip install silently (using -q)\n",
        "            subprocess.run(\n",
        "                [sys.executable, '-m', 'pip', 'install', '-q', package],\n",
        "                check=True,\n",
        "                capture_output=True\n",
        "            )\n",
        "            print(f\"✓ {package} installed\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"✗ Error installing {package}: {e.stderr.decode().strip() or 'Unknown error'}\")\n",
        "\n",
        "def install_apt_packages():\n",
        "    packages = ['aria2']\n",
        "\n",
        "    try:\n",
        "        # Run apt install silently (using -qq)\n",
        "        subprocess.run(\n",
        "            ['apt-get', '-y', 'install', '-qq'] + packages,\n",
        "            check=True,\n",
        "            capture_output=True\n",
        "        )\n",
        "        print(\"✓ apt packages installed\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"✗ Error installing apt packages: {e.stderr.decode().strip() or 'Unknown error'}\")\n",
        "\n",
        "\n",
        "print(\"Installing pip packages...\")\n",
        "install_pip_packages()\n",
        "clear_output()  # Clear the pip installation output\n",
        "\n",
        "print(\"Installing apt packages...\")\n",
        "install_apt_packages()\n",
        "clear_output()  # Clear the apt installation output\n",
        "\n",
        "print(\"Installation completed with status:\")\n",
        "print(\"- All pip packages installed successfully\" if '✗' not in install_pip_packages.__code__.co_consts else \"- Some pip packages had issues\")\n",
        "print(\"- apt packages installed successfully\" if '✗' not in install_apt_packages.__code__.co_consts else \"- apt packages had issues\")\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import gc\n",
        "import os\n",
        "import torch\n",
        "from PIL import Image\n",
        "import random\n",
        "import imageio\n",
        "from google.colab import files\n",
        "from IPython.display import display, HTML, Image as IPImage\n",
        "sys.path.insert(0, '/content/ComfyUI')\n",
        "\n",
        "from nodes import (\n",
        "    DualCLIPLoader,\n",
        "    CLIPLoader,\n",
        "    CLIPTextEncode,\n",
        "    VAEDecode,\n",
        "    VAELoader,\n",
        "    KSamplerAdvanced,\n",
        "    ConditioningZeroOut,\n",
        "    InpaintModelConditioning,\n",
        "    ImageScaleBy,\n",
        "    LoraLoaderModelOnly,\n",
        "    LoadImage,\n",
        "    SaveImage\n",
        ")\n",
        "\n",
        "from custom_nodes.ComfyUI_GGUF.nodes import UnetLoaderGGUF\n",
        "\n",
        "from comfy_extras.nodes_flux import FluxGuidance\n",
        "\n",
        "from comfy_extras.nodes_images import ImageCrop\n",
        "\n",
        "from comfy_extras.nodes_post_processing import ImageScaleToTotalPixels\n",
        "\n",
        "# from custom_nodes.ComfyUI_KJNodes.nodes.image_nodes import ImageResizeKJ\n",
        "\n",
        "from comfy_extras.nodes_mask import SolidMask\n",
        "\n",
        "def image_width_height(image):\n",
        "    if image.ndim == 4:\n",
        "        _, height, width, _ = image.shape\n",
        "    elif image.ndim == 3:\n",
        "        height, width, _ = image.shape\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported image shape: {image.shape}\")\n",
        "    return width, height\n",
        "\n",
        "def compute_value_from_size(width: int, height: int) -> float:\n",
        "    return float(width * height / 4)\n",
        "\n",
        "def fillMask(width, height, mask, box=(0, 0), color=0):\n",
        "    bg = Image.new(\"L\", (width, height), color)\n",
        "    bg.paste(mask, box, mask)\n",
        "    return bg\n",
        "\n",
        "def emptyImage(width, height, batch_size=1, color=0):\n",
        "    r = torch.full([batch_size, height, width, 1], ((color >> 16) & 0xFF) / 0xFF)\n",
        "    g = torch.full([batch_size, height, width, 1], ((color >> 8) & 0xFF) / 0xFF)\n",
        "    b = torch.full([batch_size, height, width, 1], ((color) & 0xFF) / 0xFF)\n",
        "    return torch.cat((r, g, b), dim=-1)\n",
        "\n",
        "def pil2tensor(image):\n",
        "  return torch.from_numpy(np.array(image).astype(np.float32) / 255.0).unsqueeze(0)\n",
        "\n",
        "def tensor2pil(image):\n",
        "    return Image.fromarray(np.clip(255. * image.cpu().numpy().squeeze(), 0, 255).astype(np.uint8))\n",
        "\n",
        "def lanczos(samples, width, height):\n",
        "    images = [Image.fromarray(np.clip(255. * image.movedim(0, -1).cpu().numpy(), 0, 255).astype(np.uint8)) for image in samples]\n",
        "    images = [image.resize((width, height), resample=Image.Resampling.LANCZOS) for image in images]\n",
        "    images = [torch.from_numpy(np.array(image).astype(np.float32) / 255.0).movedim(-1, 0) for image in images]\n",
        "    result = torch.stack(images)\n",
        "    return result.to(samples.device, samples.dtype)\n",
        "\n",
        "def bislerp(samples, width, height):\n",
        "    def slerp(b1, b2, r):\n",
        "        '''slerps batches b1, b2 according to ratio r, batches should be flat e.g. NxC'''\n",
        "\n",
        "        c = b1.shape[-1]\n",
        "\n",
        "        #norms\n",
        "        b1_norms = torch.norm(b1, dim=-1, keepdim=True)\n",
        "        b2_norms = torch.norm(b2, dim=-1, keepdim=True)\n",
        "\n",
        "        #normalize\n",
        "        b1_normalized = b1 / b1_norms\n",
        "        b2_normalized = b2 / b2_norms\n",
        "\n",
        "        #zero when norms are zero\n",
        "        b1_normalized[b1_norms.expand(-1,c) == 0.0] = 0.0\n",
        "        b2_normalized[b2_norms.expand(-1,c) == 0.0] = 0.0\n",
        "\n",
        "        #slerp\n",
        "        dot = (b1_normalized*b2_normalized).sum(1)\n",
        "        omega = torch.acos(dot)\n",
        "        so = torch.sin(omega)\n",
        "\n",
        "        #technically not mathematically correct, but more pleasing?\n",
        "        res = (torch.sin((1.0-r.squeeze(1))*omega)/so).unsqueeze(1)*b1_normalized + (torch.sin(r.squeeze(1)*omega)/so).unsqueeze(1) * b2_normalized\n",
        "        res *= (b1_norms * (1.0-r) + b2_norms * r).expand(-1,c)\n",
        "\n",
        "        #edge cases for same or polar opposites\n",
        "        res[dot > 1 - 1e-5] = b1[dot > 1 - 1e-5]\n",
        "        res[dot < 1e-5 - 1] = (b1 * (1.0-r) + b2 * r)[dot < 1e-5 - 1]\n",
        "        return res\n",
        "\n",
        "def common_upscale(samples, width, height, upscale_method, crop):\n",
        "        orig_shape = tuple(samples.shape)\n",
        "        if len(orig_shape) > 4:\n",
        "            samples = samples.reshape(samples.shape[0], samples.shape[1], -1, samples.shape[-2], samples.shape[-1])\n",
        "            samples = samples.movedim(2, 1)\n",
        "            samples = samples.reshape(-1, orig_shape[1], orig_shape[-2], orig_shape[-1])\n",
        "        if crop == \"center\":\n",
        "            old_width = samples.shape[-1]\n",
        "            old_height = samples.shape[-2]\n",
        "            old_aspect = old_width / old_height\n",
        "            new_aspect = width / height\n",
        "            x = 0\n",
        "            y = 0\n",
        "            if old_aspect > new_aspect:\n",
        "                x = round((old_width - old_width * (new_aspect / old_aspect)) / 2)\n",
        "            elif old_aspect < new_aspect:\n",
        "                y = round((old_height - old_height * (old_aspect / new_aspect)) / 2)\n",
        "            s = samples.narrow(-2, y, old_height - y * 2).narrow(-1, x, old_width - x * 2)\n",
        "        else:\n",
        "            s = samples\n",
        "\n",
        "        if upscale_method == \"bislerp\":\n",
        "            out = bislerp(s, width, height)\n",
        "        elif upscale_method == \"lanczos\":\n",
        "            out = lanczos(s, width, height)\n",
        "        else:\n",
        "            out = torch.nn.functional.interpolate(s, size=(height, width), mode=upscale_method)\n",
        "\n",
        "        if len(orig_shape) == 4:\n",
        "            return out\n",
        "\n",
        "        out = out.reshape((orig_shape[0], -1, orig_shape[1]) + (height, width))\n",
        "        return out.movedim(2, 1).reshape(orig_shape[:-2] + (height, width))\n",
        "\n",
        "def make(image_1, direction=\"left-right\", pixels=0, image_2=None, mask_1=None, mask_2=None):\n",
        "    if image_2 is None:\n",
        "      image_2 = emptyImage(image_1.shape[2], image_1.shape[1])\n",
        "      mask_2 = torch.full((1, image_1.shape[1], image_1.shape[2]), 1, dtype=torch.float32, device=\"cpu\")\n",
        "\n",
        "    elif image_2 is not None and mask_2 is None:\n",
        "      raise ValueError(\"mask_2 is required when image_2 is provided\")\n",
        "    if pixels > 0:\n",
        "      _, img2_h, img2_w, _ = image_2.shape\n",
        "      h = pixels if direction == 'left-right' else int(img2_h * (pixels / img2_w))\n",
        "      w = pixels if direction == 'top-bottom' else int(img2_w * (pixels / img2_h))\n",
        "\n",
        "      image_2 = image_2.movedim(-1, 1)\n",
        "      image_2 = common_upscale(image_2, w, h, 'bicubic', 'disabled')\n",
        "      image_2 = image_2.movedim(1, -1)\n",
        "\n",
        "      orig_image_2 = tensor2pil(image_2)\n",
        "      orig_mask_2 = tensor2pil(mask_2).convert('L')\n",
        "      orig_mask_2 = orig_mask_2.resize(orig_image_2.size)\n",
        "      mask_2 = pil2tensor(orig_mask_2)\n",
        "\n",
        "    _, img1_h, img1_w, _ = image_1.shape\n",
        "    _, img2_h, img2_w, _ = image_2.shape\n",
        "\n",
        "    image, mask, context_mask = None, None, None\n",
        "\n",
        "    # resize\n",
        "    if img1_h != img2_h and img1_w != img2_w:\n",
        "      width, height = img2_w, img2_h\n",
        "      if direction == 'left-right' and img1_h != img2_h:\n",
        "        scale_factor = img2_h / img1_h\n",
        "        width = round(img1_w * scale_factor)\n",
        "      elif direction == 'top-bottom' and img1_w != img2_w:\n",
        "        scale_factor = img2_w / img1_w\n",
        "        height = round(img1_h * scale_factor)\n",
        "\n",
        "      image_1 = image_1.movedim(-1, 1)\n",
        "      image_1 = common_upscale(image_1, width, height, 'bicubic', 'disabled')\n",
        "      image_1 = image_1.movedim(1, -1)\n",
        "\n",
        "    if mask_1 is None:\n",
        "      mask_1 = torch.full((1, image_1.shape[1], image_1.shape[2]), 0, dtype=torch.float32, device=\"cpu\")\n",
        "\n",
        "    orig_image_1 = tensor2pil(image_1)\n",
        "    orig_mask_1 = tensor2pil(mask_1).convert('L')\n",
        "\n",
        "    if orig_mask_1.size != orig_image_1.size:\n",
        "      orig_mask_1 = orig_mask_1.resize(orig_image_1.size)\n",
        "\n",
        "    img1_w, img1_h = orig_image_1.size\n",
        "    image_1 = pil2tensor(orig_image_1)\n",
        "    image = torch.cat((image_1, image_2), dim=2) if direction == 'left-right' else torch.cat((image_1, image_2),\n",
        "                                                                                             dim=1)\n",
        "\n",
        "    context_mask = fillMask(image.shape[2], image.shape[1], orig_mask_1)\n",
        "    context_mask = pil2tensor(context_mask)\n",
        "\n",
        "    orig_mask_2 = tensor2pil(mask_2).convert('L')\n",
        "    x = img1_w if direction == 'left-right' else 0\n",
        "    y = img1_h if direction == 'top-bottom' else 0\n",
        "    mask = fillMask(image.shape[2], image.shape[1], orig_mask_2, (x, y))\n",
        "    mask = pil2tensor(mask)\n",
        "\n",
        "    return (image, mask, context_mask, img2_w, img2_h, x, y)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "# download_loRA = False # @param {type:\"boolean\"}\n",
        "\n",
        "lora = None\n",
        "\n",
        "def download_with_aria2c(link, folder=\"/content/ComfyUI/models/loras\"):\n",
        "    import os\n",
        "\n",
        "    filename = link.split(\"/\")[-1]\n",
        "    command = f\"aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {link} -d {folder} -o {filename}\"\n",
        "\n",
        "    print(\"Executing download command:\")\n",
        "    print(command)\n",
        "\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "    get_ipython().system(command)\n",
        "\n",
        "    return filename\n",
        "\n",
        "\n",
        "\n",
        "def download_civitai_model(civitai_link, civitai_token, folder=\"/content/ComfyUI/models/loras\"):\n",
        "    import os\n",
        "    import time\n",
        "\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        model_id = civitai_link.split(\"/models/\")[1].split(\"?\")[0]\n",
        "    except IndexError:\n",
        "        raise ValueError(\"Invalid Civitai URL format. Please use a link like: https://civitai.com/api/download/models/1523247?...\")\n",
        "\n",
        "    civitai_url = f\"https://civitai.com/api/download/models/{model_id}?type=Model&format=SafeTensor\"\n",
        "    if civitai_token:\n",
        "        civitai_url += f\"&token={civitai_token}\"\n",
        "\n",
        "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"model_{timestamp}.safetensors\"\n",
        "\n",
        "    full_path = os.path.join(folder, filename)\n",
        "\n",
        "    download_command = f\"wget --max-redirect=10 --show-progress \\\"{civitai_url}\\\" -O \\\"{full_path}\\\"\"\n",
        "    print(\"Downloading from Civitai...\")\n",
        "\n",
        "    os.system(download_command)\n",
        "\n",
        "    local_path = os.path.join(folder, filename)\n",
        "    if os.path.exists(local_path) and os.path.getsize(local_path) > 0:\n",
        "        print(f\"LoRA downloaded successfully: {local_path}\")\n",
        "    else:\n",
        "        print(f\"❌ LoRA download failed or file is empty: {local_path}\")\n",
        "\n",
        "    return filename\n",
        "\n",
        "\n",
        "def download_lora(link, folder=\"/content/ComfyUI/models/loras\", civitai_token=None):\n",
        "    \"\"\"\n",
        "    Download a model file, automatically detecting if it's a Civitai link or huggingface download.\n",
        "\n",
        "    Args:\n",
        "        link: The download URL (either huggingface or Civitai)\n",
        "        folder: Destination folder for the download\n",
        "        civitai_token: Optional token for Civitai downloads (required if link is from Civitai)\n",
        "\n",
        "    Returns:\n",
        "        The filename of the downloaded model\n",
        "    \"\"\"\n",
        "    if \"civitai.com\" in link.lower():\n",
        "        if not civitai_token:\n",
        "            raise ValueError(\"Civitai token is required for Civitai downloads\")\n",
        "        return download_civitai_model(link, civitai_token, folder)\n",
        "    else:\n",
        "        return download_with_aria2c(link, folder)\n",
        "\n",
        "flux_lora_download_url = \"https://huggingface.co/RiverZ/normal-lora/resolve/main/pytorch_lora_weights.safetensors\"# @param {\"type\":\"string\"}\n",
        "token_if_civitai_url = \"Put your civitai token here\"# @param {\"type\":\"string\"}\n",
        "\n",
        "lora = download_lora(flux_lora_download_url, civitai_token=token_if_civitai_url)\n",
        "# Validate loRA file extension\n",
        "valid_extensions = {'.safetensors', '.ckpt', '.pt', '.pth', '.sft'}\n",
        "if lora:\n",
        "    if not any(lora.lower().endswith(ext) for ext in valid_extensions):\n",
        "        print(f\"❌ Invalid LoRA format: {lora}\")\n",
        "        lora = None\n",
        "    else:\n",
        "        clear_output()\n",
        "        print(\"loRA downloaded succesfully!\")\n",
        "\n",
        "def model_download(url: str, dest_dir: str, filename: str = None, silent: bool = True) -> bool:\n",
        "    \"\"\"\n",
        "    Colab-optimized download with aria2c\n",
        "\n",
        "    Args:\n",
        "        url: Download URL\n",
        "        dest_dir: Target directory (will be created if needed)\n",
        "        filename: Optional output filename (defaults to URL filename)\n",
        "        silent: If True, suppresses all output (except errors)\n",
        "\n",
        "    Returns:\n",
        "        bool: True if successful, False if failed\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create destination directory\n",
        "        Path(dest_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Set filename if not specified\n",
        "        if filename is None:\n",
        "            filename = url.split('/')[-1].split('?')[0]  # Remove URL parameters\n",
        "\n",
        "        # Build command\n",
        "        cmd = [\n",
        "            'aria2c',\n",
        "            '--console-log-level=error',\n",
        "            '-c', '-x', '16', '-s', '16', '-k', '1M',\n",
        "            '-d', dest_dir,\n",
        "            '-o', filename,\n",
        "            url\n",
        "        ]\n",
        "\n",
        "        # Add silent flags if requested\n",
        "        if silent:\n",
        "            cmd.extend(['--summary-interval=0', '--quiet'])\n",
        "            print(f\"Downloading {filename}...\", end=' ', flush=True)\n",
        "\n",
        "        # Run download\n",
        "        result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
        "\n",
        "        if silent:\n",
        "            print(\"Done!\")\n",
        "        else:\n",
        "            print(f\"Downloaded {filename} to {dest_dir}\")\n",
        "        return filename\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        error = e.stderr.strip() or \"Unknown error\"\n",
        "        print(f\"\\nError downloading {filename}: {error}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "flux_model = model_download(\"https://huggingface.co/YarvixPA/FLUX.1-Fill-dev-gguf/resolve/main/flux1-fill-dev-Q8_0.gguf\", \"/content/ComfyUI/models/unet\")\n",
        "# flux_model = model_download(\"https://huggingface.co/Isi99999/Upscalers/resolve/main/Flux/flux1-dev-fp8.safetensors\", \"/content/ComfyUI/models/unet\")\n",
        "flux_vae = model_download(\"https://huggingface.co/Isi99999/Upscalers/resolve/main/Flux/ae.sft\", \"/content/ComfyUI/models/vae\")\n",
        "flux_clip_l = model_download(\"https://huggingface.co/Isi99999/Upscalers/resolve/main/Flux/clip_l.safetensors\", \"/content/ComfyUI/models/clip\")\n",
        "flux_t5xxl = model_download(\"https://huggingface.co/Isi99999/Upscalers/resolve/main/Flux/t5xxl_fp8_e4m3fn.safetensors\", \"/content/ComfyUI/models/clip\")\n",
        "\n",
        "clip_loader = DualCLIPLoader()\n",
        "unet_loader =  UnetLoaderGGUF()\n",
        "# unet_loader =  UNETLoader()\n",
        "vae_loader =   VAELoader()\n",
        "vae_decode = VAEDecode()\n",
        "ksampler = KSamplerAdvanced()\n",
        "load_lora = LoraLoaderModelOnly()\n",
        "load_image = LoadImage()\n",
        "save_image = SaveImage()\n",
        "positive_prompt_encode = CLIPTextEncode()\n",
        "negative_prompt_encode = ConditioningZeroOut()\n",
        "inpaint_model_conditioning = InpaintModelConditioning()\n",
        "flux_guidance = FluxGuidance()\n",
        "imageScaleToTotalPixels = ImageScaleToTotalPixels()\n",
        "# umage_resize = ImageResizeKJ()\n",
        "solid_mask = SolidMask()\n",
        "image_scale_by = ImageScaleBy()\n",
        "image_crop = ImageCrop()\n",
        "\n",
        "def clear_memory():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()\n",
        "    for obj in list(globals().values()):\n",
        "        if torch.is_tensor(obj) or (hasattr(obj, \"data\") and torch.is_tensor(obj.data)):\n",
        "            del obj\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "def save_as_image(image, filename_prefix, output_dir=\"/content/ComfyUI/output\"):\n",
        "    \"\"\"Save single frame as PNG image.\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_path = f\"{output_dir}/{filename_prefix}.png\"\n",
        "\n",
        "    frame = (image.cpu().numpy() * 255).astype(np.uint8)\n",
        "\n",
        "    Image.fromarray(frame).save(output_path)\n",
        "\n",
        "    return output_path\n",
        "\n",
        "\n",
        "# def save_as_mp4(images, filename_prefix, fps, output_dir=\"/content/ComfyUI/output\"):\n",
        "#     os.makedirs(output_dir, exist_ok=True)\n",
        "#     output_path = f\"{output_dir}/{filename_prefix}.mp4\"\n",
        "\n",
        "#     frames = [(img.cpu().numpy() * 255).astype(np.uint8) for img in images]\n",
        "\n",
        "#     with imageio.get_writer(output_path, fps=fps) as writer:\n",
        "#         for frame in frames:\n",
        "#             writer.append_data(frame)\n",
        "\n",
        "#     return output_path\n",
        "\n",
        "def save_as_mp4(images, filename_prefix, fps, output_dir=\"/content/ComfyUI/output\"):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_path = f\"{output_dir}/{filename_prefix}.mp4\"\n",
        "\n",
        "    frames = []\n",
        "    for i, img in enumerate(images):\n",
        "        try:\n",
        "\n",
        "            if isinstance(img, torch.Tensor):\n",
        "                img = img.cpu().numpy()\n",
        "\n",
        "            # print(f\"Frame {i} initial shape: {img.shape}, dtype: {img.dtype}, max: {img.max()}\")  # Debug\n",
        "\n",
        "\n",
        "            if img.max() <= 1.0:\n",
        "                img = (img * 255).astype(np.uint8)\n",
        "            else:\n",
        "                img = img.astype(np.uint8)\n",
        "\n",
        "\n",
        "            if len(img.shape) == 4:  # Batch dimension? (N, C, H, W)\n",
        "                img = img[0]  # Take first image in batch\n",
        "\n",
        "            if len(img.shape) == 3:\n",
        "                if img.shape[0] in (1, 3, 4):  # CHW format\n",
        "                    img = np.transpose(img, (1, 2, 0))\n",
        "                elif img.shape[2] > 4:  # Too many channels\n",
        "                    img = img[:, :, :3]\n",
        "            elif len(img.shape) == 2:\n",
        "                img = np.expand_dims(img, axis=-1)\n",
        "\n",
        "            # print(f\"Frame {i} processed shape: {img.shape}\")  # Debug\n",
        "\n",
        "            # Final validation\n",
        "            if len(img.shape) != 3 or img.shape[2] not in (1, 3, 4):\n",
        "                raise ValueError(f\"Invalid frame shape after processing: {img.shape}\")\n",
        "\n",
        "            frames.append(img)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing frame {i}: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    try:\n",
        "        with imageio.get_writer(output_path, fps=fps) as writer:\n",
        "            for i, frame in enumerate(frames):\n",
        "                # print(f\"Writing frame {i} with shape: {frame.shape}\")  # Debug\n",
        "                writer.append_data(frame)\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing video: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "    return output_path\n",
        "\n",
        "import cv2\n",
        "import shutil\n",
        "from IPython.display import Video\n",
        "import datetime\n",
        "\n",
        "\n",
        "def upload_file():\n",
        "    \"\"\"Handle file upload (image or video) and return paths.\"\"\"\n",
        "    os.makedirs('/content/ComfyUI/input', exist_ok=True)\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    paths = []\n",
        "    for filename in uploaded.keys():\n",
        "        src_path = f'/content/ComfyUI/{filename}'\n",
        "        dest_path = f'/content/ComfyUI/input/{filename}'\n",
        "        shutil.move(src_path, dest_path)\n",
        "        paths.append(dest_path)\n",
        "        print(f\"File saved to: {dest_path}\")\n",
        "\n",
        "    return paths[0] if paths else None\n",
        "\n",
        "def display_video(video_path):\n",
        "    from IPython.display import HTML\n",
        "    from base64 import b64encode\n",
        "\n",
        "    video_data = open(video_path,'rb').read()\n",
        "\n",
        "    # Determine MIME type based on file extension\n",
        "    if video_path.lower().endswith('.mp4'):\n",
        "        mime_type = \"video/mp4\"\n",
        "    elif video_path.lower().endswith('.webm'):\n",
        "        mime_type = \"video/webm\"\n",
        "    elif video_path.lower().endswith('.webp'):\n",
        "        mime_type = \"image/webp\"\n",
        "    else:\n",
        "        mime_type = \"video/mp4\"  # default\n",
        "\n",
        "    data_url = f\"data:{mime_type};base64,\" + b64encode(video_data).decode()\n",
        "\n",
        "    display(HTML(f\"\"\"\n",
        "    <video width=512 controls autoplay loop>\n",
        "        <source src=\"{data_url}\" type=\"{mime_type}\">\n",
        "    </video>\n",
        "    \"\"\"))\n",
        "\n",
        "def edit_input(\n",
        "    image_path: str = None,\n",
        "    positive_prompt: str = \"\",\n",
        "    guidance: float = 50,\n",
        "    seed: int = 0,\n",
        "    steps: int = 20,\n",
        "    cfg: float = 1.0,\n",
        "    sampler_name: str = \"euler\",\n",
        "    scheduler: str = \"simple\",\n",
        "    denoise: float = 1.0,\n",
        "    show_input_with_output: bool = False,\n",
        "    overwrite: bool = False,\n",
        "    LoRA_Strength: float = 1.0\n",
        "\n",
        "):\n",
        "\n",
        "    with torch.inference_mode():\n",
        "\n",
        "        print(\"Loading Text_Encoder...\")\n",
        "        clip = clip_loader.load_clip(flux_t5xxl, flux_clip_l, \"flux\")[0]\n",
        "\n",
        "        positive_prompt = f'A diptych with two side-by-side images of the same scene. On the right, the scene is exactly the same as on the left but {positive_prompt}'\n",
        "\n",
        "        prompt_encode = positive_prompt_encode.encode(clip, positive_prompt)[0]\n",
        "        positive = flux_guidance.append(prompt_encode, guidance)[0]\n",
        "        negative = negative_prompt_encode.zero_out(prompt_encode)[0]\n",
        "\n",
        "\n",
        "        del clip\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "\n",
        "        if image_path is None:\n",
        "            print(\"Please upload an image\")\n",
        "            image_path = upload_file()\n",
        "        if image_path is None:\n",
        "            print(\"No image uploaded!\")\n",
        "        # loaded_image = load_image.load_image(image_path)[0]\n",
        "\n",
        "        output_path = \"\"\n",
        "        base_name = \"edited_Image\"\n",
        "        if not overwrite:\n",
        "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            base_name += f\"_{timestamp}\"\n",
        "\n",
        "        try:\n",
        "            if image_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                # Single image processing\n",
        "                image = load_image.load_image(image_path)[0]\n",
        "                print(\"Processing Image...\")\n",
        "                # width_int, height_int = image_width_height(loaded_image)\n",
        "                # megapixels = compute_value_from_size(width_int, height_int)\n",
        "\n",
        "                # image_pixels = imageScaleToTotalPixels.upscale(loaded_image, \"nearest-exact\", megapixels)[0]\n",
        "\n",
        "                # width_int, height_int = image_width_height(image_pixels)\n",
        "\n",
        "                # image, image_width, image_height = image_resize.resize(loaded_image, width_int, height_int, False, \"nearest-exact\", 2)\n",
        "\n",
        "\n",
        "                # image = image_scale_by.upscale(loaded_image, \"lanczos\",0.5)[0]\n",
        "\n",
        "                width_int, height_int = image_width_height(image)\n",
        "\n",
        "\n",
        "                # print(\"Applying solid mask...\")\n",
        "                mask = solid_mask.solid(1.00, width_int, height_int)[0]\n",
        "\n",
        "                # print(\"Making image for ICLora\")\n",
        "                image, mask, *_ = make(image, direction=\"left-right\", pixels=0, image_2=image, mask_1=None, mask_2=mask)\n",
        "\n",
        "                print(\"Loading VAE...\")\n",
        "                vae = vae_loader.load_vae(flux_vae)[0]\n",
        "\n",
        "                positive, negative, latent = inpaint_model_conditioning.encode(positive, negative, image, vae, mask)\n",
        "\n",
        "                print(\"Loading Unet Model...\")\n",
        "                model = unet_loader.load_unet(flux_model)[0]\n",
        "                # model = unet_loader.load_unet(flux_model, \"default\")[0]\n",
        "\n",
        "                if lora is not None:\n",
        "                    print(\"Loading Lora...\")\n",
        "                    model = load_lora.load_lora_model_only(model, lora, LoRA_Strength)[0]\n",
        "\n",
        "                clear_output()\n",
        "\n",
        "                print(\"Editing image...\")\n",
        "                image_out_latent = ksampler.sample(\n",
        "                    model=model,\n",
        "                    add_noise=\"enable\",\n",
        "                    noise_seed=seed,\n",
        "                    steps=steps,\n",
        "                    cfg=cfg,\n",
        "                    sampler_name=sampler_name,\n",
        "                    scheduler=scheduler,\n",
        "                    positive=positive,\n",
        "                    negative=negative,\n",
        "                    latent_image=latent,\n",
        "                    start_at_step=0,\n",
        "                    end_at_step=1000,\n",
        "                    return_with_leftover_noise=\"disable\"\n",
        "\n",
        "                )[0]\n",
        "\n",
        "                del model\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "                print(\"Decoding latents...\")\n",
        "                decoded = vae_decode.decode(vae, image_out_latent)[0]\n",
        "\n",
        "                del vae\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "                only_result_image = image_crop.crop(decoded,width_int, height_int,width_int,0)[0]\n",
        "\n",
        "                output_path = \"\"\n",
        "                base_name = \"ComfyUI\"\n",
        "                if not overwrite:\n",
        "                    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "                    base_name += f\"_{timestamp}\"\n",
        "\n",
        "                print(\"Saving as PNG image...\")\n",
        "                output_path = save_as_image(only_result_image[0], base_name)\n",
        "\n",
        "                display(IPImage(filename=output_path))\n",
        "\n",
        "                if show_input_with_output:\n",
        "                    output_path2 = save_as_image(decoded[0], \"image_compare\")\n",
        "                    display(IPImage(filename=output_path2))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during upscaling/saving: {str(e)}\")\n",
        "            raise\n",
        "        finally:\n",
        "            clear_memory()\n",
        "\n",
        "\n",
        "print(\"✅ Environment Setup Complete!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Upload Image/Video\n",
        "\n",
        "file_uploaded = upload_file()\n",
        "display_upload = True # @param {type:\"boolean\"}\n",
        "if display_upload:\n",
        "    if file_uploaded.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "        display(IPImage(filename=file_uploaded))\n",
        "    else:\n",
        "        display_video(file_uploaded)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "dOS6D7vMaAEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "positive_prompt=\"Add glasses to her face.\" # @param {\"type\":\"string\"}\n",
        "guidance=50 # @param {\"type\":\"slider\",\"min\":0.0,\"max\":100.0,\"step\":0.1}\n",
        "seed=0 # @param {\"type\":\"integer\"}\n",
        "steps = 20 # @param {\"type\":\"slider\",\"min\":0,\"max\":100,\"step\":1}\n",
        "cfg = 1 # @param {\"type\":\"slider\",\"min\":0,\"max\":20,\"step\":1}\n",
        "sampler_name=\"euler\" # @param [\"uni_pc\", \"uni_pc_bh2\", \"ddim\",\"euler\", \"euler_cfg_pp\", \"euler_ancestral\", \"euler_ancestral_cfg_pp\", \"heun\", \"heunpp2\",\"dpm_2\", \"dpm_2_ancestral\",\"lms\", \"dpm_fast\", \"dpm_adaptive\", \"dpmpp_2s_ancestral\", \"dpmpp_2s_ancestral_cfg_pp\", \"dpmpp_sde\", \"dpmpp_sde_gpu\",\"dpmpp_2m\", \"dpmpp_2m_cfg_pp\", \"dpmpp_2m_sde\", \"dpmpp_2m_sde_gpu\", \"dpmpp_3m_sde\", \"dpmpp_3m_sde_gpu\", \"ddpm\", \"lcm\",\"ipndm\", \"ipndm_v\", \"deis\", \"res_multistep\", \"res_multistep_cfg_pp\", \"res_multistep_ancestral\", \"res_multistep_ancestral_cfg_pp\",\"gradient_estimation\", \"er_sde\", \"seeds_2\", \"seeds_3\"]\n",
        "scheduler=\"simple\" # @param [\"simple\",\"normal\",\"karras\",\"exponential\",\"sgm_uniform\",\"ddim_uniform\",\"beta\",\"linear_quadratic\",\"kl_optimal\"]\n",
        "denoise=1 # @param {\"type\":\"slider\",\"min\":0.0,\"max\":1.0,\"step\":0.01}\n",
        "overwrite_previous_output=False # @param {type:\"boolean\"}\n",
        "show_input_with_output=False # @param {type:\"boolean\"}\n",
        "LoRA_Strength=1.0 # @param {\"type\":\"slider\",\"min\":-100,\"max\":100,\"step\":0.01}\n",
        "\n",
        "import random\n",
        "seed = seed if seed != 0 else random.randint(0, 2**32 - 1)\n",
        "print(f\"Using seed: {seed}\")\n",
        "\n",
        "edit_input(\n",
        "    image_path = file_uploaded,\n",
        "    positive_prompt = positive_prompt,\n",
        "    guidance = guidance,\n",
        "    seed = seed,\n",
        "    steps = steps,\n",
        "    cfg = cfg,\n",
        "    sampler_name = sampler_name,\n",
        "    scheduler = scheduler,\n",
        "    denoise = denoise,\n",
        "    show_input_with_output = show_input_with_output,\n",
        "    overwrite = overwrite_previous_output,\n",
        "    LoRA_Strength = LoRA_Strength\n",
        ")\n",
        "\n",
        "clear_memory()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "D9NFwJVBaD78"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}