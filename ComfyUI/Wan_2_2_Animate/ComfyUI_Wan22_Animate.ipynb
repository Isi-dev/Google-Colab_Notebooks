{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Wan2.2 Animate for character animation and replacement in ComfyUI (WIP)**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- Run the cell below to get a link (e.g. https://localhost:8188/) which you can use to launch the comfyUI interface. Remember to switch models in the workflows to either safetensors or GGUF depending on the model format you selected for download.\n",
        "- You can get the workflows here: https://github.com/Isi-dev/Google-Colab_Notebooks/tree/main/ComfyUI/Wan_2_2_Animate\n",
        "- Github project page: https://github.com/Wan-Video/Wan2.2\n",
        "- Notebook source: https://github.com/Isi-dev/Google-Colab_Notebooks\n",
        "- Premium notebooks I highly recommend: https://isinse.gumroad.com/\n",
        "- Google Colab Youtube Playlist: https://www.youtube.com/playlist?list=PLdi1sS5pbSYeA470Sb1wARR4OieCBIqMv\n",
        "- Even $1 helps support my work: https://buymeacoffee.com/isiomo\n",
        "\n"
      ],
      "metadata": {
        "id": "V1mwCaODEfO3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "uKxPNeW5zzF1",
        "outputId": "a80b3124-4684-43c8-e636-927e2534bc23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Click the link below to launch the comfyui interface\n",
            "\u001b[31mWarning: This function may stop working due to changes in browser security.\n",
            "Try `serve_kernel_port_as_iframe` instead. \u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(8188, \"/\", \"https://localhost:8188/\", window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FETCH ComfyRegistry Data: 5/98\n",
            "FETCH ComfyRegistry Data: 10/98\n",
            "FETCH ComfyRegistry Data: 15/98\n",
            "FETCH ComfyRegistry Data: 20/98\n",
            "FETCH ComfyRegistry Data: 25/98\n",
            "FETCH ComfyRegistry Data: 30/98\n",
            "FETCH ComfyRegistry Data: 35/98\n",
            "FETCH ComfyRegistry Data: 40/98\n",
            "FETCH ComfyRegistry Data: 45/98\n",
            "FETCH ComfyRegistry Data: 50/98\n",
            "FETCH ComfyRegistry Data: 55/98\n",
            "FETCH ComfyRegistry Data: 60/98\n",
            "FETCH ComfyRegistry Data: 65/98\n",
            "FETCH ComfyRegistry Data: 70/98\n",
            "FETCH ComfyRegistry Data: 75/98\n",
            "FETCH ComfyRegistry Data: 80/98\n",
            "FETCH ComfyRegistry Data: 85/98\n",
            "FETCH ComfyRegistry Data: 90/98\n",
            "FETCH ComfyRegistry Data: 95/98\n",
            "FETCH ComfyRegistry Data [DONE]\n",
            "[ComfyUI-Manager] default cache updated: https://api.comfy.org/nodes\n",
            "FETCH DATA from: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/custom-node-list.json [DONE]\n",
            "[ComfyUI-Manager] All startup tasks have been completed.\n",
            "got prompt\n",
            "Failed to find /content/ComfyUI/custom_nodes/comfyui_controlnet_aux/ckpts/hr16/yolox-onnx/yolox_l.torchscript.pt.\n",
            " Downloading from huggingface.co\n",
            "cacher folder is /tmp, you can change it by custom_tmp_path in config.yaml\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:982: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
            "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
            "  warnings.warn(\n",
            "[Errno 2] No such file or directory: '/tmp/ckpts'\n",
            "model_path is /content/ComfyUI/custom_nodes/comfyui_controlnet_aux/ckpts/hr16/yolox-onnx/yolox_l.torchscript.pt\n",
            "Failed to find /content/ComfyUI/custom_nodes/comfyui_controlnet_aux/ckpts/hr16/DWPose-TorchScript-BatchSize5/dw-ll_ucoco_384_bs5.torchscript.pt.\n",
            " Downloading from huggingface.co\n",
            "cacher folder is /tmp, you can change it by custom_tmp_path in config.yaml\n",
            "[Errno 2] No such file or directory: '/tmp/ckpts'\n",
            "model_path is /content/ComfyUI/custom_nodes/comfyui_controlnet_aux/ckpts/hr16/DWPose-TorchScript-BatchSize5/dw-ll_ucoco_384_bs5.torchscript.pt\n",
            "\n",
            "DWPose: Using yolox_l.torchscript.pt for bbox detection and dw-ll_ucoco_384_bs5.torchscript.pt for pose estimation\n",
            "DWPose: Caching TorchScript module yolox_l.torchscript.pt on ...\n",
            "DWPose: Caching TorchScript module dw-ll_ucoco_384_bs5.torchscript.pt on ...\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1784: UserWarning: backend:cudaMallocAsync ignores max_split_size_mb,roundup_power2_divisions, and garbage_collect_threshold. (Triggered internally at /pytorch/c10/cuda/CUDAAllocatorConfig.cpp:387.)\n",
            "  return forward_call(*args, **kwargs)\n",
            "DWPose: Bbox 1592.51ms\n",
            "DWPose: Pose 2300.65ms on 1 people\n",
            "\n",
            "DWPose: Bbox 5094.99ms\n",
            "DWPose: Pose 3913.48ms on 1 people\n",
            "\n",
            "DWPose: Bbox 16.53ms\n",
            "DWPose: Pose 21.47ms on 1 people\n",
            "\n",
            "DWPose: Bbox 16.86ms\n",
            "DWPose: Pose 18.64ms on 1 people\n",
            "\n",
            "DWPose: Bbox 16.54ms\n",
            "DWPose: Pose 19.51ms on 1 people\n",
            "\n",
            "DWPose: Bbox 16.91ms\n",
            "DWPose: Pose 19.28ms on 1 people\n",
            "\n",
            "DWPose: Bbox 17.78ms\n",
            "DWPose: Pose 20.28ms on 1 people\n",
            "\n",
            "DWPose: Bbox 17.35ms\n",
            "DWPose: Pose 20.91ms on 1 people\n",
            "\n",
            "DWPose: Bbox 17.11ms\n",
            "DWPose: Pose 20.70ms on 1 people\n",
            "\n",
            "DWPose: Bbox 16.98ms\n",
            "DWPose: Pose 20.30ms on 1 people\n",
            "\n",
            "DWPose: Bbox 16.83ms\n",
            "DWPose: Pose 19.78ms on 1 people\n",
            "\n",
            "DWPose: Bbox 16.85ms\n",
            "DWPose: Pose 19.56ms on 1 people\n",
            "\n",
            "DWPose: Bbox 16.91ms\n",
            "DWPose: Pose 18.80ms on 1 people\n",
            "\n",
            "DWPose: Bbox 18.95ms\n",
            "DWPose: Pose 20.71ms on 1 people\n",
            "\n",
            "DWPose: Bbox 19.35ms\n",
            "DWPose: Pose 21.25ms on 1 people\n",
            "\n",
            "DWPose: Bbox 16.98ms\n",
            "DWPose: Pose 19.86ms on 1 people\n",
            "\n",
            "DWPose: Bbox 16.96ms\n",
            "DWPose: Pose 18.86ms on 1 people\n",
            "\n",
            "DWPose: Bbox 16.86ms\n",
            "DWPose: Pose 19.68ms on 1 people\n",
            "\n",
            "DWPose: Bbox 17.07ms\n",
            "DWPose: Pose 20.23ms on 1 people\n",
            "\n",
            "DWPose: Bbox 17.22ms\n",
            "DWPose: Pose 19.01ms on 1 people\n",
            "\n",
            "DWPose: Bbox 16.85ms\n",
            "DWPose: Pose 19.96ms on 1 people\n",
            "\n",
            "DWPose: Bbox 19.73ms\n",
            "DWPose: Pose 20.20ms on 1 people\n",
            "\n",
            "DWPose: Bbox 16.97ms\n",
            "DWPose: Pose 19.12ms on 1 people\n",
            "\n",
            "DWPose: Bbox 16.86ms\n",
            "DWPose: Pose 19.28ms on 1 people\n",
            "\n",
            "DWPose: Bbox 16.91ms\n",
            "DWPose: Pose 19.04ms on 1 people\n",
            "\n",
            "DWPose: Bbox 16.98ms\n",
            "DWPose: Pose 19.13ms on 1 people\n",
            "\n",
            "DWPose: Bbox 19.51ms\n",
            "DWPose: Pose 20.11ms on 1 people\n",
            "\n",
            "DWPose: Bbox 16.91ms\n",
            "DWPose: Pose 19.19ms on 1 people\n",
            "\n",
            "DWPose: Bbox 16.81ms\n",
            "DWPose: Pose 19.10ms on 1 people\n",
            "\n",
            "tensor.shape: torch.Size([29, 848, 480, 3])\n",
            "Using scaled fp8: fp8 matrix mult: False, scale input: False\n",
            "CLIP/text encoder model load device: cuda:0, offload device: cpu, current: cpu, dtype: torch.float16\n",
            "Requested to load WanTEModel\n",
            "loaded completely 20877.250149536132 6419.477203369141 True\n",
            "Requested to load CLIPVisionModelProjection\n",
            "loaded completely 13233.672356414794 1208.09814453125 True\n",
            "Clip embeds shape: torch.Size([1, 257, 1280]), dtype: torch.float32\n",
            "Combined clip embeds shape: torch.Size([1, 257, 1280])\n",
            "CUDA Compute Capability: 8.9\n",
            "Detected model in_channels: 36\n",
            "Model cross attention type: i2v, num_heads: 40, num_layers: 40\n",
            "Model variant detected: 14B\n",
            "model_type FLOW\n",
            "Loading LoRA: lightx2v_T2V_14B_cfg_step_distill_v2_lora_rank32_bf16 with strength: 1.0\n",
            "Loading LoRA: wan2.2_animate_14B_relight_lora_bf16 with strength: 1.2\n",
            "Using GGUF to load and assign model weights to device...\n",
            "Loading transformer parameters to cuda:0: 100% 1427/1427 [00:00<00:00, 10619.00it/s]\n",
            "------- Scheduler info -------\n",
            "Total timesteps: tensor([999, 961, 909, 833, 714, 499], device='cuda:0')\n",
            "Using timesteps: tensor([999, 961, 909, 833, 714, 499], device='cuda:0')\n",
            "Using sigmas: tensor([1.0000, 0.9615, 0.9090, 0.8333, 0.7142, 0.4999, 0.0000])\n",
            "------------------------------\n",
            "sigmas: tensor([1.0000, 0.9615, 0.9090, 0.8333, 0.7142, 0.4999, 0.0000])\n",
            "Input sequence length: 14310\n",
            "Sampling 33 frames at 480x848 with 6 steps\n",
            "100% 6/6 [01:42<00:00, 17.14s/it]\n",
            "Allocated memory: memory=0.862 GB\n",
            "Max allocated memory: max_memory=8.680 GB\n",
            "Max reserved memory: max_reserved=9.562 GB\n",
            "[{}]\n",
            "model_path:  /content/ComfyUI/models/sam2/sam2.1_hiera_small-fp16.safetensors\n",
            "Downloading SAM2 model to: /content/ComfyUI/models/sam2/sam2.1_hiera_small-fp16.safetensors\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:982: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
            "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
            "  warnings.warn(\n",
            "Fetching 1 files: 100% 1/1 [00:01<00:00,  1.36s/it]\n",
            "Using model config: /content/ComfyUI/custom_nodes/ComfyUI-segment-anything-2/sam2_configs/sam2.1_hiera_s.yaml\n",
            "Resizing to model input image size:  1024\n",
            "combined labels:  [1. 1.]\n",
            "combined labels shape:  (2,)\n",
            "propagate in video: 100% 29/29 [00:01<00:00, 18.86it/s]\n",
            "Prompt executed in 211.03 seconds\n",
            "\n",
            "Stopped server\n"
          ]
        }
      ],
      "source": [
        "# @markdown # 💥Prepare Environment\n",
        "\n",
        "\n",
        "#Animate Q_8 GGUF: https://huggingface.co/Kijai/WanVideo_comfy_GGUF/resolve/main/Wan22Animate/Wan2_2_Animate_14B_Q8_0.gguf\n",
        "\n",
        "#Animate Q4_k_M GGUF: https://huggingface.co/Kijai/WanVideo_comfy_GGUF/resolve/main/Wan22Animate/Wan2_2_Animate_14B_Q4_K_M.gguf\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "!pip install torch==2.8.0 torchvision==0.23.0\n",
        "\n",
        "# using_T4_GPU = True # @param {type:\"boolean\"}\n",
        "using_T4_GPU = False\n",
        "include_manager = True # @param {type:\"boolean\"}\n",
        "\n",
        "%cd /content\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "!pip install -q torchsde einops diffusers accelerate xformers==0.0.32.post1 triton==3.4 sageattention\n",
        "!pip install av spandrel albumentations onnx opencv-python onnxruntime\n",
        "!pip install color-matcher\n",
        "!pip install onnxruntime-gpu -y\n",
        "!git clone https://github.com/comfyanonymous/ComfyUI\n",
        "!pip install -r /content/ComfyUI/requirements.txt\n",
        "clear_output()\n",
        "\n",
        "\n",
        "%cd /content/ComfyUI/custom_nodes\n",
        "# !git clone https://github.com/pythongosssss/ComfyUI-Custom-Scripts.git\n",
        "if include_manager:\n",
        "    !git clone https://github.com/ltdrdata/ComfyUI-Manager\n",
        "!git clone --branch forQwen https://github.com/Isi-dev/ComfyUI_GGUF.git\n",
        "!git clone https://github.com/Isi-dev/ComfyUI_DeleteModelPassthrough.git\n",
        "!git clone https://github.com/Isi-dev/comfyui_controlnet_aux\n",
        "!git clone https://github.com/kijai/ComfyUI-WanVideoWrapper\n",
        "!git clone https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite\n",
        "!git clone https://github.com/kijai/ComfyUI-KJNodes.git\n",
        "!git clone https://github.com/kijai/ComfyUI-segment-anything-2\n",
        "!git clone https://github.com/kijai/ComfyUI-Florence2\n",
        "!git clone https://github.com/john-mnz/ComfyUI-Inspyrenet-Rembg.git\n",
        "!git clone https://github.com/Isi-dev/ComfyUI_Animation_Nodes_and_Workflows\n",
        "%cd /content/ComfyUI/custom_nodes/ComfyUI_GGUF\n",
        "!pip install -r requirements.txt\n",
        "%cd /content/ComfyUI/custom_nodes/ComfyUI_DeleteModelPassthrough\n",
        "!pip install -r requirements.txt\n",
        "%cd /content/ComfyUI/custom_nodes/comfyui_controlnet_aux\n",
        "!pip install -r requirements.txt\n",
        "%cd /content/ComfyUI/custom_nodes/ComfyUI-WanVideoWrapper\n",
        "!pip install -r requirements.txt\n",
        "%cd /content/ComfyUI/custom_nodes/ComfyUI-VideoHelperSuite\n",
        "!pip install -r requirements.txt\n",
        "%cd /content/ComfyUI/custom_nodes/ComfyUI-KJNodes\n",
        "!pip install -r requirements.txt\n",
        "%cd /content/ComfyUI/custom_nodes/ComfyUI-Florence2\n",
        "!pip install -r requirements.txt\n",
        "%cd /content/ComfyUI/custom_nodes/ComfyUI-Inspyrenet-Rembg\n",
        "!pip install -r requirements.txt\n",
        "%cd /content/ComfyUI/custom_nodes/ComfyUI_Animation_Nodes_and_Workflows\n",
        "!pip install -r requirements.txt\n",
        "if include_manager:\n",
        "    %cd /content/ComfyUI/custom_nodes/ComfyUI-Manager\n",
        "    !pip install -r requirements.txt\n",
        "\n",
        "clear_output()\n",
        "\n",
        "\n",
        "%cd /content/ComfyUI\n",
        "\n",
        "\n",
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "# sys.path.insert(0, '/content/ComfyUI')\n",
        "\n",
        "def install_apt_packages():\n",
        "    packages = ['aria2']\n",
        "\n",
        "    try:\n",
        "        # Run apt install silently (using -qq)\n",
        "        subprocess.run(\n",
        "            ['apt-get', '-y', 'install', '-qq'] + packages,\n",
        "            check=True,\n",
        "            capture_output=True\n",
        "        )\n",
        "        print(\"✓ apt packages installed\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"✗ Error installing apt packages: {e.stderr.decode().strip() or 'Unknown error'}\")\n",
        "\n",
        "\n",
        "print(\"Installing apt packages...\")\n",
        "install_apt_packages()\n",
        "\n",
        "def download_with_aria2c(link, folder=\"/content/ComfyUI/models/loras\"):\n",
        "    import os\n",
        "\n",
        "    filename = link.split(\"/\")[-1]\n",
        "    command = f\"aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {link} -d {folder} -o {filename}\"\n",
        "\n",
        "    print(\"Executing download command:\")\n",
        "    print(command)\n",
        "\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "    get_ipython().system(command)\n",
        "\n",
        "    return filename\n",
        "\n",
        "\n",
        "\n",
        "def download_civitai_model(civitai_link, civitai_token, folder=\"/content/ComfyUI/models/loras\"):\n",
        "    import os\n",
        "    import time\n",
        "\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        model_id = civitai_link.split(\"/models/\")[1].split(\"?\")[0]\n",
        "    except IndexError:\n",
        "        raise ValueError(\"Invalid Civitai URL format. Please use a link like: https://civitai.com/api/download/models/1523247?...\")\n",
        "\n",
        "    civitai_url = f\"https://civitai.com/api/download/models/{model_id}?type=Model&format=SafeTensor\"\n",
        "    if civitai_token:\n",
        "        civitai_url += f\"&token={civitai_token}\"\n",
        "\n",
        "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"model_{timestamp}.safetensors\"\n",
        "\n",
        "    full_path = os.path.join(folder, filename)\n",
        "\n",
        "    download_command = f\"wget --max-redirect=10 --show-progress \\\"{civitai_url}\\\" -O \\\"{full_path}\\\"\"\n",
        "    print(\"Downloading from Civitai...\")\n",
        "\n",
        "    os.system(download_command)\n",
        "\n",
        "    local_path = os.path.join(folder, filename)\n",
        "    if os.path.exists(local_path) and os.path.getsize(local_path) > 0:\n",
        "        print(f\"LoRA downloaded successfully: {local_path}\")\n",
        "    else:\n",
        "        print(f\"❌ LoRA download failed or file is empty: {local_path}\")\n",
        "\n",
        "    return filename\n",
        "\n",
        "def download_lora(link, folder=\"/content/ComfyUI/models/loras\", civitai_token=None):\n",
        "    \"\"\"\n",
        "    Download a model file, automatically detecting if it's a Civitai link or huggingface download.\n",
        "\n",
        "    Args:\n",
        "        link: The download URL (either huggingface or Civitai)\n",
        "        folder: Destination folder for the download\n",
        "        civitai_token: Optional token for Civitai downloads (required if link is from Civitai)\n",
        "\n",
        "    Returns:\n",
        "        The filename of the downloaded model\n",
        "    \"\"\"\n",
        "    if \"civitai.com\" in link.lower():\n",
        "        if not civitai_token:\n",
        "            raise ValueError(\"Civitai token is required for Civitai downloads\")\n",
        "        return download_civitai_model(link, civitai_token, folder)\n",
        "    else:\n",
        "        return download_with_aria2c(link, folder)\n",
        "\n",
        "def model_download(url: str, dest_dir: str, filename: str = None, silent: bool = True) -> bool:\n",
        "    \"\"\"\n",
        "    Colab-optimized download with aria2c\n",
        "\n",
        "    Args:\n",
        "        url: Download URL\n",
        "        dest_dir: Target directory (will be created if needed)\n",
        "        filename: Optional output filename (defaults to URL filename)\n",
        "        silent: If True, suppresses all output (except errors)\n",
        "\n",
        "    Returns:\n",
        "        bool: True if successful, False if failed\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create destination directory\n",
        "        Path(dest_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Set filename if not specified\n",
        "        if filename is None:\n",
        "            filename = url.split('/')[-1].split('?')[0]  # Remove URL parameters\n",
        "\n",
        "        # Build command\n",
        "        cmd = [\n",
        "            'aria2c',\n",
        "            '--console-log-level=error',\n",
        "            '-c', '-x', '16', '-s', '16', '-k', '1M',\n",
        "            '-d', dest_dir,\n",
        "            '-o', filename,\n",
        "            url\n",
        "        ]\n",
        "\n",
        "        # Add silent flags if requested\n",
        "        if silent:\n",
        "            cmd.extend(['--summary-interval=0', '--quiet'])\n",
        "            print(f\"Downloading {filename}...\", end=' ', flush=True)\n",
        "\n",
        "        # Run download\n",
        "        result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
        "\n",
        "        if silent:\n",
        "            print(\"Done!\")\n",
        "        else:\n",
        "            print(f\"Downloaded {filename} to {dest_dir}\")\n",
        "        return filename\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        error = e.stderr.strip() or \"Unknown error\"\n",
        "        print(f\"\\nError downloading {filename}: {error}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "model_GGUF_download_url = \"https://huggingface.co/Kijai/WanVideo_comfy_GGUF/resolve/main/Wan22Animate/Wan2_2_Animate_14B_Q4_K_M.gguf\"# @param {\"type\":\"string\"}\n",
        "# model_lowNoiseGGUF_download_url = \"https://huggingface.co/bullerwins/Wan2.2-T2V-A14B-GGUF/resolve/main/wan2.2_t2v_low_noise_14B_Q4_K_M.gguf\"# @param {\"type\":\"string\"}\n",
        "# model_I2V_highNoiseGGUF_download_url = \"https://huggingface.co/bullerwins/Wan2.2-I2V-A14B-GGUF/resolve/main/wan2.2_i2v_high_noise_14B_Q4_K_M.gguf\"# @param {\"type\":\"string\"}\n",
        "# model_I2V_lowNoiseGGUF_download_url = \"https://huggingface.co/bullerwins/Wan2.2-I2V-A14B-GGUF/resolve/main/wan2.2_i2v_low_noise_14B_Q4_K_M.gguf\"# @param {\"type\":\"string\"}\n",
        "# vaceBlocks_highNoiseGGUF_download_url = \"https://huggingface.co/Kijai/WanVideo_comfy_GGUF/resolve/main/VACE/Wan2_2_Fun_VACE_module_A14B_HIGH_Q4_K_M.gguf\"# @param {\"type\":\"string\"}\n",
        "# vaceBlocks_lowNoiseGGUF_download_url = \"https://huggingface.co/Kijai/WanVideo_comfy_GGUF/resolve/main/VACE/Wan2_2_Fun_VACE_module_A14B_LOW_Q4_K_M.gguf\"# @param {\"type\":\"string\"}\n",
        "\n",
        "\n",
        "download_safetensors_instead = True # @param {type:\"boolean\"}\n",
        "model_download_url = \"https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled/resolve/main/Wan22Animate/Wan2_2-Animate-14B_fp8_e4m3fn_scaled_KJ.safetensors\"# @param {\"type\":\"string\"}\n",
        "# model_lowNoise_download_url = \"https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled/resolve/main/T2V/Wan2_2-T2V-A14B-LOW_fp8_e4m3fn_scaled_KJ.safetensors\"# @param {\"type\":\"string\"}\n",
        "# model_I2V_highNoise_download_url = \"https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled/resolve/main/I2V/Wan2_2-I2V-A14B-HIGH_fp8_e4m3fn_scaled_KJ.safetensors\"# @param {\"type\":\"string\"}\n",
        "# model_I2V_lowNoise_download_url = \"https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled/resolve/main/I2V/Wan2_2-I2V-A14B-LOW_fp8_e4m3fn_scaled_KJ.safetensors\"# @param {\"type\":\"string\"}\n",
        "# vaceBlocks_highNoise_download_url = \"https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled/resolve/main/VACE/Wan2_2_Fun_VACE_module_A14B_HIGH_fp8_e4m3fn_scaled_KJ.safetensors\"# @param {\"type\":\"string\"}\n",
        "# vaceBlocks_lowNoise_download_url = \"https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled/resolve/main/VACE/Wan2_2_Fun_VACE_module_A14B_LOW_fp8_e4m3fn_scaled_KJ.safetensors\"# @param {\"type\":\"string\"}\n",
        "\n",
        "\n",
        "\n",
        "if download_safetensors_instead:\n",
        "    model_download(model_download_url, \"/content/ComfyUI/models/diffusion_models\")\n",
        "    # model_download(model_lowNoise_download_url, \"/content/ComfyUI/models/diffusion_models\")\n",
        "    # model_download(model_I2V_highNoise_download_url, \"/content/ComfyUI/models/diffusion_models\")\n",
        "    # model_download(model_I2V_lowNoise_download_url, \"/content/ComfyUI/models/diffusion_models\")\n",
        "    # model_download(vaceBlocks_highNoise_download_url, \"/content/ComfyUI/models/diffusion_models\")\n",
        "    # model_download(vaceBlocks_lowNoise_download_url, \"/content/ComfyUI/models/diffusion_models\")\n",
        "else:\n",
        "    model_download(model_GGUF_download_url, \"/content/ComfyUI/models/diffusion_models\")\n",
        "    # model_download(model_lowNoiseGGUF_download_url, \"/content/ComfyUI/models/diffusion_models\")\n",
        "    # model_download(model_I2V_highNoiseGGUF_download_url, \"/content/ComfyUI/models/diffusion_models\")\n",
        "    # model_download(model_I2V_lowNoiseGGUF_download_url, \"/content/ComfyUI/models/diffusion_models\")\n",
        "    # model_download(vaceBlocks_highNoiseGGUF_download_url, \"/content/ComfyUI/models/diffusion_models\")\n",
        "    # model_download(vaceBlocks_lowNoiseGGUF_download_url, \"/content/ComfyUI/models/diffusion_models\")\n",
        "\n",
        "\n",
        "text_encoder = \"https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors\"# @param {\"type\":\"string\"}\n",
        "vae = \"https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors\"# @param {\"type\":\"string\"}\n",
        "clip_vision = \"https://huggingface.co/Isi99999/Wan_Extras/resolve/main/clip_vision_h.safetensors\"# @param {\"type\":\"string\"}\n",
        "\n",
        "\n",
        "model_download(text_encoder, \"/content/ComfyUI/models/text_encoders\")\n",
        "model_download(vae, \"/content/ComfyUI/models/vae\")\n",
        "model_download(clip_vision, \"/content/ComfyUI/models/clip_vision\")\n",
        "\n",
        "\n",
        "download_high_noise_speed_LoRA = True # @param {type:\"boolean\"}\n",
        "high_noise_speed_LoRA_download_url = \"https://huggingface.co/Kijai/WanVideo_comfy/resolve/main/Lightx2v/lightx2v_T2V_14B_cfg_step_distill_v2_lora_rank32_bf16.safetensors\"# @param {\"type\":\"string\"}\n",
        "download_low_noise_speed_LoRA = True # @param {type:\"boolean\"}\n",
        "low_noise_speed_LoRA_download_url = \"https://huggingface.co/Kijai/WanVideo_comfy/resolve/main/Lightx2v/lightx2v_T2V_14B_cfg_step_distill_v2_lora_rank32_bf16.safetensors\"# @param {\"type\":\"string\"}\n",
        "\n",
        "if download_high_noise_speed_LoRA:\n",
        "    lightx2v_lora = model_download(high_noise_speed_LoRA_download_url, \"/content/ComfyUI/models/loras\")\n",
        "if download_low_noise_speed_LoRA:\n",
        "    lightx2v_lora_lowNoise = model_download(low_noise_speed_LoRA_download_url, \"/content/ComfyUI/models/loras\")\n",
        "\n",
        "download_segmentation_model = True # @param {type:\"boolean\"}\n",
        "segmentation_model = \"https://huggingface.co/Kijai/sam2-safetensors/resolve/main/sam2.1_hiera_small.safetensors\"# @param {\"type\":\"string\"}\n",
        "if download_segmentation_model:\n",
        "    model_download(segmentation_model, \"/content/ComfyUI/models/sam2\")\n",
        "\n",
        "\n",
        "download_loRA1 = True # @param {type:\"boolean\"}\n",
        "lora1_download_url = \"https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/loras/wan2.2_animate_14B_relight_lora_bf16.safetensors\"# @param {\"type\":\"string\"}\n",
        "\n",
        "download_loRA_2 = False # @param {type:\"boolean\"}\n",
        "lora_2_download_url = \"Put your loRA here\"# @param {\"type\":\"string\"}\n",
        "\n",
        "download_loRA_3 = False # @param {type:\"boolean\"}\n",
        "lora3_download_url = \"Put your loRA here\"# @param {\"type\":\"string\"}\n",
        "\n",
        "download_loRA_4 = False # @param {type:\"boolean\"}\n",
        "lora_4_download_url = \"Put your loRA here\"# @param {\"type\":\"string\"}\n",
        "\n",
        "token_if_civitai_url = \"\"# @param {\"type\":\"string\"}\n",
        "\n",
        "\n",
        "\n",
        "lora1 = None\n",
        "if download_loRA1:\n",
        "    lora1 = download_lora(lora1_download_url, civitai_token=token_if_civitai_url)\n",
        "# Validate loRA file extension\n",
        "valid_extensions = {'.safetensors', '.ckpt', '.pt', '.pth', '.sft'}\n",
        "if lora1:\n",
        "    if not any(lora1.lower().endswith(ext) for ext in valid_extensions):\n",
        "        print(f\"❌ Invalid LoRA format: {lora1}\")\n",
        "        lora1 = None\n",
        "    else:\n",
        "        clear_output()\n",
        "        print(\"loRA1 downloaded succesfully!\")\n",
        "\n",
        "lora_2 = None\n",
        "if download_loRA_2:\n",
        "    lora_2 = download_lora(lora_2_download_url, civitai_token=token_if_civitai_url)\n",
        "if lora_2:\n",
        "    if not any(lora_2.lower().endswith(ext) for ext in valid_extensions):\n",
        "        print(f\"❌ Invalid LoRA format: {lora_2}\")\n",
        "        lora_2 = None\n",
        "    else:\n",
        "        clear_output()\n",
        "        print(\"loRA 2 downloaded succesfully!\")\n",
        "\n",
        "lora_3 = None\n",
        "if download_loRA_3:\n",
        "    lora_3 = download_lora(lora3_download_url, civitai_token=token_if_civitai_url)\n",
        "if lora_3:\n",
        "    if not any(lora_3.lower().endswith(ext) for ext in valid_extensions):\n",
        "        print(f\"❌ Invalid LoRA format: {lora_3}\")\n",
        "        lora_3 = None\n",
        "    else:\n",
        "        clear_output()\n",
        "        print(\"loRA 3 downloaded succesfully!\")\n",
        "\n",
        "lora_4 = None\n",
        "if download_loRA_4:\n",
        "    lora_4 = download_lora(lora_4_download_url, civitai_token=token_if_civitai_url)\n",
        "if lora_4:\n",
        "    if not any(lora_4.lower().endswith(ext) for ext in valid_extensions):\n",
        "        print(f\"❌ Invalid LoRA format: {lora_4}\")\n",
        "        lora_4 = None\n",
        "    else:\n",
        "        clear_output()\n",
        "        print(\"loRA 4 downloaded succesfully!\")\n",
        "\n",
        "\n",
        "# model = model_download(flux_model_download_url, \"/content/ComfyUI/models/unet\")\n",
        "# text_encoder = model_download(flux_text_encoder_download_url, \"/content/ComfyUI/models/clip\")\n",
        "# text_encoder2 = model_download(flux_text_encoder_download_url2, \"/content/ComfyUI/models/clip\")\n",
        "# vae = model_download(flux_vae_download_url, \"/content/ComfyUI/models/vae\")\n",
        "# lora = model_download(flux_uso_lora_download_url, \"/content/ComfyUI/models/loras\")\n",
        "# patch = model_download(flux_uso_patch_download_url, \"/content/ComfyUI/models/model_patches\")\n",
        "# clip_vision = model_download(clip_vision_download_url, \"/content/ComfyUI/models/clip_vision\")\n",
        "# turbo = model_download(speed_lora_download_url, \"/content/ComfyUI/models/loras\")\n",
        "\n",
        "use_cloudflare = False # @param {type:\"boolean\"}\n",
        "use_interface_in_cell = False # @param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "if use_cloudflare:\n",
        "    !wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "    !dpkg -i cloudflared-linux-amd64.deb\n",
        "\n",
        "    import subprocess\n",
        "    import threading\n",
        "    import time\n",
        "    import socket\n",
        "    import urllib.request\n",
        "\n",
        "    def iframe_thread(port):\n",
        "      while True:\n",
        "          time.sleep(0.5)\n",
        "          sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "          result = sock.connect_ex(('127.0.0.1', port))\n",
        "          if result == 0:\n",
        "            break\n",
        "          sock.close()\n",
        "      print(\"\\nComfyUI finished loading, trying to launch cloudflared (if it gets stuck here cloudflared is having issues)\\n\")\n",
        "\n",
        "      p = subprocess.Popen([\"cloudflared\", \"tunnel\", \"--url\", \"http://127.0.0.1:{}\".format(port)], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "      for line in p.stderr:\n",
        "        l = line.decode()\n",
        "        if \"trycloudflare.com \" in l:\n",
        "          print(\"This is the URL to access ComfyUI:\", l[l.find(\"http\"):], end='')\n",
        "        #print(l, end='')\n",
        "    clear_output()\n",
        "\n",
        "    threading.Thread(target=iframe_thread, daemon=True, args=(8188,)).start()\n",
        "\n",
        "    if using_T4_GPU:\n",
        "        !python main.py --cache-none --dont-print-server\n",
        "    else:\n",
        "        !python main.py --dont-print-server\n",
        "\n",
        "\n",
        "elif use_interface_in_cell:\n",
        "    import threading\n",
        "    import time\n",
        "    import socket\n",
        "    def iframe_thread(port):\n",
        "      while True:\n",
        "          time.sleep(0.5)\n",
        "          sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "          result = sock.connect_ex(('127.0.0.1', port))\n",
        "          if result == 0:\n",
        "            break\n",
        "          sock.close()\n",
        "      from google.colab import output\n",
        "      output.serve_kernel_port_as_iframe(port, height=1024)\n",
        "      clear_output()\n",
        "      print(\"to open it in a window you can open this link here:\")\n",
        "      output.serve_kernel_port_as_window(port)\n",
        "\n",
        "    threading.Thread(target=iframe_thread, daemon=True, args=(8188,)).start()\n",
        "\n",
        "    if using_T4_GPU:\n",
        "        !python main.py --cache-none --dont-print-server\n",
        "    else:\n",
        "        !python main.py --dont-print-server\n",
        "\n",
        "else:\n",
        "    import socket, time, threading\n",
        "    from google.colab import output\n",
        "\n",
        "    def link_thread(port):\n",
        "        while True:\n",
        "            time.sleep(0.5)\n",
        "            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "            result = sock.connect_ex(('127.0.0.1', port))\n",
        "            if result == 0:\n",
        "                break\n",
        "            sock.close()\n",
        "        clear_output()\n",
        "        print(\"Click the link below to launch the comfyui interface\")\n",
        "        output.serve_kernel_port_as_window(port)\n",
        "\n",
        "\n",
        "    # Start thread for port 8188\n",
        "    threading.Thread(target=link_thread, daemon=True, args=(8188,)).start()\n",
        "\n",
        "    if using_T4_GPU:\n",
        "        !python main.py --cache-none --dont-print-server\n",
        "    else:\n",
        "        !python main.py --dont-print-server\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}