{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Qwen-Image-Edit-2511 with Mask Editor**\n",
        "- This notebook preserves the input imageâ€™s aspect ratio much better than previous versions even without using the mask editor. I plan to write a detailed guide on how to use it and will include the link here when itâ€™s ready.\n",
        "- The Qwen Image Edit 2509 model (replaced with 2511 which is not always better) works best with clear prompts e.g for 3 uploaded images: \"Let the girl in image 1 wear the outfit from image 2 and assume the pose from image 3.\" While inpainting with the mask editor offers more control, it produces less impressive results and sometimes does nothing. You can try the mask editor, but prompt-only editing generally gives better results, as thatâ€™s how the model was originally designed to work.\n",
        "- You can find models here: https://huggingface.co/QuantStack/Qwen-Image-Edit-2509-GGUF/tree/main\n",
        "- Enabling the speedup LoRA will internally set the steps to 4 and the CFG to 1.\n",
        "- To use a lora, put its huggingface or civitai download link in the `lora_download_url` textbox, select the `download_lora` checkbox, and if using civitai, input your civitai token before running the code to `Prepare Environment`. Remember to describe the main subject of the image and include the trigger words for the LoRA in the prompt.\n",
        "- Github project page: https://github.com/QwenLM/Qwen-Image\n",
        "- Notebook source: https://github.com/Isi-dev/Google-Colab_Notebooks\n",
        "- Premium notebooks I highly recommend: https://isinse.gumroad.com/\n",
        "- Even $1 helps support my work: https://buymeacoffee.com/isiomo\n",
        "\n"
      ],
      "metadata": {
        "id": "8gNEkuc4oyv8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-3uUiopZ2hy",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title {\"single-column\":true}\n",
        "\n",
        "# qwen_model_download_url: https://huggingface.co/city96/Qwen-Image-gguf/resolve/main/qwen-image-Q4_K_M.gguf\n",
        "\n",
        "# qwen_edit_model_download_url: https://huggingface.co/QuantStack/Qwen-Image-Edit-GGUF/resolve/main/Qwen_Image_Edit-Q4_K_M.gguf\n",
        "\n",
        "# qwen_edit_model_2509_download_url: https://huggingface.co/QuantStack/Qwen-Image-Edit-2509-GGUF/resolve/main/Qwen-Image-Edit-2509-Q4_K_M.gguf\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# @markdown # ðŸ’¥1. Prepare Environment\n",
        "# !pip install --quiet torch torchvision --index-url https://download.pytorch.org/whl/cu124\n",
        "# !pip install --upgrade --quiet torch torchvision --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install torch==2.8.0 torchvision==0.23.0\n",
        "%cd /content\n",
        "\n",
        "from IPython.display import clear_output\n",
        "!git clone --branch ComfyUI_v0.3.60 https://github.com/Isi-dev/ComfyUI\n",
        "clear_output()\n",
        "%cd /content/ComfyUI/custom_nodes\n",
        "!git clone https://github.com/Isi-dev/ComfyUI_Img2PaintingAssistant\n",
        "!git clone --branch forQwen https://github.com/Isi-dev/ComfyUI_GGUF.git\n",
        "clear_output()\n",
        "!git clone https://github.com/Isi-dev/comfyui_controlnet_aux\n",
        "clear_output()\n",
        "# !git clone https://github.com/Isi-dev/ComfyUI_KJNodes.git\n",
        "# clear_output()\n",
        "%cd /content/ComfyUI/custom_nodes/ComfyUI_GGUF\n",
        "!pip install -r requirements.txt\n",
        "clear_output()\n",
        "%cd /content/ComfyUI/custom_nodes/comfyui_controlnet_aux\n",
        "!pip install -r requirements.txt\n",
        "clear_output()\n",
        "\n",
        "# %cd /content/ComfyUI/custom_nodes/ComfyUI_KJNodes\n",
        "# !pip install -r requirements.txt\n",
        "# clear_output()\n",
        "%cd /content/ComfyUI\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "\n",
        "def install_pip_packages():\n",
        "    packages = [\n",
        "        'torchsde',\n",
        "        'av',\n",
        "        'diffusers',\n",
        "        # 'transformers',\n",
        "        'xformers==0.0.32.post1',\n",
        "        'accelerate',\n",
        "        'triton==3.4',\n",
        "        'sageattention==1.0.6',\n",
        "        # 'omegaconf',\n",
        "        # 'tqdm',\n",
        "        # 'librosa',\n",
        "        'einops',\n",
        "        'spandrel',\n",
        "        'albumentations',\n",
        "        'insightface',\n",
        "        'onnx',\n",
        "        'opencv-python',\n",
        "        'segment_anything',\n",
        "        'ultralytics',\n",
        "        'onnxruntime',\n",
        "        'onnxruntime-gpu'\n",
        "\n",
        "    ]\n",
        "\n",
        "\n",
        "\n",
        "    for package in packages:\n",
        "        try:\n",
        "            # Run pip install silently (using -q)\n",
        "            subprocess.run(\n",
        "                [sys.executable, '-m', 'pip', 'install', '-q', package],\n",
        "                check=True,\n",
        "                capture_output=True\n",
        "            )\n",
        "            print(f\"âœ“ {package} installed\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"âœ— Error installing {package}: {e.stderr.decode().strip() or 'Unknown error'}\")\n",
        "\n",
        "def install_apt_packages():\n",
        "    packages = ['aria2']\n",
        "\n",
        "    try:\n",
        "        # Run apt install silently (using -qq)\n",
        "        subprocess.run(\n",
        "            ['apt-get', '-y', 'install', '-qq'] + packages,\n",
        "            check=True,\n",
        "            capture_output=True\n",
        "        )\n",
        "        print(\"âœ“ apt packages installed\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"âœ— Error installing apt packages: {e.stderr.decode().strip() or 'Unknown error'}\")\n",
        "\n",
        "\n",
        "print(\"Installing pip packages...\")\n",
        "install_pip_packages()\n",
        "!pip install albumentations onnx opencv-python onnxruntime\n",
        "!pip install onnxruntime-gpu\n",
        "clear_output()  # Clear the pip installation output\n",
        "\n",
        "print(\"Installing apt packages...\")\n",
        "install_apt_packages()\n",
        "clear_output()  # Clear the apt installation output\n",
        "\n",
        "print(\"Installation completed with status:\")\n",
        "print(\"- All pip packages installed successfully\" if 'âœ—' not in install_pip_packages.__code__.co_consts else \"- Some pip packages had issues\")\n",
        "print(\"- apt packages installed successfully\" if 'âœ—' not in install_apt_packages.__code__.co_consts else \"- apt packages had issues\")\n",
        "\n",
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import gc\n",
        "import torch\n",
        "from PIL import Image\n",
        "import random\n",
        "import imageio\n",
        "from google.colab import files\n",
        "from IPython.display import display, HTML, Image as IPImage\n",
        "sys.path.insert(0, '/content/ComfyUI')\n",
        "\n",
        "from nodes import (\n",
        "    DualCLIPLoader,\n",
        "    CLIPLoader,\n",
        "    CLIPTextEncode,\n",
        "    VAEEncode,\n",
        "    VAEEncodeForInpaint,\n",
        "    VAEDecode,\n",
        "    VAELoader,\n",
        "    KSampler,\n",
        "    KSamplerAdvanced,\n",
        "    ConditioningZeroOut,\n",
        "    InpaintModelConditioning,\n",
        "    ImageScaleBy,\n",
        "    ImageScale,\n",
        "    LoraLoaderModelOnly,\n",
        "    LoadImage,\n",
        "    SaveImage\n",
        ")\n",
        "\n",
        "from custom_nodes.ComfyUI_GGUF.nodes import UnetLoaderGGUF\n",
        "\n",
        "from comfy_extras.nodes_qwen import TextEncodeQwenImageEditPlus\n",
        "\n",
        "from comfy_extras.nodes_edit_model import ReferenceLatent\n",
        "\n",
        "from comfy_extras.nodes_hunyuan import EmptyHunyuanLatentVideo\n",
        "\n",
        "from comfy_extras.nodes_model_advanced import ModelSamplingSD3\n",
        "\n",
        "from comfy_extras.nodes_cfg import CFGNorm\n",
        "\n",
        "\n",
        "\n",
        "# from comfy_extras.nodes_flux import (\n",
        "#     FluxGuidance,\n",
        "#     FluxKontextImageScale\n",
        "# )\n",
        "\n",
        "# from comfy_extras.nodes_images import (\n",
        "#     ImageCrop,\n",
        "#     ImageStitch\n",
        "\n",
        "# )\n",
        "\n",
        "from custom_nodes.ComfyUI_Img2PaintingAssistant.image_to_painting_node import Painting\n",
        "\n",
        "from comfy_extras.nodes_sd3 import EmptySD3LatentImage\n",
        "\n",
        "from comfy_extras.nodes_post_processing import ImageScaleToTotalPixels\n",
        "\n",
        "from comfy_extras.nodes_mask import SolidMask\n",
        "\n",
        "from custom_nodes.comfyui_controlnet_aux.node_wrappers.dwpose import DWPose_Preprocessor\n",
        "\n",
        "def image_width_height(image):\n",
        "    if image.ndim == 4:\n",
        "        _, height, width, _ = image.shape\n",
        "    elif image.ndim == 3:\n",
        "        height, width, _ = image.shape\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported image shape: {image.shape}\")\n",
        "    return width, height\n",
        "\n",
        "def compute_value_from_size(width: int, height: int) -> float:\n",
        "    return float(width * height / 4)\n",
        "\n",
        "def fillMask(width, height, mask, box=(0, 0), color=0):\n",
        "    bg = Image.new(\"L\", (width, height), color)\n",
        "    bg.paste(mask, box, mask)\n",
        "    return bg\n",
        "\n",
        "def emptyImage(width, height, batch_size=1, color=0):\n",
        "    r = torch.full([batch_size, height, width, 1], ((color >> 16) & 0xFF) / 0xFF)\n",
        "    g = torch.full([batch_size, height, width, 1], ((color >> 8) & 0xFF) / 0xFF)\n",
        "    b = torch.full([batch_size, height, width, 1], ((color) & 0xFF) / 0xFF)\n",
        "    return torch.cat((r, g, b), dim=-1)\n",
        "\n",
        "def pil2tensor(image):\n",
        "  return torch.from_numpy(np.array(image).astype(np.float32) / 255.0).unsqueeze(0)\n",
        "\n",
        "def tensor2pil(image):\n",
        "    return Image.fromarray(np.clip(255. * image.cpu().numpy().squeeze(), 0, 255).astype(np.uint8))\n",
        "\n",
        "def lanczos(samples, width, height):\n",
        "    images = [Image.fromarray(np.clip(255. * image.movedim(0, -1).cpu().numpy(), 0, 255).astype(np.uint8)) for image in samples]\n",
        "    images = [image.resize((width, height), resample=Image.Resampling.LANCZOS) for image in images]\n",
        "    images = [torch.from_numpy(np.array(image).astype(np.float32) / 255.0).movedim(-1, 0) for image in images]\n",
        "    result = torch.stack(images)\n",
        "    return result.to(samples.device, samples.dtype)\n",
        "\n",
        "def bislerp(samples, width, height):\n",
        "    def slerp(b1, b2, r):\n",
        "        '''slerps batches b1, b2 according to ratio r, batches should be flat e.g. NxC'''\n",
        "\n",
        "        c = b1.shape[-1]\n",
        "\n",
        "        #norms\n",
        "        b1_norms = torch.norm(b1, dim=-1, keepdim=True)\n",
        "        b2_norms = torch.norm(b2, dim=-1, keepdim=True)\n",
        "\n",
        "        #normalize\n",
        "        b1_normalized = b1 / b1_norms\n",
        "        b2_normalized = b2 / b2_norms\n",
        "\n",
        "        #zero when norms are zero\n",
        "        b1_normalized[b1_norms.expand(-1,c) == 0.0] = 0.0\n",
        "        b2_normalized[b2_norms.expand(-1,c) == 0.0] = 0.0\n",
        "\n",
        "        #slerp\n",
        "        dot = (b1_normalized*b2_normalized).sum(1)\n",
        "        omega = torch.acos(dot)\n",
        "        so = torch.sin(omega)\n",
        "\n",
        "        #technically not mathematically correct, but more pleasing?\n",
        "        res = (torch.sin((1.0-r.squeeze(1))*omega)/so).unsqueeze(1)*b1_normalized + (torch.sin(r.squeeze(1)*omega)/so).unsqueeze(1) * b2_normalized\n",
        "        res *= (b1_norms * (1.0-r) + b2_norms * r).expand(-1,c)\n",
        "\n",
        "        #edge cases for same or polar opposites\n",
        "        res[dot > 1 - 1e-5] = b1[dot > 1 - 1e-5]\n",
        "        res[dot < 1e-5 - 1] = (b1 * (1.0-r) + b2 * r)[dot < 1e-5 - 1]\n",
        "        return res\n",
        "\n",
        "def common_upscale(samples, width, height, upscale_method, crop):\n",
        "        orig_shape = tuple(samples.shape)\n",
        "        if len(orig_shape) > 4:\n",
        "            samples = samples.reshape(samples.shape[0], samples.shape[1], -1, samples.shape[-2], samples.shape[-1])\n",
        "            samples = samples.movedim(2, 1)\n",
        "            samples = samples.reshape(-1, orig_shape[1], orig_shape[-2], orig_shape[-1])\n",
        "        if crop == \"center\":\n",
        "            old_width = samples.shape[-1]\n",
        "            old_height = samples.shape[-2]\n",
        "            old_aspect = old_width / old_height\n",
        "            new_aspect = width / height\n",
        "            x = 0\n",
        "            y = 0\n",
        "            if old_aspect > new_aspect:\n",
        "                x = round((old_width - old_width * (new_aspect / old_aspect)) / 2)\n",
        "            elif old_aspect < new_aspect:\n",
        "                y = round((old_height - old_height * (old_aspect / new_aspect)) / 2)\n",
        "            s = samples.narrow(-2, y, old_height - y * 2).narrow(-1, x, old_width - x * 2)\n",
        "        else:\n",
        "            s = samples\n",
        "\n",
        "        if upscale_method == \"bislerp\":\n",
        "            out = bislerp(s, width, height)\n",
        "        elif upscale_method == \"lanczos\":\n",
        "            out = lanczos(s, width, height)\n",
        "        else:\n",
        "            out = torch.nn.functional.interpolate(s, size=(height, width), mode=upscale_method)\n",
        "\n",
        "        if len(orig_shape) == 4:\n",
        "            return out\n",
        "\n",
        "        out = out.reshape((orig_shape[0], -1, orig_shape[1]) + (height, width))\n",
        "        return out.movedim(2, 1).reshape(orig_shape[:-2] + (height, width))\n",
        "\n",
        "def make(image_1, direction=\"left-right\", pixels=0, image_2=None, mask_1=None, mask_2=None):\n",
        "    if image_2 is None:\n",
        "      image_2 = emptyImage(image_1.shape[2], image_1.shape[1])\n",
        "      mask_2 = torch.full((1, image_1.shape[1], image_1.shape[2]), 1, dtype=torch.float32, device=\"cpu\")\n",
        "\n",
        "    elif image_2 is not None and mask_2 is None:\n",
        "      raise ValueError(\"mask_2 is required when image_2 is provided\")\n",
        "    if pixels > 0:\n",
        "      _, img2_h, img2_w, _ = image_2.shape\n",
        "      h = pixels if direction == 'left-right' else int(img2_h * (pixels / img2_w))\n",
        "      w = pixels if direction == 'top-bottom' else int(img2_w * (pixels / img2_h))\n",
        "\n",
        "      image_2 = image_2.movedim(-1, 1)\n",
        "      image_2 = common_upscale(image_2, w, h, 'bicubic', 'disabled')\n",
        "      image_2 = image_2.movedim(1, -1)\n",
        "\n",
        "      orig_image_2 = tensor2pil(image_2)\n",
        "      orig_mask_2 = tensor2pil(mask_2).convert('L')\n",
        "      orig_mask_2 = orig_mask_2.resize(orig_image_2.size)\n",
        "      mask_2 = pil2tensor(orig_mask_2)\n",
        "\n",
        "    _, img1_h, img1_w, _ = image_1.shape\n",
        "    _, img2_h, img2_w, _ = image_2.shape\n",
        "\n",
        "    image, mask, context_mask = None, None, None\n",
        "\n",
        "    # resize\n",
        "    if img1_h != img2_h and img1_w != img2_w:\n",
        "      width, height = img2_w, img2_h\n",
        "      if direction == 'left-right' and img1_h != img2_h:\n",
        "        scale_factor = img2_h / img1_h\n",
        "        width = round(img1_w * scale_factor)\n",
        "      elif direction == 'top-bottom' and img1_w != img2_w:\n",
        "        scale_factor = img2_w / img1_w\n",
        "        height = round(img1_h * scale_factor)\n",
        "\n",
        "      image_1 = image_1.movedim(-1, 1)\n",
        "      image_1 = common_upscale(image_1, width, height, 'bicubic', 'disabled')\n",
        "      image_1 = image_1.movedim(1, -1)\n",
        "\n",
        "    if mask_1 is None:\n",
        "      mask_1 = torch.full((1, image_1.shape[1], image_1.shape[2]), 0, dtype=torch.float32, device=\"cpu\")\n",
        "\n",
        "    orig_image_1 = tensor2pil(image_1)\n",
        "    orig_mask_1 = tensor2pil(mask_1).convert('L')\n",
        "\n",
        "    if orig_mask_1.size != orig_image_1.size:\n",
        "      orig_mask_1 = orig_mask_1.resize(orig_image_1.size)\n",
        "\n",
        "    img1_w, img1_h = orig_image_1.size\n",
        "    image_1 = pil2tensor(orig_image_1)\n",
        "    image = torch.cat((image_1, image_2), dim=2) if direction == 'left-right' else torch.cat((image_1, image_2),\n",
        "                                                                                             dim=1)\n",
        "\n",
        "    context_mask = fillMask(image.shape[2], image.shape[1], orig_mask_1)\n",
        "    context_mask = pil2tensor(context_mask)\n",
        "\n",
        "    orig_mask_2 = tensor2pil(mask_2).convert('L')\n",
        "    x = img1_w if direction == 'left-right' else 0\n",
        "    y = img1_h if direction == 'top-bottom' else 0\n",
        "    mask = fillMask(image.shape[2], image.shape[1], orig_mask_2, (x, y))\n",
        "    mask = pil2tensor(mask)\n",
        "\n",
        "    return (image, mask, context_mask, img2_w, img2_h, x, y)\n",
        "\n",
        "\n",
        "def load_mask_for_inpaint(image_tensor: torch.Tensor, mask_path: str) -> torch.Tensor:\n",
        "    # Image tensor is [B, H, W, C] in ComfyUI\n",
        "    B, H, W, C = image_tensor.shape\n",
        "\n",
        "    # Load mask as grayscale\n",
        "    mask_img = Image.open(mask_path).convert(\"L\")\n",
        "\n",
        "    # Resize mask to match image\n",
        "    if mask_img.size != (W, H):\n",
        "        mask_img = mask_img.resize((W, H), Image.NEAREST)\n",
        "\n",
        "    # Convert to binary mask (1=masked, 0=keep)\n",
        "    mask_np = (np.array(mask_img) > 127).astype(np.float32)\n",
        "\n",
        "    # Convert to torch tensor -> shape [B, 1, H, W]\n",
        "    mask_tensor = torch.from_numpy(mask_np).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    # Expand to batch size\n",
        "    if B > 1:\n",
        "        mask_tensor = mask_tensor.expand(B, -1, -1, -1)\n",
        "\n",
        "    return mask_tensor\n",
        "\n",
        "def resize_and_pad_to_1024_maskFit(image, interpolation=\"lanczos\", is_mask=False):\n",
        "    \"\"\"\n",
        "    Resize and pad image or mask to 1024x1024, centering it with black (or zero) padding.\n",
        "\n",
        "    Args:\n",
        "        image (torch.Tensor): Tensor [B,H,W,C] for images or masks.\n",
        "        interpolation (str): Interpolation mode for resizing (\"lanczos\" for images, \"nearest\" for masks).\n",
        "        is_mask (bool): If True, process as a mask (forces nearest-neighbor).\n",
        "\n",
        "    Returns:\n",
        "        (torch.Tensor, list[dict]): Padded image/mask and metadata.\n",
        "    \"\"\"\n",
        "    batch_size, orig_h, orig_w, channels = image.shape\n",
        "    target_size = 1024\n",
        "    meta = []\n",
        "\n",
        "    # Switch to (B,C,H,W) for comfy upscale\n",
        "    image_chw = image.permute(0, 3, 1, 2)\n",
        "\n",
        "    if orig_h <= target_size and orig_w <= target_size:\n",
        "        scale = 1.0\n",
        "    else:\n",
        "        scale = target_size / max(orig_h, orig_w)\n",
        "\n",
        "    new_w = int(round(orig_w * scale))\n",
        "    new_h = int(round(orig_h * scale))\n",
        "\n",
        "    # Use nearest interpolation for masks\n",
        "    interp_mode = \"nearest-exact\" if is_mask else interpolation\n",
        "\n",
        "    resized = common_upscale(\n",
        "        image_chw, new_w, new_h, interp_mode, \"disabled\"\n",
        "    )\n",
        "\n",
        "    # Create zero-padded tensor\n",
        "    padded = torch.zeros(\n",
        "        (batch_size, channels, target_size, target_size),\n",
        "        dtype=image.dtype,\n",
        "        device=image.device\n",
        "    )\n",
        "\n",
        "    y_offset = (target_size - new_h) // 2\n",
        "    x_offset = (target_size - new_w) // 2\n",
        "\n",
        "    padded[:, :, y_offset:y_offset+new_h, x_offset:x_offset+new_w] = resized\n",
        "\n",
        "    meta.append({\n",
        "        \"orig_h\": orig_h,\n",
        "        \"orig_w\": orig_w,\n",
        "        \"resized_h\": new_h,\n",
        "        \"resized_w\": new_w,\n",
        "        \"scale\": scale,\n",
        "        \"x_offset\": x_offset,\n",
        "        \"y_offset\": y_offset\n",
        "    })\n",
        "\n",
        "    # Back to (B,H,W,C)\n",
        "    padded = padded.permute(0, 2, 3, 1)\n",
        "\n",
        "    # Ensure mask stays binary if it was one\n",
        "    if is_mask:\n",
        "        padded = (padded > 0.5).float()\n",
        "\n",
        "    return padded, meta\n",
        "\n",
        "\n",
        "\n",
        "def resize_and_pad_to_1024(image, interpolation=\"lanczos\"):\n",
        "\n",
        "    batch_size, orig_h, orig_w, channels = image.shape\n",
        "    target_size = 1024\n",
        "    meta = []\n",
        "\n",
        "    # Switch to (B,C,H,W) for comfy upscale\n",
        "    image_chw = image.permute(0, 3, 1, 2)\n",
        "\n",
        "    if orig_h <= target_size and orig_w <= target_size:\n",
        "        scale = 1.0\n",
        "    else:\n",
        "        scale = target_size / max(orig_h, orig_w)\n",
        "\n",
        "    new_w = int(round(orig_w * scale))\n",
        "    new_h = int(round(orig_h * scale))\n",
        "\n",
        "    resized = common_upscale(\n",
        "        image_chw, new_w, new_h, interpolation, \"disabled\"\n",
        "    )\n",
        "\n",
        "    # Create black padded tensor\n",
        "    padded = torch.zeros(\n",
        "        (batch_size, channels, target_size, target_size),\n",
        "        dtype=image.dtype,\n",
        "        device=image.device\n",
        "    )\n",
        "\n",
        "    y_offset = (target_size - new_h) // 2\n",
        "    x_offset = (target_size - new_w) // 2\n",
        "\n",
        "    padded[:, :, y_offset:y_offset+new_h, x_offset:x_offset+new_w] = resized\n",
        "\n",
        "    # Store metadata for recovery\n",
        "    meta.append({\n",
        "        \"orig_h\": orig_h,\n",
        "        \"orig_w\": orig_w,\n",
        "        \"resized_h\": new_h,\n",
        "        \"resized_w\": new_w,\n",
        "        \"scale\": scale,\n",
        "        \"x_offset\": x_offset,\n",
        "        \"y_offset\": y_offset\n",
        "    })\n",
        "\n",
        "    # Back to (B,H,W,C)\n",
        "    padded = padded.permute(0, 2, 3, 1)\n",
        "    return padded, meta\n",
        "\n",
        "\n",
        "\n",
        "def recover_original_from_1024(padded, meta, interpolation=\"lanczos\"):\n",
        "\n",
        "    batch_size, _, _, channels = padded.shape\n",
        "    padded_chw = padded.permute(0, 3, 1, 2)\n",
        "    recovered_list = []\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        m = meta[i]\n",
        "\n",
        "        # Crop away the padding\n",
        "        cropped = padded_chw[i:i+1, :,\n",
        "                             m[\"y_offset\"]:m[\"y_offset\"]+m[\"resized_h\"],\n",
        "                             m[\"x_offset\"]:m[\"x_offset\"]+m[\"resized_w\"]]\n",
        "\n",
        "        # Resize back to original dims\n",
        "        recovered = common_upscale(\n",
        "            cropped, m[\"orig_w\"], m[\"orig_h\"], interpolation, \"disabled\"\n",
        "        )\n",
        "        recovered_list.append(recovered)\n",
        "\n",
        "    recovered = torch.cat(recovered_list, dim=0)\n",
        "    recovered = recovered.permute(0, 2, 3, 1)  # back to (B,H,W,C)\n",
        "    return recovered\n",
        "\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def download_with_aria2c(link, folder=\"/content/ComfyUI/models/loras\"):\n",
        "    import os\n",
        "\n",
        "    filename = link.split(\"/\")[-1]\n",
        "    command = f\"aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {link} -d {folder} -o {filename}\"\n",
        "\n",
        "    print(\"Executing download command:\")\n",
        "    print(command)\n",
        "\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "    get_ipython().system(command)\n",
        "\n",
        "    return filename\n",
        "\n",
        "\n",
        "\n",
        "def download_civitai_model(civitai_link, civitai_token, folder=\"/content/ComfyUI/models/loras\"):\n",
        "    import os\n",
        "    import time\n",
        "\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        model_id = civitai_link.split(\"/models/\")[1].split(\"?\")[0]\n",
        "    except IndexError:\n",
        "        raise ValueError(\"Invalid Civitai URL format. Please use a link like: https://civitai.com/api/download/models/1523247?...\")\n",
        "\n",
        "    civitai_url = f\"https://civitai.com/api/download/models/{model_id}?type=Model&format=SafeTensor\"\n",
        "    if civitai_token:\n",
        "        civitai_url += f\"&token={civitai_token}\"\n",
        "\n",
        "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"model_{timestamp}.safetensors\"\n",
        "\n",
        "    full_path = os.path.join(folder, filename)\n",
        "\n",
        "    download_command = f\"wget --max-redirect=10 --show-progress \\\"{civitai_url}\\\" -O \\\"{full_path}\\\"\"\n",
        "    print(\"Downloading from Civitai...\")\n",
        "\n",
        "    os.system(download_command)\n",
        "\n",
        "    local_path = os.path.join(folder, filename)\n",
        "    if os.path.exists(local_path) and os.path.getsize(local_path) > 0:\n",
        "        print(f\"LoRA downloaded successfully: {local_path}\")\n",
        "    else:\n",
        "        print(f\"âŒ LoRA download failed or file is empty: {local_path}\")\n",
        "\n",
        "    return filename\n",
        "\n",
        "\n",
        "def download_lora(link, folder=\"/content/ComfyUI/models/loras\", civitai_token=None):\n",
        "    \"\"\"\n",
        "    Download a model file, automatically detecting if it's a Civitai link or huggingface download.\n",
        "\n",
        "    Args:\n",
        "        link: The download URL (either huggingface or Civitai)\n",
        "        folder: Destination folder for the download\n",
        "        civitai_token: Optional token for Civitai downloads (required if link is from Civitai)\n",
        "\n",
        "    Returns:\n",
        "        The filename of the downloaded model\n",
        "    \"\"\"\n",
        "    if \"civitai.com\" in link.lower():\n",
        "        if not civitai_token:\n",
        "            raise ValueError(\"Civitai token is required for Civitai downloads\")\n",
        "        return download_civitai_model(link, civitai_token, folder)\n",
        "    else:\n",
        "        return download_with_aria2c(link, folder)\n",
        "\n",
        "qwen_model_download_url = \"https://huggingface.co/unsloth/Qwen-Image-Edit-2511-GGUF/resolve/main/qwen-image-edit-2511-Q4_K_M.gguf\"# @param {\"type\":\"string\"}\n",
        "qwen_text_encoder_download_url = \"https://huggingface.co/Isi99999/Qwen_image_based_models/resolve/main/qwen_2.5_vl_7b_fp8_scaled.safetensors\"\n",
        "qwen_vae_download_url = \"https://huggingface.co/Isi99999/Qwen_image_based_models/resolve/main/qwen_image_vae.safetensors\"\n",
        "qwen_speedup_lora_download_url = \"https://huggingface.co/lightx2v/Qwen-Image-Lightning/resolve/main/Qwen-Image-Lightning-4steps-V1.0-bf16.safetensors\"# @param {\"type\":\"string\"}\n",
        "\n",
        "download_loRA = False # @param {type:\"boolean\"}\n",
        "qwen_lora_download_url = \"https://civitai.com/api/download/models/2196307?type=Model&format=SafeTensor\"# @param {\"type\":\"string\"}\n",
        "default_LoRA_info = \"Outfit Extractor LoRA. Civitai token required. ðŸŒŸTrigger words : (extract the outfit onto a white background). \"# @param {\"type\":\"string\"}\n",
        "\n",
        "download_loRA_2 = False # @param {type:\"boolean\"}\n",
        "lora_2_download_url = \"\"# @param {\"type\":\"string\"}\n",
        "\n",
        "download_loRA_3 = False # @param {type:\"boolean\"}\n",
        "lora3_download_url = \"\"# @param {\"type\":\"string\"}\n",
        "\n",
        "download_loRA_4 = False # @param {type:\"boolean\"}\n",
        "lora_4_download_url = \"\"# @param {\"type\":\"string\"}\n",
        "\n",
        "token_if_civitai_url = \"\"# @param {\"type\":\"string\"}\n",
        "\n",
        "\n",
        "\n",
        "lora = None\n",
        "if download_loRA:\n",
        "    lora = download_lora(qwen_lora_download_url, civitai_token=token_if_civitai_url)\n",
        "# Validate loRA file extension\n",
        "valid_extensions = {'.safetensors', '.ckpt', '.pt', '.pth', '.sft'}\n",
        "if lora:\n",
        "    if not any(lora.lower().endswith(ext) for ext in valid_extensions):\n",
        "        print(f\"âŒ Invalid LoRA format: {lora}\")\n",
        "        lora = None\n",
        "    else:\n",
        "        clear_output()\n",
        "        print(\"loRA downloaded succesfully!\")\n",
        "\n",
        "lora_2 = None\n",
        "if download_loRA_2:\n",
        "    lora_2 = download_lora(lora_2_download_url, civitai_token=token_if_civitai_url)\n",
        "if lora_2:\n",
        "    if not any(lora_2.lower().endswith(ext) for ext in valid_extensions):\n",
        "        print(f\"âŒ Invalid LoRA format: {lora_2}\")\n",
        "        lora_2 = None\n",
        "    else:\n",
        "        clear_output()\n",
        "        print(\"loRA 2 downloaded succesfully!\")\n",
        "\n",
        "lora_3 = None\n",
        "if download_loRA_3:\n",
        "    lora_3 = download_lora(lora3_download_url, civitai_token=token_if_civitai_url)\n",
        "if lora_3:\n",
        "    if not any(lora_3.lower().endswith(ext) for ext in valid_extensions):\n",
        "        print(f\"âŒ Invalid LoRA format: {lora_3}\")\n",
        "        lora_3 = None\n",
        "    else:\n",
        "        clear_output()\n",
        "        print(\"loRA 3 downloaded succesfully!\")\n",
        "\n",
        "lora_4 = None\n",
        "if download_loRA_4:\n",
        "    lora_4 = download_lora(lora_4_download_url, civitai_token=token_if_civitai_url)\n",
        "if lora_4:\n",
        "    if not any(lora_4.lower().endswith(ext) for ext in valid_extensions):\n",
        "        print(f\"âŒ Invalid LoRA format: {lora_4}\")\n",
        "        lora_4 = None\n",
        "    else:\n",
        "        clear_output()\n",
        "        print(\"loRA 4 downloaded succesfully!\")\n",
        "\n",
        "\n",
        "def model_download(url: str, dest_dir: str, filename: str = None, silent: bool = True) -> bool:\n",
        "    \"\"\"\n",
        "    Colab-optimized download with aria2c\n",
        "\n",
        "    Args:\n",
        "        url: Download URL\n",
        "        dest_dir: Target directory (will be created if needed)\n",
        "        filename: Optional output filename (defaults to URL filename)\n",
        "        silent: If True, suppresses all output (except errors)\n",
        "\n",
        "    Returns:\n",
        "        bool: True if successful, False if failed\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create destination directory\n",
        "        Path(dest_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Set filename if not specified\n",
        "        if filename is None:\n",
        "            filename = url.split('/')[-1].split('?')[0]  # Remove URL parameters\n",
        "\n",
        "        # Build command\n",
        "        cmd = [\n",
        "            'aria2c',\n",
        "            '--console-log-level=error',\n",
        "            '-c', '-x', '16', '-s', '16', '-k', '1M',\n",
        "            '-d', dest_dir,\n",
        "            '-o', filename,\n",
        "            url\n",
        "        ]\n",
        "\n",
        "        # Add silent flags if requested\n",
        "        if silent:\n",
        "            cmd.extend(['--summary-interval=0', '--quiet'])\n",
        "            print(f\"Downloading {filename}...\", end=' ', flush=True)\n",
        "\n",
        "        # Run download\n",
        "        result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
        "\n",
        "        if silent:\n",
        "            print(\"Done!\")\n",
        "        else:\n",
        "            print(f\"Downloaded {filename} to {dest_dir}\")\n",
        "        return filename\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        error = e.stderr.strip() or \"Unknown error\"\n",
        "        print(f\"\\nError downloading {filename}: {error}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "flux_model = model_download(qwen_model_download_url, \"/content/ComfyUI/models/unet\")\n",
        "flux_vae = model_download(qwen_vae_download_url, \"/content/ComfyUI/models/vae\")\n",
        "flux_t5xxl = model_download(qwen_text_encoder_download_url, \"/content/ComfyUI/models/clip\")\n",
        "flux_1_turbo = model_download(qwen_speedup_lora_download_url, \"/content/ComfyUI/models/loras\")\n",
        "\n",
        "\n",
        "\n",
        "paint = Painting()\n",
        "\n",
        "def clear_memory():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()\n",
        "    for obj in list(globals().values()):\n",
        "        if torch.is_tensor(obj) or (hasattr(obj, \"data\") and torch.is_tensor(obj.data)):\n",
        "            del obj\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "def save_as_image(image, filename_prefix, output_dir=\"/content/ComfyUI/output\"):\n",
        "    \"\"\"Save single frame as PNG image.\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_path = f\"{output_dir}/{filename_prefix}.png\"\n",
        "\n",
        "    frame = (image.cpu().numpy() * 255).astype(np.uint8)\n",
        "\n",
        "    Image.fromarray(frame).save(output_path)\n",
        "\n",
        "    return output_path\n",
        "\n",
        "def save_as_image2(image, filename_prefix, output_dir=\"/content/ComfyUI/output\"):\n",
        "    \"\"\"Save single frame as PNG image.\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_path = f\"{output_dir}/{filename_prefix}.png\"\n",
        "\n",
        "    if isinstance(image, torch.Tensor):\n",
        "        image = image.cpu().numpy()\n",
        "    if image.ndim == 4:  # Batch dimension\n",
        "        image = image[0]\n",
        "    if image.shape[0] == 3:  # CHW to HWC\n",
        "        image = np.transpose(image, (1, 2, 0))\n",
        "    image = (image * 255).astype(np.uint8)\n",
        "\n",
        "    Image.fromarray(image).save(output_path)\n",
        "    return output_path\n",
        "\n",
        "\n",
        "def save_as_mp4(images, filename_prefix, fps, output_dir=\"/content/ComfyUI/output\"):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_path = f\"{output_dir}/{filename_prefix}.mp4\"\n",
        "\n",
        "    frames = []\n",
        "    for i, img in enumerate(images):\n",
        "        try:\n",
        "\n",
        "            if isinstance(img, torch.Tensor):\n",
        "                img = img.cpu().numpy()\n",
        "\n",
        "            # print(f\"Frame {i} initial shape: {img.shape}, dtype: {img.dtype}, max: {img.max()}\")  # Debug\n",
        "\n",
        "\n",
        "            if img.max() <= 1.0:\n",
        "                img = (img * 255).astype(np.uint8)\n",
        "            else:\n",
        "                img = img.astype(np.uint8)\n",
        "\n",
        "\n",
        "            if len(img.shape) == 4:  # Batch dimension? (N, C, H, W)\n",
        "                img = img[0]  # Take first image in batch\n",
        "\n",
        "            if len(img.shape) == 3:\n",
        "                if img.shape[0] in (1, 3, 4):  # CHW format\n",
        "                    img = np.transpose(img, (1, 2, 0))\n",
        "                elif img.shape[2] > 4:  # Too many channels\n",
        "                    img = img[:, :, :3]\n",
        "            elif len(img.shape) == 2:\n",
        "                img = np.expand_dims(img, axis=-1)\n",
        "\n",
        "            # print(f\"Frame {i} processed shape: {img.shape}\")  # Debug\n",
        "\n",
        "            # Final validation\n",
        "            if len(img.shape) != 3 or img.shape[2] not in (1, 3, 4):\n",
        "                raise ValueError(f\"Invalid frame shape after processing: {img.shape}\")\n",
        "\n",
        "            frames.append(img)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing frame {i}: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    try:\n",
        "        with imageio.get_writer(output_path, fps=fps) as writer:\n",
        "            for i, frame in enumerate(frames):\n",
        "                # print(f\"Writing frame {i} with shape: {frame.shape}\")  # Debug\n",
        "                writer.append_data(frame)\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing video: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "    return output_path\n",
        "\n",
        "import cv2\n",
        "import shutil\n",
        "from IPython.display import Video\n",
        "import datetime\n",
        "\n",
        "\n",
        "def upload_file():\n",
        "    \"\"\"Handle file upload (image or video) and return paths.\"\"\"\n",
        "    os.makedirs('/content/ComfyUI/input', exist_ok=True)\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    paths = []\n",
        "    for filename in uploaded.keys():\n",
        "        src_path = f'/content/ComfyUI/{filename}'\n",
        "        dest_path = f'/content/ComfyUI/input/{filename}'\n",
        "        shutil.move(src_path, dest_path)\n",
        "        paths.append(dest_path)\n",
        "        print(f\"File saved to: {dest_path}\")\n",
        "\n",
        "    return paths[0] if paths else None\n",
        "\n",
        "def display_video(video_path):\n",
        "    from IPython.display import HTML\n",
        "    from base64 import b64encode\n",
        "\n",
        "    video_data = open(video_path,'rb').read()\n",
        "\n",
        "    # Determine MIME type based on file extension\n",
        "    if video_path.lower().endswith('.mp4'):\n",
        "        mime_type = \"video/mp4\"\n",
        "    elif video_path.lower().endswith('.webm'):\n",
        "        mime_type = \"video/webm\"\n",
        "    elif video_path.lower().endswith('.webp'):\n",
        "        mime_type = \"image/webp\"\n",
        "    else:\n",
        "        mime_type = \"video/mp4\"  # default\n",
        "\n",
        "    data_url = f\"data:{mime_type};base64,\" + b64encode(video_data).decode()\n",
        "\n",
        "    display(HTML(f\"\"\"\n",
        "    <video width=512 controls autoplay loop>\n",
        "        <source src=\"{data_url}\" type=\"{mime_type}\">\n",
        "    </video>\n",
        "    \"\"\"))\n",
        "\n",
        "outputImagePath = None\n",
        "\n",
        "def edit_input(\n",
        "    image_path: str = None,\n",
        "    use_inpainting: bool = False,\n",
        "    mask_path: str = None,\n",
        "    meta_file_from_mask: str = None,\n",
        "    image_path2: str = None,\n",
        "    image_path3: str = None,\n",
        "    positive_prompt: str = \"\",\n",
        "    negative_prompt: str = \"\",\n",
        "    guidance: float = 2.5,\n",
        "    convert_image_3_to_pose: bool = False,\n",
        "    change_resolution: bool = False,\n",
        "    width: int = 832,\n",
        "    height: int = 480,\n",
        "    change_output_resolution: bool = False,\n",
        "    width_out: int = 1024,\n",
        "    height_out: int = 1024,\n",
        "    pad_output_to_1024_by_1024 = False,\n",
        "    resize_final_output: bool = False,\n",
        "    r_width: int = 832,\n",
        "    r_height: int = 480,\n",
        "    seed: int = 0,\n",
        "    steps: int = 20,\n",
        "    cfg: float = 1.0,\n",
        "    sampler_name: str = \"euler\",\n",
        "    scheduler: str = \"simple\",\n",
        "    denoise: float = 1.0,\n",
        "    use_turbo_lora: bool = False,\n",
        "    use_lora: bool = False,\n",
        "    LoRA_Strength: float = 1.0,\n",
        "    use_lora2: bool = False,\n",
        "    LoRA2_Strength: float = 1.0,\n",
        "    use_lora3: bool = False,\n",
        "    LoRA3_Strength: float = 1.0,\n",
        "    use_lora4: bool = False,\n",
        "    LoRA4_Strength: float = 1.0,\n",
        "    overwrite: bool = False\n",
        "\n",
        "):\n",
        "\n",
        "    with torch.inference_mode():\n",
        "\n",
        "        clip_loader = CLIPLoader()\n",
        "        unet_loader =  UnetLoaderGGUF()\n",
        "        model_sampling = ModelSamplingSD3()\n",
        "        text_encode_qwen_image_edit = TextEncodeQwenImageEditPlus()\n",
        "        # unet_loader =  UNETLoader()\n",
        "        vae_loader =   VAELoader()\n",
        "        vae_encode = VAEEncode()\n",
        "        # vae_encode_inpaint = VAEEncodeForInpaint()\n",
        "        vae_decode = VAEDecode()\n",
        "        ksampler = KSampler()\n",
        "        load_lora = LoraLoaderModelOnly()\n",
        "        load_turbo_lora = LoraLoaderModelOnly()\n",
        "        load_image = LoadImage()\n",
        "        load_image2 = LoadImage()\n",
        "        save_image = SaveImage()\n",
        "        positive_prompt_encode = CLIPTextEncode()\n",
        "        negative_prompt_encode = CLIPTextEncode()\n",
        "        dw_pose_estimator = DWPose_Preprocessor()\n",
        "        inpaint_model_conditioning = InpaintModelConditioning()\n",
        "        empty_latent_image = EmptySD3LatentImage()\n",
        "        # flux_guidance = FluxGuidance()\n",
        "        # flux_kontext_scale = FluxKontextImageScale()\n",
        "        # image_stitch = ImageStitch()\n",
        "        # reference_latent = ReferenceLatent()\n",
        "        imageScaleToTotalPixels = ImageScaleToTotalPixels()\n",
        "        # umage_resize = ImageResizeKJ()\n",
        "        # solid_mask = SolidMask()\n",
        "        image_scaler1 = ImageScale()\n",
        "        image_scaler = ImageScale()\n",
        "        image_scale_by = ImageScaleBy()\n",
        "        # image_crop = ImageCrop()\n",
        "        cfg_norm = CFGNorm()\n",
        "\n",
        "\n",
        "        clip=None\n",
        "        # print(\"Loading Text_Encoder...\")\n",
        "        # clip = clip_loader.load_clip(flux_t5xxl, type=\"qwen_image\", device=\"cuda\")[0]\n",
        "        # if hasattr(clip, \"to\"):\n",
        "        #     clip = clip.to(\"cuda\")\n",
        "        # elif hasattr(clip, \"model\"):\n",
        "        #     clip.model = clip.model.to(\"cuda\")\n",
        "        # else:\n",
        "        #     for attr in dir(clip):\n",
        "        #         obj = getattr(clip, attr)\n",
        "        #         if hasattr(obj, \"to\"):\n",
        "        #             try:\n",
        "        #                 obj.to(\"cuda\")\n",
        "        #                 print(f\"Moved {attr} to CUDA\")\n",
        "        #             except:\n",
        "        #                 pass\n",
        "\n",
        "\n",
        "\n",
        "        print(\"Loading VAE...\")\n",
        "        vae = vae_loader.load_vae(flux_vae)[0]\n",
        "\n",
        "\n",
        "        if image_path is not None:\n",
        "            image = load_image.load_image(image_path)[0]\n",
        "            if change_resolution and use_inpainting is False:\n",
        "                    image = image_scaler1.upscale(\n",
        "                        image,\n",
        "                        \"lanczos\",\n",
        "                        width,\n",
        "                        height,\n",
        "                        \"disabled\"\n",
        "                    )[0]\n",
        "\n",
        "            if use_inpainting is False:\n",
        "                image, meta = resize_and_pad_to_1024(image)\n",
        "\n",
        "            if use_inpainting and mask_path is not None:\n",
        "                mask = load_mask_for_inpaint(image, mask_path)\n",
        "                meta = meta_file_from_mask\n",
        "\n",
        "\n",
        "                # mask = mask.permute(0, 2, 3, 1)  # Convert to [B,H,W,1]\n",
        "                # mask, _ = resize_and_pad_to_1024_maskFit(mask, is_mask=True)\n",
        "                # mask = mask.permute(0, 3, 1, 2)  # Back to [B,1,H,W]\n",
        "            if use_inpainting is False:\n",
        "                image = imageScaleToTotalPixels.execute(image, \"lanczos\", 1)[0]\n",
        "\n",
        "            if image_path2 is not None:\n",
        "                image2 = load_image2.load_image(image_path2)[0]\n",
        "                image2, meta2 = resize_and_pad_to_1024(image2)\n",
        "                # width_int, height_int = image_width_height(image)\n",
        "                # width_int2, height_int2 = image_width_height(image2)\n",
        "                # if width_int != width_int2 or height_int != height_int2:\n",
        "                #     image2 = image_scaler.upscale(\n",
        "                #         image2,\n",
        "                #         \"lanczos\",\n",
        "                #         width_int,\n",
        "                #         height_int,\n",
        "                #         \"disabled\"\n",
        "                #     )[0]\n",
        "\n",
        "                # image = image_stitch.stitch(\n",
        "                #         image,\n",
        "                #         \"right\",\n",
        "                #         True,\n",
        "                #         0,\n",
        "                #         \"white\",\n",
        "                #         image2,\n",
        "                #     )[0]\n",
        "\n",
        "                # image = flux_kontext_scale.scale(image)[0]\n",
        "            else:\n",
        "                image2 = None\n",
        "\n",
        "            if image_path3 is not None:\n",
        "                image3 = load_image.load_image(image_path3)[0]\n",
        "                image3, meta3 = resize_and_pad_to_1024(image3)\n",
        "                if convert_image_3_to_pose:\n",
        "                    result_dict = dw_pose_estimator.estimate_pose(image3)\n",
        "                    image3 = result_dict[\"result\"][0]\n",
        "                    dw_image = image_scaler.upscale(\n",
        "                        image3,\n",
        "                        \"lanczos\",\n",
        "                        width//4,\n",
        "                        height//4,\n",
        "                        \"disabled\"\n",
        "                    )[0]\n",
        "                    dw_path = save_as_image2(dw_image, \"dwpose image\")\n",
        "                    # display(IPImage(filename=dw_path))\n",
        "\n",
        "\n",
        "\n",
        "            else:\n",
        "                image3 = None\n",
        "\n",
        "            print(\"Loading Text_Encoder...\")\n",
        "            clip = clip_loader.load_clip(flux_t5xxl, type=\"qwen_image\", device=\"cuda\")[0]\n",
        "            if hasattr(clip, \"to\"):\n",
        "                clip = clip.to(\"cuda\")\n",
        "            elif hasattr(clip, \"model\"):\n",
        "                clip.model = clip.model.to(\"cuda\")\n",
        "            else:\n",
        "                for attr in dir(clip):\n",
        "                    obj = getattr(clip, attr)\n",
        "                    if hasattr(obj, \"to\"):\n",
        "                        try:\n",
        "                            obj.to(\"cuda\")\n",
        "                            print(f\"Moved {attr} to CUDA\")\n",
        "                        except:\n",
        "                            pass\n",
        "\n",
        "            positive_prompt = text_encode_qwen_image_edit.encode(clip, positive_prompt, vae=vae, image1=image, image2=image2, image3=image3)[0]\n",
        "            negative_prompt = text_encode_qwen_image_edit.encode(clip, negative_prompt, vae=vae,image1=image, image2=image2, image3=image3)[0]\n",
        "\n",
        "            del clip\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "\n",
        "            if use_inpainting and mask_path is not None:\n",
        "                # latent = vae_encode_inpaint.encode(vae, image, mask)[0]\n",
        "                print(\"Preparing inpainting...\")\n",
        "                positive_prompt, negative_prompt, latent = inpaint_model_conditioning.encode(positive_prompt, negative_prompt, image, vae, mask)\n",
        "            else:\n",
        "                latent = vae_encode.encode(vae, image)[0]\n",
        "\n",
        "        else:\n",
        "            print(\"Loading Text_Encoder...\")\n",
        "            clip = clip_loader.load_clip(flux_t5xxl, type=\"qwen_image\", device=\"cuda\")[0]\n",
        "\n",
        "            print(\"You did not upload the main image, so an image will be generated instead.\")\n",
        "            positive_prompt = positive_prompt_encode.encode(clip, positive_prompt)[0]\n",
        "            negative_prompt = negative_prompt_encode.encode(clip, negative_prompt)[0]\n",
        "            latent = empty_latent_image.generate(width, height, 1)[0]\n",
        "\n",
        "            del clip\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "        base_name = \"generated_Image\"\n",
        "        if not overwrite:\n",
        "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            base_name += f\"_{timestamp}\"\n",
        "\n",
        "        try:\n",
        "            print(\"Loading Unet Model...\")\n",
        "            model = unet_loader.load_unet(flux_model)[0]\n",
        "\n",
        "            model = model_sampling.patch(model, guidance, 1)[0]\n",
        "\n",
        "            usedcfg = cfg\n",
        "            usedSteps = steps\n",
        "\n",
        "            if use_turbo_lora:\n",
        "                print(\"Loading speedup Lora...\")\n",
        "                model = load_turbo_lora.load_lora_model_only(model, flux_1_turbo, 1)[0]\n",
        "                usedSteps = 4\n",
        "                usedcfg=1\n",
        "\n",
        "\n",
        "            if use_lora and lora is not None:\n",
        "                print(\"Loading Lora...\")\n",
        "                model = load_lora.load_lora_model_only(model, lora, LoRA_Strength)[0]\n",
        "\n",
        "            if use_lora2 and lora_2 is not None:\n",
        "                print(\"Loading Lora...\")\n",
        "                model = load_lora.load_lora_model_only(model, lora_2, LoRA2_Strength)[0]\n",
        "\n",
        "            if use_lora3 and lora_3 is not None:\n",
        "                print(\"Loading Lora...\")\n",
        "                model = load_lora.load_lora_model_only(model, lora_3, LoRA3_Strength)[0]\n",
        "\n",
        "            if use_lora4 and lora_4 is not None:\n",
        "                print(\"Loading Lora...\")\n",
        "                model = load_lora.load_lora_model_only(model, lora_4, LoRA4_Strength)[0]\n",
        "\n",
        "            model = cfg_norm.execute(model, 1)[0]\n",
        "\n",
        "            if change_output_resolution:\n",
        "                latent = empty_latent_image.generate(width_out, height_out, 1)[0]\n",
        "\n",
        "            clear_output()\n",
        "\n",
        "            if image_path3 is not None:\n",
        "                if convert_image_3_to_pose:\n",
        "                    print(\"Your character will assume this pose:\")\n",
        "                    display(IPImage(filename=dw_path))\n",
        "\n",
        "            print(\"Editing image...\")\n",
        "            # image_out_latent = ksampler.sample(\n",
        "            #     model=model,\n",
        "            #     add_noise=\"enable\",\n",
        "            #     noise_seed=seed,\n",
        "            #     steps=usedSteps,\n",
        "            #     cfg=usedcfg,\n",
        "            #     sampler_name=sampler_name,\n",
        "            #     scheduler=scheduler,\n",
        "            #     positive=positive_prompt,\n",
        "            #     negative=negative_prompt,\n",
        "            #     latent_image=latent,\n",
        "            #     start_at_step=0,\n",
        "            #     end_at_step=1000,\n",
        "            #     return_with_leftover_noise=\"disable\"\n",
        "\n",
        "            # )[0]\n",
        "\n",
        "            image_out_latent = ksampler.sample(\n",
        "                    model=model,\n",
        "                    seed=seed,\n",
        "                    steps=usedSteps,\n",
        "                    cfg=usedcfg,\n",
        "                    sampler_name=sampler_name,\n",
        "                    scheduler=scheduler,\n",
        "                    positive=positive_prompt,\n",
        "                    negative=negative_prompt,\n",
        "                    latent_image=latent,\n",
        "                    denoise=1.00\n",
        "                )[0]\n",
        "\n",
        "            del positive_prompt\n",
        "            del negative_prompt\n",
        "\n",
        "\n",
        "            del model\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            print(\"Decoding latents...\")\n",
        "            decoded = vae_decode.decode(vae, image_out_latent)[0]\n",
        "\n",
        "            if pad_output_to_1024_by_1024 is False:\n",
        "                # if image_path is not None:\n",
        "                if image_path is not None:\n",
        "                    decoded = recover_original_from_1024(decoded, meta)\n",
        "\n",
        "\n",
        "            if resize_final_output:\n",
        "                    decoded = image_scaler1.upscale(\n",
        "                        decoded,\n",
        "                        \"lanczos\",\n",
        "                        r_width,\n",
        "                        r_height,\n",
        "                        \"disabled\"\n",
        "                    )[0]\n",
        "\n",
        "            del vae\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "\n",
        "\n",
        "            # only_result_image = image_crop.crop(decoded,width_int, height_int,width_int,0)[0]\n",
        "\n",
        "            global outputImagePath\n",
        "            base_name = \"ComfyUI\"\n",
        "            if not overwrite:\n",
        "                timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "                base_name += f\"_{timestamp}\"\n",
        "\n",
        "            outputImagePath = save_as_image(decoded[0], base_name)\n",
        "            display(IPImage(filename=outputImagePath))\n",
        "\n",
        "            del decoded\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during Image Editing/saving: {str(e)}\")\n",
        "            raise\n",
        "        finally:\n",
        "            clear_memory()\n",
        "\n",
        "\n",
        "clear_output()\n",
        "\n",
        "print(\"âœ… Environment Setup Complete!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown # Upload Image 1\n",
        "\n",
        "file_uploaded = upload_file()\n",
        "display_upload = True # @param {type:\"boolean\"}\n",
        "if display_upload:\n",
        "    if file_uploaded.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "        display(IPImage(filename=file_uploaded))\n",
        "    else:\n",
        "        print(\"Image format cannnot be displayed.\")\n",
        "recent_upload = True"
      ],
      "metadata": {
        "cellView": "form",
        "id": "dOS6D7vMaAEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown # ðŸ’¥3. Launch Mask Editor for Image 1 & Save after Applying Mask for Inpainting (Experimental)\n",
        "from google.colab import output\n",
        "import base64, io\n",
        "\n",
        "\n",
        "file_uploaded  = globals().get(\"file_uploaded\", None)\n",
        "\n",
        "if file_uploaded is None:\n",
        "    print(\"You need to upload Image 1 before launching the mask editor. \")\n",
        "\n",
        "\n",
        "edit_recent_output_image=False # @param {type:\"boolean\"}\n",
        "display_image_with_mask=True # @param {type:\"boolean\"}\n",
        "if edit_recent_output_image and outputImagePath is not None:\n",
        "    file_uploaded = outputImagePath\n",
        "if recent_upload:\n",
        "    load_image_file = LoadImage()\n",
        "    image_file = load_image_file.load_image(file_uploaded)[0]\n",
        "\n",
        "    image_file, meta_file = resize_and_pad_to_1024(image_file)\n",
        "    file_uploaded = save_as_image2(image_file, \"image_1024_file\")\n",
        "    recent_upload = False\n",
        "\n",
        "\n",
        "\n",
        "mask_path = \" \"\n",
        "\n",
        "with open(file_uploaded, \"rb\") as f:\n",
        "    b64_img = base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "\n",
        "html = f\"\"\"\n",
        "<div id=\"editor\" style=\"font-family:system-ui, -apple-system, Segoe UI, Roboto, sans-serif; text-align:center;\">\n",
        "  <h3>ðŸ–Œï¸ Mask Editor</h3>\n",
        "\n",
        "  <div style=\"margin:8px 0; display:flex; gap:12px; justify-content:center; align-items:center; flex-wrap:wrap;\">\n",
        "    <label>Brush color:\n",
        "      <input id=\"colorPicker\" type=\"color\" value=\"#808080\" />\n",
        "    </label>\n",
        "    <label>Brush size:\n",
        "      <input id=\"sizeRange\" type=\"range\" min=\"3\" max=\"100\" value=\"20\" />\n",
        "      <span id=\"sizeVal\">20</span> px\n",
        "    </label>\n",
        "    <label>Opacity:\n",
        "      <input id=\"opacityRange\" type=\"range\" min=\"10\" max=\"100\" value=\"50\" />\n",
        "      <span id=\"opacityVal\">0.50</span>\n",
        "    </label>\n",
        "    <button id=\"modeBtn\" title=\"Toggle draw/erase\">âœï¸ Draw</button>\n",
        "    <button id=\"clearBtn\" title=\"Clear strokes\">ðŸ§¹ Clear</button>\n",
        "    <button id=\"saveBtn\" style=\"font-weight:600;\">ðŸ’¾ Save Mask</button>\n",
        "  </div>\n",
        "\n",
        "  <!-- Three layers: base image, drawing strokes, brush preview -->\n",
        "  <div style=\"display:inline-block; position:relative; border:1px solid #000;\">\n",
        "    <canvas id=\"imgCanvas\" style=\"display:block;\"></canvas>\n",
        "    <canvas id=\"drawCanvas\" style=\"position:absolute; left:0; top:0;\"></canvas>\n",
        "    <canvas id=\"brushCanvas\" style=\"position:absolute; left:0; top:0; pointer-events:none;\"></canvas>\n",
        "  </div>\n",
        "</div>\n",
        "\n",
        "<script>\n",
        "(() => {{\n",
        "  const base64Image = \"data:image/png;base64,{b64_img}\";\n",
        "  const imgCanvas = document.getElementById('imgCanvas');\n",
        "  const drawCanvas = document.getElementById('drawCanvas');\n",
        "  const brushCanvas = document.getElementById('brushCanvas');\n",
        "  const imgCtx = imgCanvas.getContext('2d');\n",
        "  const drawCtx = drawCanvas.getContext('2d');\n",
        "  const brushCtx = brushCanvas.getContext('2d');\n",
        "\n",
        "  // Controls\n",
        "  const colorPicker = document.getElementById('colorPicker');\n",
        "  const sizeRange = document.getElementById('sizeRange');\n",
        "  const sizeVal = document.getElementById('sizeVal');\n",
        "  const opacityRange = document.getElementById('opacityRange');\n",
        "  const opacityVal = document.getElementById('opacityVal');\n",
        "  const modeBtn = document.getElementById('modeBtn');\n",
        "  const clearBtn = document.getElementById('clearBtn');\n",
        "  const saveBtn = document.getElementById('saveBtn');\n",
        "\n",
        "  let drawing = false;\n",
        "  let mode = 'draw'; // 'draw' | 'erase'\n",
        "  let brushSize = parseInt(sizeRange.value, 10);\n",
        "  let brushColor = colorPicker.value;\n",
        "  let brushOpacity = parseInt(opacityRange.value, 10) / 100;\n",
        "\n",
        "  sizeVal.textContent = brushSize;\n",
        "  opacityVal.textContent = brushOpacity.toFixed(2);\n",
        "\n",
        "  // Load base image\n",
        "  const img = new Image();\n",
        "  img.src = base64Image;\n",
        "  img.onload = () => {{\n",
        "    [imgCanvas, drawCanvas, brushCanvas].forEach(c => {{\n",
        "      c.width = img.width;\n",
        "      c.height = img.height;\n",
        "    }});\n",
        "    imgCtx.drawImage(img, 0, 0);\n",
        "    drawCtx.lineCap = 'round';\n",
        "    drawCtx.lineJoin = 'round';\n",
        "  }};\n",
        "\n",
        "  function rgbaFromHex(hex, a) {{\n",
        "    const h = hex.replace('#', '');\n",
        "    const r = parseInt(h.substring(0,2), 16);\n",
        "    const g = parseInt(h.substring(2,4), 16);\n",
        "    const b = parseInt(h.substring(4,6), 16);\n",
        "    return `rgba(${{r}}, ${{g}}, ${{b}}, ${{a}})`;\n",
        "  }}\n",
        "\n",
        "  function getPos(e) {{\n",
        "    const rect = drawCanvas.getBoundingClientRect();\n",
        "    return {{ x: e.clientX - rect.left, y: e.clientY - rect.top }};\n",
        "  }}\n",
        "\n",
        "  function pointerDown(e) {{\n",
        "    e.preventDefault();\n",
        "    drawing = true;\n",
        "    const p = getPos(e);\n",
        "    drawCtx.beginPath();\n",
        "    drawCtx.moveTo(p.x, p.y);\n",
        "    drawStroke(e);\n",
        "  }}\n",
        "\n",
        "  function pointerMove(e) {{\n",
        "    const p = getPos(e);\n",
        "\n",
        "    // Update brush preview\n",
        "    brushCtx.clearRect(0, 0, brushCanvas.width, brushCanvas.height);\n",
        "    brushCtx.strokeStyle = brushColor;\n",
        "    brushCtx.globalAlpha = 1.0;\n",
        "    brushCtx.lineWidth = 2;\n",
        "    brushCtx.beginPath();\n",
        "    brushCtx.arc(p.x, p.y, brushSize/2, 0, Math.PI*2);\n",
        "    brushCtx.stroke();\n",
        "\n",
        "    if (drawing) drawStroke(e);\n",
        "  }}\n",
        "\n",
        "  function pointerUp() {{\n",
        "    drawing = false;\n",
        "  }}\n",
        "\n",
        "  function drawStroke(e) {{\n",
        "    const p = getPos(e);\n",
        "    if (mode === 'erase') {{\n",
        "      drawCtx.globalCompositeOperation = 'destination-out';\n",
        "      drawCtx.strokeStyle = 'rgba(0,0,0,1)';\n",
        "    }} else {{\n",
        "      drawCtx.globalCompositeOperation = 'source-over';\n",
        "      drawCtx.strokeStyle = rgbaFromHex(brushColor, brushOpacity);\n",
        "    }}\n",
        "    drawCtx.lineWidth = brushSize;\n",
        "    drawCtx.lineTo(p.x, p.y);\n",
        "    drawCtx.stroke();\n",
        "    drawCtx.beginPath();\n",
        "    drawCtx.moveTo(p.x, p.y);\n",
        "  }}\n",
        "\n",
        "  // Events\n",
        "  drawCanvas.addEventListener('pointerdown', pointerDown);\n",
        "  drawCanvas.addEventListener('pointermove', pointerMove);\n",
        "  window.addEventListener('pointerup', pointerUp);\n",
        "  drawCanvas.addEventListener('pointerleave', pointerUp);\n",
        "\n",
        "  // UI Controls\n",
        "  colorPicker.addEventListener('input', e => brushColor = e.target.value);\n",
        "  sizeRange.addEventListener('input', e => {{\n",
        "    brushSize = parseInt(e.target.value, 10);\n",
        "    sizeVal.textContent = brushSize;\n",
        "  }});\n",
        "  opacityRange.addEventListener('input', e => {{\n",
        "    brushOpacity = parseInt(e.target.value, 10) / 100;\n",
        "    opacityVal.textContent = brushOpacity.toFixed(2);\n",
        "  }});\n",
        "  modeBtn.addEventListener('click', () => {{\n",
        "    mode = (mode === 'draw') ? 'erase' : 'draw';\n",
        "    modeBtn.textContent = (mode === 'draw') ? 'âœï¸ Draw' : 'ðŸ§½ Erase';\n",
        "  }});\n",
        "  clearBtn.addEventListener('click', () => drawCtx.clearRect(0, 0, drawCanvas.width, drawCanvas.height));\n",
        "\n",
        "  // Save mask + overlay\n",
        "  saveBtn.addEventListener('click', () => {{\n",
        "    const overlayCanvas = document.createElement('canvas');\n",
        "    overlayCanvas.width = imgCanvas.width;\n",
        "    overlayCanvas.height = imgCanvas.height;\n",
        "    const octx = overlayCanvas.getContext('2d');\n",
        "    octx.drawImage(imgCanvas, 0, 0);\n",
        "    octx.drawImage(drawCanvas, 0, 0);\n",
        "    const overlayDataURL = overlayCanvas.toDataURL('image/png');\n",
        "\n",
        "    const maskCanvas = document.createElement('canvas');\n",
        "    maskCanvas.width = drawCanvas.width;\n",
        "    maskCanvas.height = drawCanvas.height;\n",
        "    const mctx = maskCanvas.getContext('2d');\n",
        "    const strokeData = drawCtx.getImageData(0, 0, drawCanvas.width, drawCanvas.height);\n",
        "    const d = strokeData.data;\n",
        "    const thresh = 10;\n",
        "    for (let i = 0; i < d.length; i += 4) {{\n",
        "      const a = d[i+3];\n",
        "      if (a > thresh) {{\n",
        "        d[i]=255; d[i+1]=255; d[i+2]=255; d[i+3]=255;\n",
        "      }} else {{\n",
        "        d[i]=0; d[i+1]=0; d[i+2]=0; d[i+3]=255;\n",
        "      }}\n",
        "    }}\n",
        "    mctx.putImageData(strokeData,0,0);\n",
        "    const maskDataURL = maskCanvas.toDataURL('image/png');\n",
        "\n",
        "    google.colab.kernel.invokeFunction('notebook.save_mask_js',\n",
        "      [overlayDataURL, maskDataURL], {{}});\n",
        "\n",
        "    document.getElementById('editor').innerHTML = \"<p><b>âœ… Mask saved!</b></p>\";\n",
        "  }});\n",
        "}})();\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "def _decode_data_url_to_pil(data_url: str) -> Image.Image:\n",
        "  header, b64 = data_url.split(',', 1)\n",
        "  return Image.open(io.BytesIO(base64.b64decode(b64)))\n",
        "\n",
        "def save_mask_js(overlay_data_url: str, mask_data_url: str):\n",
        "  overlay = _decode_data_url_to_pil(overlay_data_url).convert(\"RGBA\")\n",
        "  mask = _decode_data_url_to_pil(mask_data_url).convert(\"L\")\n",
        "  global mask_path\n",
        "  global display_image_with_mask\n",
        "  overlay.save(\"/content/overlay.png\")\n",
        "  # global mask_path\n",
        "  mask_path = \"/content/mask.png\"\n",
        "  mask.save(mask_path)\n",
        "  # print(\"âœ… Saved: /content/overlay.png\")\n",
        "  if display_image_with_mask:\n",
        "      display(overlay)\n",
        "  # print(\"âœ… Saved: /content/mask.png\")\n",
        "  # display(mask)\n",
        "\n",
        "output.register_callback(\"notebook.save_mask_js\", save_mask_js)\n",
        "display(HTML(html))\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fn-A2AirLOSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown # Upload Image 2 (optional)\n",
        "\n",
        "file_uploaded2 = upload_file()\n",
        "display_upload = False # @param {type:\"boolean\"}\n",
        "if display_upload:\n",
        "    if file_uploaded2.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "        display(IPImage(filename=file_uploaded2))\n",
        "    else:\n",
        "        print(\"Image format cannnot be displayed.\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "zb4UUhuHtLRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown # Upload Image 3 (optional)\n",
        "\n",
        "file_uploaded3 = upload_file()\n",
        "display_upload = False # @param {type:\"boolean\"}\n",
        "if display_upload:\n",
        "    if file_uploaded3.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "        display(IPImage(filename=file_uploaded3))\n",
        "    else:\n",
        "        print(\"Image format cannnot be displayed.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Z30L6OHWY6bD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title {\"single-column\":true}\n",
        "# @markdown # ðŸ’¥2. Edit Image\n",
        "\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "positive_prompt=\"Change the image to vector style while maintaining the positions and proportions of the people and objects in the image\" # @param {\"type\":\"string\"}\n",
        "negative_prompt=\"jpeg compression\" # @param {\"type\":\"string\"}\n",
        "shift=3 # @param {\"type\":\"slider\",\"min\":0.0,\"max\":100.0,\"step\":0.1}\n",
        "# @markdown ---\n",
        "# @markdown ### Image Settings\n",
        "# generate_image=False # @param {type:\"boolean\"}\n",
        "# generate_image=False\n",
        "use_inpainting=False # @param {type:\"boolean\"}\n",
        "convert_image_3_to_pose=False # @param {type:\"boolean\"}\n",
        "do_not_use_uploaded_image1=False # @param {type:\"boolean\"}\n",
        "do_not_use_uploaded_image2=False # @param {type:\"boolean\"}\n",
        "do_not_use_uploaded_image3=False # @param {type:\"boolean\"}\n",
        "change_resolution=False # @param {type:\"boolean\"}\n",
        "new_width = 512 # @param {\"type\":\"number\"}\n",
        "new_height = 768 # @param {\"type\":\"number\"}\n",
        "pad_output_to_1024_by_1024 = False # @param {type:\"boolean\"}\n",
        "change_output_resolution=False\n",
        "width_out = 768\n",
        "height_out = 1360\n",
        "resize_output=False # @param {type:\"boolean\"}\n",
        "r_width=720 # @param {\"type\":\"number\"}\n",
        "r_height=1280 # @param {\"type\":\"number\"}\n",
        "edit_output_image=False # @param {type:\"boolean\"}\n",
        "overwrite_previous_output=False # @param {type:\"boolean\"}\n",
        "# @markdown ---\n",
        "# @markdown ### Sampler Settings\n",
        "seed=0 # @param {\"type\":\"integer\"}\n",
        "steps = 20 # @param {\"type\":\"slider\",\"min\":0,\"max\":100,\"step\":1}\n",
        "cfg = 2.5 # @param {\"type\":\"slider\",\"min\":0,\"max\":20,\"step\":0.5}\n",
        "sampler_name=\"euler\" # @param [\"uni_pc\", \"uni_pc_bh2\", \"ddim\",\"euler\", \"euler_cfg_pp\", \"euler_ancestral\", \"euler_ancestral_cfg_pp\", \"heun\", \"heunpp2\",\"dpm_2\", \"dpm_2_ancestral\",\"lms\", \"dpm_fast\", \"dpm_adaptive\", \"dpmpp_2s_ancestral\", \"dpmpp_2s_ancestral_cfg_pp\", \"dpmpp_sde\", \"dpmpp_sde_gpu\",\"dpmpp_2m\", \"dpmpp_2m_cfg_pp\", \"dpmpp_2m_sde\", \"dpmpp_2m_sde_gpu\", \"dpmpp_3m_sde\", \"dpmpp_3m_sde_gpu\", \"ddpm\", \"lcm\",\"ipndm\", \"ipndm_v\", \"deis\", \"res_multistep\", \"res_multistep_cfg_pp\", \"res_multistep_ancestral\", \"res_multistep_ancestral_cfg_pp\",\"gradient_estimation\", \"er_sde\", \"seeds_2\", \"seeds_3\"]\n",
        "scheduler=\"simple\" # @param [\"simple\",\"normal\",\"karras\",\"exponential\",\"sgm_uniform\",\"ddim_uniform\",\"beta\",\"linear_quadratic\",\"kl_optimal\"]\n",
        "denoise=1 # @param {\"type\":\"slider\",\"min\":0.0,\"max\":1.0,\"step\":0.01}\n",
        "# @markdown ---\n",
        "# @markdown ### LoRA Settings\n",
        "use_speedup_lora=True # @param {type:\"boolean\"}\n",
        "use_lora=False # @param {type:\"boolean\"}\n",
        "LoRA_Strength=1 # @param {\"type\":\"slider\",\"min\":-10,\"max\":10,\"step\":0.01}\n",
        "use_lora2=False # @param {type:\"boolean\"}\n",
        "LoRA2_Strength=1 # @param {\"type\":\"slider\",\"min\":-10,\"max\":10,\"step\":0.01}\n",
        "use_lora3=False # @param {type:\"boolean\"}\n",
        "LoRA3_Strength=1 # @param {\"type\":\"slider\",\"min\":-10,\"max\":10,\"step\":0.01}\n",
        "use_lora4=False # @param {type:\"boolean\"}\n",
        "LoRA4_Strength=1 # @param {\"type\":\"slider\",\"min\":-10,\"max\":10,\"step\":0.01}\n",
        "\n",
        "file_uploaded11  = globals().get(\"file_uploaded\", None)\n",
        "file_uploaded22 = globals().get(\"file_uploaded2\", None)\n",
        "file_uploaded33 = globals().get(\"file_uploaded3\", None)\n",
        "\n",
        "# file_uploaded11=None\n",
        "\n",
        "mask_path_checked = globals().get(\"mask_path\", None)\n",
        "\n",
        "meta_file_from_mask = globals().get(\"meta_file\", None)\n",
        "\n",
        "# print(meta_file_from_mask)\n",
        "\n",
        "if edit_output_image and outputImagePath is not None:\n",
        "    file_uploaded11 = outputImagePath\n",
        "    file_uploaded22 = None\n",
        "    file_uploaded33 = None\n",
        "\n",
        "import random\n",
        "seed = seed if seed != 0 else random.randint(0, 2**32 - 1)\n",
        "print(f\"Using seed: {seed}\")\n",
        "\n",
        "if do_not_use_uploaded_image1 and file_uploaded11 is not None:\n",
        "    file_uploaded11 = None\n",
        "\n",
        "if do_not_use_uploaded_image2 and file_uploaded22 is not None:\n",
        "    file_uploaded22 = None\n",
        "\n",
        "if do_not_use_uploaded_image3 and file_uploaded33 is not None:\n",
        "    file_uploaded33 = None\n",
        "\n",
        "edit_input(\n",
        "    image_path = file_uploaded11,\n",
        "    use_inpainting=use_inpainting,\n",
        "    mask_path = mask_path_checked,\n",
        "    meta_file_from_mask = meta_file_from_mask,\n",
        "    image_path2 = file_uploaded22,\n",
        "    image_path3 = file_uploaded33,\n",
        "    positive_prompt = positive_prompt,\n",
        "    negative_prompt = negative_prompt,\n",
        "    guidance = shift,\n",
        "    convert_image_3_to_pose = convert_image_3_to_pose,\n",
        "    change_resolution=change_resolution,\n",
        "    width=new_width,\n",
        "    height=new_height,\n",
        "    change_output_resolution=change_output_resolution,\n",
        "    width_out=width_out,\n",
        "    height_out=height_out,\n",
        "    pad_output_to_1024_by_1024=pad_output_to_1024_by_1024,\n",
        "    resize_final_output=resize_output,\n",
        "    r_width=r_width,\n",
        "    r_height=r_height,\n",
        "    seed = seed,\n",
        "    steps = steps,\n",
        "    cfg = cfg,\n",
        "    sampler_name = sampler_name,\n",
        "    scheduler = scheduler,\n",
        "    denoise = denoise,\n",
        "    use_turbo_lora = use_speedup_lora,\n",
        "    use_lora = use_lora,\n",
        "    LoRA_Strength = LoRA_Strength,\n",
        "    use_lora2 = use_lora2,\n",
        "    LoRA2_Strength = LoRA2_Strength,\n",
        "    use_lora3 = use_lora3,\n",
        "    LoRA3_Strength = LoRA3_Strength,\n",
        "    use_lora4 = use_lora4,\n",
        "    LoRA4_Strength = LoRA4_Strength,\n",
        "    overwrite = overwrite_previous_output\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "duration = end_time - start_time\n",
        "mins, secs = divmod(duration, 60)\n",
        "print(f\"Seed: {seed}\")\n",
        "print(f\"âœ… Generation completed in {int(mins)} min {secs:.2f} sec\")\n",
        "\n",
        "clear_memory()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "D9NFwJVBaD78"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}