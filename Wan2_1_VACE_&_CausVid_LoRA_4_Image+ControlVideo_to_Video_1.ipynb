{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IH9vTx04LzJz"
      },
      "source": [
        "# **Wan2.1 VACE & CausVid LoRA for Image & Control Video to Video**\n",
        "\n",
        "- You can use the default settings to generate a 4 second video in less than 5 minutes with the T4 GPU and the dwpose controlnet for two subject animation. You can increase the max_frames value to 41 or more if your image and control video contains only one subject. If your session crashes, then try a lower model quant or reduce the max_frames value.\n",
        "- You can use the dwpose controlnet by disabling both the canny and depth controlnets. Note that dwpose uses more VRAM.\n",
        "- If you want to generate a video with the CausVid lora disabled, then set steps to 20 and cfg_scale to 4.\n",
        "- For best results, the height of the subject(s) in the image should be similar to the height of the subject(s) in the control video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gpREbxnOQQfd"
      },
      "outputs": [],
      "source": [
        "# @title Prepare Environment\n",
        "!pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0\n",
        "%cd /content\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "!git clone --branch ComfyUI_v0.3.36 https://github.com/Isi-dev/ComfyUI\n",
        "%cd /content/ComfyUI/custom_nodes\n",
        "!git clone https://github.com/Isi-dev/ComfyUI_GGUF.git\n",
        "!git clone https://github.com/Isi-dev/comfyui_controlnet_aux\n",
        "clear_output()\n",
        "%cd /content/ComfyUI/custom_nodes/ComfyUI_GGUF\n",
        "!pip install -r requirements.txt\n",
        "clear_output()\n",
        "%cd /content/ComfyUI/custom_nodes/comfyui_controlnet_aux\n",
        "!pip install -r requirements.txt\n",
        "clear_output()\n",
        "\n",
        "%cd /content\n",
        "!git clone https://github.com/Isi-dev/Practical-RIFE\n",
        "%cd /content/Practical-RIFE\n",
        "!pip install git+https://github.com/rk-exxec/scikit-video.git@numpy_deprecation\n",
        "!mkdir -p /content/Practical-RIFE/train_log\n",
        "!wget -q https://huggingface.co/Isi99999/Frame_Interpolation_Models/resolve/main/4.25/train_log/IFNet_HDv3.py -O /content/Practical-RIFE/train_log/IFNet_HDv3.py\n",
        "!wget -q https://huggingface.co/Isi99999/Frame_Interpolation_Models/resolve/main/4.25/train_log/RIFE_HDv3.py -O /content/Practical-RIFE/train_log/RIFE_HDv3.py\n",
        "!wget -q https://huggingface.co/Isi99999/Frame_Interpolation_Models/resolve/main/4.25/train_log/refine.py -O /content/Practical-RIFE/train_log/refine.py\n",
        "!wget -q https://huggingface.co/Isi99999/Frame_Interpolation_Models/resolve/main/4.25/train_log/flownet.pkl -O /content/Practical-RIFE/train_log/flownet.pkl\n",
        "clear_output()\n",
        "\n",
        "%cd /content/ComfyUI\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "\n",
        "def install_pip_packages():\n",
        "    packages = [\n",
        "        'torchsde',\n",
        "        'av',\n",
        "        'diffusers',\n",
        "        'transformers',\n",
        "        'xformers==0.0.29.post2',\n",
        "        'accelerate',\n",
        "        # 'omegaconf',\n",
        "        'tqdm',\n",
        "        # 'librosa',\n",
        "        # 'triton',\n",
        "        # 'sageattention',\n",
        "        'color-matcher',\n",
        "        'onnxruntime',\n",
        "        'onnxruntime-gpu',\n",
        "        'einops'\n",
        "    ]\n",
        "\n",
        "    for package in packages:\n",
        "        try:\n",
        "            # Run pip install silently (using -q)\n",
        "            subprocess.run(\n",
        "                [sys.executable, '-m', 'pip', 'install', '-q', package],\n",
        "                check=True,\n",
        "                capture_output=True\n",
        "            )\n",
        "            print(f\"✓ {package} installed\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"✗ Error installing {package}: {e.stderr.decode().strip() or 'Unknown error'}\")\n",
        "\n",
        "def install_apt_packages():\n",
        "    packages = ['aria2']\n",
        "\n",
        "    try:\n",
        "        # Run apt install silently (using -qq)\n",
        "        subprocess.run(\n",
        "            ['apt-get', '-y', 'install', '-qq'] + packages,\n",
        "            check=True,\n",
        "            capture_output=True\n",
        "        )\n",
        "        print(\"✓ apt packages installed\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"✗ Error installing apt packages: {e.stderr.decode().strip() or 'Unknown error'}\")\n",
        "\n",
        "# Run installations\n",
        "print(\"Installing pip packages...\")\n",
        "install_pip_packages()\n",
        "clear_output()  # Clear the pip installation output\n",
        "\n",
        "print(\"Installing apt packages...\")\n",
        "install_apt_packages()\n",
        "clear_output()  # Clear the apt installation output\n",
        "\n",
        "print(\"Installation completed with status:\")\n",
        "print(\"- All pip packages installed successfully\" if '✗' not in install_pip_packages.__code__.co_consts else \"- Some pip packages had issues\")\n",
        "print(\"- apt packages installed successfully\" if '✗' not in install_apt_packages.__code__.co_consts else \"- apt packages had issues\")\n",
        "\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import gc\n",
        "import sys\n",
        "import random\n",
        "import os\n",
        "import imageio\n",
        "import subprocess\n",
        "from google.colab import files\n",
        "from IPython.display import display, HTML, Image as IPImage\n",
        "import glob\n",
        "from IPython.display import Video as outVid\n",
        "import datetime\n",
        "sys.path.insert(0, '/content/ComfyUI')\n",
        "\n",
        "from comfy import model_management\n",
        "\n",
        "from nodes import (\n",
        "    CheckpointLoaderSimple,\n",
        "    CLIPLoader,\n",
        "    CLIPTextEncode,\n",
        "    VAEDecode,\n",
        "    VAELoader,\n",
        "    KSampler,\n",
        "    UNETLoader,\n",
        "    LoraLoaderModelOnly,\n",
        "    ControlNetLoader,\n",
        "    ControlNetApplyAdvanced,\n",
        "    ImageScale,\n",
        "    LoadImage\n",
        "    # CLIPVisionLoader,\n",
        "    # CLIPVisionEncode\n",
        ")\n",
        "\n",
        "from custom_nodes.ComfyUI_GGUF.nodes import UnetLoaderGGUF\n",
        "from comfy_extras.nodes_model_advanced import ModelSamplingSD3\n",
        "from comfy_extras.nodes_images import SaveAnimatedWEBP\n",
        "from comfy_extras.nodes_video import SaveWEBM\n",
        "from comfy_extras.nodes_wan import WanVaceToVideo\n",
        "from comfy_extras.nodes_wan import TrimVideoLatent\n",
        "from custom_nodes.comfyui_controlnet_aux.node_wrappers.dwpose import DWPose_Preprocessor\n",
        "from custom_nodes.comfyui_controlnet_aux.node_wrappers.depth_anything import Depth_Anything_Preprocessor\n",
        "from custom_nodes.comfyui_controlnet_aux.node_wrappers.canny import Canny_Edge_Preprocessor\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "def download_with_aria2c(link, folder=\"/content/ComfyUI/models/loras\"):\n",
        "    import os\n",
        "\n",
        "    filename = link.split(\"/\")[-1]\n",
        "    command = f\"aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {link} -d {folder} -o {filename}\"\n",
        "\n",
        "    print(\"Executing download command:\")\n",
        "    print(command)\n",
        "\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "    get_ipython().system(command)\n",
        "\n",
        "    return filename\n",
        "\n",
        "def download_civitai_model(civitai_link, civitai_token, folder=\"/content/ComfyUI/models/loras\"):\n",
        "    import os\n",
        "    import time\n",
        "\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        model_id = civitai_link.split(\"/models/\")[1].split(\"?\")[0]\n",
        "    except IndexError:\n",
        "        raise ValueError(\"Invalid Civitai URL format. Please use a link like: https://civitai.com/api/download/models/1523247?...\")\n",
        "\n",
        "    civitai_url = f\"https://civitai.com/api/download/models/{model_id}?type=Model&format=SafeTensor\"\n",
        "    if civitai_token:\n",
        "        civitai_url += f\"&token={civitai_token}\"\n",
        "\n",
        "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"model_{timestamp}.safetensors\"\n",
        "\n",
        "    full_path = os.path.join(folder, filename)\n",
        "\n",
        "    download_command = f\"wget --max-redirect=10 --show-progress \\\"{civitai_url}\\\" -O \\\"{full_path}\\\"\"\n",
        "    print(\"Downloading from Civitai...\")\n",
        "\n",
        "    os.system(download_command)\n",
        "\n",
        "    local_path = os.path.join(folder, filename)\n",
        "    if os.path.exists(local_path) and os.path.getsize(local_path) > 0:\n",
        "        print(f\"LoRA downloaded successfully: {local_path}\")\n",
        "    else:\n",
        "        print(f\"❌ LoRA download failed or file is empty: {local_path}\")\n",
        "\n",
        "    return filename\n",
        "\n",
        "\n",
        "def download_lora(link, folder=\"/content/ComfyUI/models/loras\", civitai_token=None):\n",
        "    \"\"\"\n",
        "    Download a model file, automatically detecting if it's a Civitai link or huggingface download.\n",
        "\n",
        "    Args:\n",
        "        link: The download URL (either huggingface or Civitai)\n",
        "        folder: Destination folder for the download\n",
        "        civitai_token: Optional token for Civitai downloads (required if link is from Civitai)\n",
        "\n",
        "    Returns:\n",
        "        The filename of the downloaded model\n",
        "    \"\"\"\n",
        "    if \"civitai.com\" in link.lower():\n",
        "        if not civitai_token:\n",
        "            raise ValueError(\"Civitai token is required for Civitai downloads\")\n",
        "        return download_civitai_model(link, civitai_token, folder)\n",
        "    else:\n",
        "        return download_with_aria2c(link, folder)\n",
        "\n",
        "\n",
        "\n",
        "def model_download(url: str, dest_dir: str, filename: str = None, silent: bool = True) -> bool:\n",
        "    \"\"\"\n",
        "    Colab-optimized download with aria2c\n",
        "\n",
        "    Args:\n",
        "        url: Download URL\n",
        "        dest_dir: Target directory (will be created if needed)\n",
        "        filename: Optional output filename (defaults to URL filename)\n",
        "        silent: If True, suppresses all output (except errors)\n",
        "\n",
        "    Returns:\n",
        "        bool: True if successful, False if failed\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create destination directory\n",
        "        Path(dest_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Set filename if not specified\n",
        "        if filename is None:\n",
        "            filename = url.split('/')[-1].split('?')[0]  # Remove URL parameters\n",
        "\n",
        "        # Build command\n",
        "        cmd = [\n",
        "            'aria2c',\n",
        "            '--console-log-level=error',\n",
        "            '-c', '-x', '16', '-s', '16', '-k', '1M',\n",
        "            '-d', dest_dir,\n",
        "            '-o', filename,\n",
        "            url\n",
        "        ]\n",
        "\n",
        "        # Add silent flags if requested\n",
        "        if silent:\n",
        "            cmd.extend(['--summary-interval=0', '--quiet'])\n",
        "            print(f\"Downloading {filename}...\", end=' ', flush=True)\n",
        "\n",
        "        # Run download\n",
        "        result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
        "\n",
        "        if silent:\n",
        "            print(\"Done!\")\n",
        "        else:\n",
        "            print(f\"Downloaded {filename} to {dest_dir}\")\n",
        "        return filename\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        error = e.stderr.strip() or \"Unknown error\"\n",
        "        print(f\"\\nError downloading {filename}: {error}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "model_quant = \"Q8_0\" # @param [\"Q4_K_M\", \"Q5_0\", \"Q5_K_M\", \"Q6_K\", \"Q8_0\"]\n",
        "\n",
        "download_loRA_1 = False # @param {type:\"boolean\"}\n",
        "lora_1_download_url = \"Put your loRA here\"# @param {\"type\":\"string\"}\n",
        "\n",
        "download_loRA_2 = False # @param {type:\"boolean\"}\n",
        "lora_2_download_url = \"Put your loRA here\"# @param {\"type\":\"string\"}\n",
        "\n",
        "token_if_civitai_url = \"Put your civitai token here\"# @param {\"type\":\"string\"}\n",
        "\n",
        "lora_1 = None\n",
        "if download_loRA_1:\n",
        "    lora_1 = download_lora(lora_1_download_url, civitai_token=token_if_civitai_url)\n",
        "# Validate loRA file extension\n",
        "valid_extensions = {'.safetensors', '.ckpt', '.pt', '.pth', '.sft'}\n",
        "if lora_1:\n",
        "    if not any(lora_1.lower().endswith(ext) for ext in valid_extensions):\n",
        "        print(f\"❌ Invalid LoRA format: {lora_1}\")\n",
        "        lora_1 = None\n",
        "    else:\n",
        "        clear_output()\n",
        "        print(\"loRA 1 downloaded succesfully!\")\n",
        "\n",
        "lora_2 = None\n",
        "if download_loRA_2:\n",
        "    lora_2 = download_lora(lora_2_download_url, civitai_token=token_if_civitai_url)\n",
        "if lora_2:\n",
        "    if not any(lora_2.lower().endswith(ext) for ext in valid_extensions):\n",
        "        print(f\"❌ Invalid LoRA format: {lora_2}\")\n",
        "        lora_2 = None\n",
        "    else:\n",
        "        clear_output()\n",
        "        print(\"loRA 2 downloaded succesfully!\")\n",
        "\n",
        "\n",
        "# !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors -d /content/ComfyUI/models/clip_vision -o clip_vision_h.safetensors\n",
        "\n",
        "# model_quant = \"Q8_0\"\n",
        "\n",
        "if model_quant == \"Q4_K_M\":\n",
        "    dit_model = model_download(\"https://huggingface.co/Isi99999/Wan2.1BasedModels/resolve/main/Wan2.1_14B_VACE-Q4_K_M.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
        "elif model_quant == \"Q5_0\":\n",
        "    dit_model = model_download(\"https://huggingface.co/Isi99999/Wan2.1BasedModels/resolve/main/Wan2.1_14B_VACE-Q5_0.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
        "elif model_quant == \"Q5_K_M\":\n",
        "    dit_model = model_download(\"https://huggingface.co/Isi99999/Wan2.1BasedModels/resolve/main/Wan2.1-VACE-14B-Q5_K_M.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
        "elif model_quant == \"Q6_K\":\n",
        "    dit_model = model_download(\"https://huggingface.co/Isi99999/Wan2.1BasedModels/resolve/main/Wan2.1-VACE-14B-Q6_K.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
        "else:\n",
        "    dit_model = model_download(\"https://huggingface.co/Isi99999/Wan2.1BasedModels/resolve/main/Wan2.1-VACE-14B-Q8_0.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
        "causvid_lora = model_download(\"https://huggingface.co/Isi99999/Wan2.1BasedModels/resolve/main/Wan21_CausVid_14B_T2V_lora_rank32.safetensors\", \"/content/ComfyUI/models/loras\")\n",
        "# causvid_lora = model_download(\"https://huggingface.co/Kijai/WanVideo_comfy/resolve/main/Wan21_CausVid_14B_T2V_lora_rank32_v2.safetensors\", \"/content/ComfyUI/models/loras\")\n",
        "text_encoder = model_download(\"https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors\", \"/content/ComfyUI/models/text_encoders\")\n",
        "vae_model = model_download(\"https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors\", \"/content/ComfyUI/models/vae\")\n",
        "\n",
        "depth_model = model_download(\"https://huggingface.co/Isi99999/Upscalers/resolve/main/Flux/depth_anything_vitl14.pth\", \"/content/ComfyUI/custom_nodes/comfyui_controlnet_aux/ckpts/LiheYoung/Depth-Anything/checkpoints\")\n",
        "\n",
        "\n",
        "def clear_memory():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()\n",
        "    for obj in list(globals().values()):\n",
        "        if torch.is_tensor(obj) or (hasattr(obj, \"data\") and torch.is_tensor(obj.data)):\n",
        "            del obj\n",
        "    gc.collect()\n",
        "\n",
        "def save_as_mp4(images, filename_prefix, fps, output_dir=\"/content/ComfyUI/output\"):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_path = f\"{output_dir}/{filename_prefix}.mp4\"\n",
        "\n",
        "    frames = [(img.cpu().numpy() * 255).astype(np.uint8) for img in images]\n",
        "\n",
        "    with imageio.get_writer(output_path, fps=fps) as writer:\n",
        "        for frame in frames:\n",
        "            writer.append_data(frame)\n",
        "\n",
        "    return output_path\n",
        "\n",
        "def save_as_webp(images, filename_prefix, fps, quality=90, lossless=False, method=4, output_dir=\"/content/ComfyUI/output\"):\n",
        "    \"\"\"Save images as animated WEBP using imageio.\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_path = f\"{output_dir}/{filename_prefix}.webp\"\n",
        "\n",
        "\n",
        "    frames = [(img.cpu().numpy() * 255).astype(np.uint8) for img in images]\n",
        "\n",
        "\n",
        "    kwargs = {\n",
        "        'fps': int(fps),\n",
        "        'quality': int(quality),\n",
        "        'lossless': bool(lossless),\n",
        "        'method': int(method)\n",
        "    }\n",
        "\n",
        "    with imageio.get_writer(\n",
        "        output_path,\n",
        "        format='WEBP',\n",
        "        mode='I',\n",
        "        **kwargs\n",
        "    ) as writer:\n",
        "        for frame in frames:\n",
        "            writer.append_data(frame)\n",
        "\n",
        "    return output_path\n",
        "\n",
        "def save_as_webm(images, filename_prefix, fps, codec=\"vp9\", quality=32, output_dir=\"/content/ComfyUI/output\"):\n",
        "    \"\"\"Save images as WEBM using imageio.\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_path = f\"{output_dir}/{filename_prefix}.webm\"\n",
        "\n",
        "\n",
        "    frames = [(img.cpu().numpy() * 255).astype(np.uint8) for img in images]\n",
        "\n",
        "\n",
        "    kwargs = {\n",
        "        'fps': int(fps),\n",
        "        'quality': int(quality),\n",
        "        'codec': str(codec),\n",
        "        'output_params': ['-crf', str(int(quality))]\n",
        "    }\n",
        "\n",
        "    with imageio.get_writer(\n",
        "        output_path,\n",
        "        format='FFMPEG',\n",
        "        mode='I',\n",
        "        **kwargs\n",
        "    ) as writer:\n",
        "        for frame in frames:\n",
        "            writer.append_data(frame)\n",
        "\n",
        "    return output_path\n",
        "\n",
        "def save_as_image(image, filename_prefix, output_dir=\"/content/ComfyUI/output\"):\n",
        "    \"\"\"Save single frame as PNG image.\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_path = f\"{output_dir}/{filename_prefix}.png\"\n",
        "\n",
        "    frame = (image.cpu().numpy() * 255).astype(np.uint8)\n",
        "\n",
        "    Image.fromarray(frame).save(output_path)\n",
        "\n",
        "    return output_path\n",
        "\n",
        "\n",
        "def upload_image():\n",
        "    \"\"\"Handle image upload in Colab and store in /content/ComfyUI/input/\"\"\"\n",
        "    from google.colab import files\n",
        "    import os\n",
        "    import shutil\n",
        "\n",
        "    os.makedirs('/content/ComfyUI/input', exist_ok=True)\n",
        "\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # Move each uploaded file to ComfyUI input directory\n",
        "    for filename in uploaded.keys():\n",
        "        src_path = f'/content/ComfyUI/{filename}'\n",
        "        dest_path = f'/content/ComfyUI/input/{filename}'\n",
        "\n",
        "        shutil.move(src_path, dest_path)\n",
        "        print(f\"Image saved to: {dest_path}\")\n",
        "        return dest_path\n",
        "\n",
        "    return None\n",
        "\n",
        "vid_15fps = \"\"\n",
        "\n",
        "import shutil\n",
        "import cv2\n",
        "\n",
        "def upload_file():\n",
        "    \"\"\"Handle file upload (image or video) and return paths.\"\"\"\n",
        "    os.makedirs('/content/ComfyUI/input', exist_ok=True)\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    paths = []\n",
        "    for filename in uploaded.keys():\n",
        "        src_path = f'/content/ComfyUI/{filename}'\n",
        "        dest_path = f'/content/ComfyUI/input/{filename}'\n",
        "        shutil.move(src_path, dest_path)\n",
        "        paths.append(dest_path)\n",
        "        print(f\"File saved to: {dest_path}\")\n",
        "\n",
        "    return paths[0] if paths else None\n",
        "\n",
        "\n",
        "def extract_frames(video_path, max_frames=None):\n",
        "    \"\"\"Extract frames from video and return as a batch tensor.\"\"\"\n",
        "    vidcap = cv2.VideoCapture(video_path)\n",
        "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "    frames = []\n",
        "\n",
        "    while True:\n",
        "        success, frame = vidcap.read()\n",
        "        if not success or (max_frames and len(frames) >= max_frames):\n",
        "            break\n",
        "\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frame = torch.from_numpy(frame).float() / 255.0\n",
        "        frames.append(frame)\n",
        "\n",
        "    if not frames:\n",
        "        return None, fps\n",
        "\n",
        "    # Stack frames into a batch tensor: (N, H, W, 3)\n",
        "    batch = torch.stack(frames, dim=0)\n",
        "    print(f\"Extracted {len(frames)} frames (shape: {batch.shape})\")\n",
        "    return batch, fps\n",
        "\n",
        "\n",
        "def select_every_n_frame_tensor(\n",
        "    frames_tensor: torch.Tensor,\n",
        "    fps: float,\n",
        "    n: int,\n",
        "    skip_first: int = 0,\n",
        "    max_output_frames: int = 0\n",
        "):\n",
        "    if frames_tensor is None or frames_tensor.ndim != 4:\n",
        "        raise ValueError(\"frames_tensor must be a 4D tensor of shape (N, H, W, C)\")\n",
        "    if n < 1:\n",
        "        raise ValueError(\"n must be >= 1\")\n",
        "\n",
        "    total_frames = frames_tensor.shape[0]\n",
        "\n",
        "    if skip_first >= total_frames:\n",
        "        print(\"No frames available after skipping.\")\n",
        "        return None, 0.0\n",
        "\n",
        "    frames_to_use = frames_tensor[skip_first:]\n",
        "\n",
        "    # Select every nth frame\n",
        "    selected_frames = frames_to_use[::n]\n",
        "\n",
        "    # Cap output if needed\n",
        "    if max_output_frames > 0 and selected_frames.shape[0] > max_output_frames:\n",
        "        selected_frames = selected_frames[:max_output_frames]\n",
        "\n",
        "    adjusted_fps = fps / n\n",
        "\n",
        "    if max_output_frames:\n",
        "        print(f\"Frame cap: {max_output_frames} -> Final output: {selected_frames.shape[0]} frames\")\n",
        "    print(f\"Adjusted FPS: {adjusted_fps:.2f}  -> Final output: {selected_frames.shape[0]} frames\")\n",
        "\n",
        "    return selected_frames, adjusted_fps\n",
        "\n",
        "\n",
        "def colormatch(image_ref, image_target, method='mkl', strength=1.0):\n",
        "    try:\n",
        "        from color_matcher import ColorMatcher\n",
        "    except ImportError:\n",
        "        raise Exception(\"Can't import color-matcher\")\n",
        "\n",
        "    cm = ColorMatcher()\n",
        "    image_ref, image_target = image_ref.cpu(), image_target.cpu()\n",
        "\n",
        "    ref_np = image_ref.squeeze().numpy()\n",
        "    target_np = image_target.squeeze().numpy()\n",
        "    batch_size = image_target.size(0)\n",
        "\n",
        "    if image_ref.size(0) > 1 and image_ref.size(0) != batch_size:\n",
        "        raise ValueError(\"ColorMatch: Use either a single reference image or a batch matching target size.\")\n",
        "\n",
        "    out = []\n",
        "    for i in range(batch_size):\n",
        "        tgt = target_np if batch_size == 1 else target_np[i]\n",
        "        ref = ref_np if image_ref.size(0) == 1 else ref_np[i]\n",
        "        try:\n",
        "            matched = cm.transfer(src=tgt, ref=ref, method=method)\n",
        "            result = tgt + strength * (matched - tgt)\n",
        "            out.append(torch.from_numpy(result))\n",
        "        except Exception as e:\n",
        "            print(f\"Color match error: {e}\")\n",
        "            break\n",
        "\n",
        "    return (torch.stack(out).float().clamp(0, 1),)\n",
        "\n",
        "def image_width_height(image):\n",
        "    if image.ndim == 4:\n",
        "        _, height, width, _ = image.shape\n",
        "    elif image.ndim == 3:\n",
        "        height, width, _ = image.shape\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported image shape: {image.shape}\")\n",
        "    return width, height\n",
        "\n",
        "def generate_video(\n",
        "    image_path: str = None,\n",
        "    video_path: str = None,\n",
        "    positive_prompt: str = \"a cute anime girl with massive fennec ears and a big fluffy tail wearing a maid outfit turning around\",\n",
        "    negative_prompt: str = \"色调艳丽，过曝，静态，细节模糊不清，字幕，风格，作品，画作，画面，静止，整体发灰，最差质量，低质量，JPEG压缩残留，丑陋的，残缺的，多余的手指，画得不好的手部，画得不好的脸部，畸形的，毁容的，形态畸形的肢体，手指融合，静止不动的画面，杂乱的背景，三条腿，背景人很多，倒着走 \",\n",
        "    change_resolution: bool = False,\n",
        "    width: int = 832,\n",
        "    height: int = 480,\n",
        "    select_every_nth: int = 1,\n",
        "    skip_first_frames: int = 0,\n",
        "    seed: int = 82628696717253,\n",
        "    use_causvid: bool = False,\n",
        "    causvid_lora_strength: float = 0.25,\n",
        "    steps: int = 20,\n",
        "    cfg_scale: float = 1.0,\n",
        "    sampler_name: str = \"uni_pc\",\n",
        "    scheduler: str = \"simple\",\n",
        "    frames: int = 33,\n",
        "    fps: int = 16,\n",
        "    remove_first_frame: bool = True,\n",
        "    match_colors: bool = False,\n",
        "    output_format: str = \"mp4\",\n",
        "    overwrite: bool = False,\n",
        "    use_lora_1: bool = False,\n",
        "    LoRA_1_Strength: float = 1.0,\n",
        "    use_lora_2: bool = False,\n",
        "    LoRA_2_Strength: float = 1.0,\n",
        "    use_controlnet_canny: bool = False,\n",
        "    low_threshold: int = 100,\n",
        "    high_threshold: int = 200,\n",
        "    resolution: int = 512,\n",
        "    use_controlnet_depth: bool = False\n",
        "\n",
        "):\n",
        "\n",
        "    with torch.inference_mode():\n",
        "\n",
        "        # Initialize nodes\n",
        "        unet_loader = UnetLoaderGGUF()\n",
        "        model_sampling = ModelSamplingSD3()\n",
        "        clip_loader = CLIPLoader()\n",
        "        clip_encode_positive = CLIPTextEncode()\n",
        "        clip_encode_negative = CLIPTextEncode()\n",
        "        vae_loader = VAELoader()\n",
        "        # clip_vision_loader = CLIPVisionLoader()\n",
        "        # clip_vision_encode = CLIPVisionEncode()\n",
        "        image_scaler = ImageScale()\n",
        "        load_image = LoadImage()\n",
        "        load_lora = LoraLoaderModelOnly()\n",
        "        load_lora_1 = LoraLoaderModelOnly()\n",
        "        load_lora_2 = LoraLoaderModelOnly()\n",
        "        wan_vace_to_video = WanVaceToVideo()\n",
        "        trim_video_latent = TrimVideoLatent()\n",
        "        ksampler = KSampler()\n",
        "        vae_decode = VAEDecode()\n",
        "        save_webp = SaveAnimatedWEBP()\n",
        "        save_webm = SaveWEBM()\n",
        "        canny_edge = Canny_Edge_Preprocessor()\n",
        "        dw_pose_estimator = DWPose_Preprocessor()\n",
        "        depth_anything = Depth_Anything_Preprocessor()\n",
        "\n",
        "\n",
        "        print(\"Loading Text_Encoder...\")\n",
        "        clip = clip_loader.load_clip(text_encoder, \"wan\", \"default\")[0]\n",
        "\n",
        "        positive = clip_encode_positive.encode(clip, positive_prompt)[0]\n",
        "        negative = clip_encode_negative.encode(clip, negative_prompt)[0]\n",
        "\n",
        "        del clip\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        if image_path is None:\n",
        "            print(\"Please upload an image file:\")\n",
        "            image_path = upload_image()\n",
        "        if image_path is None:\n",
        "            print(\"No image uploaded!\")\n",
        "        loaded_image = load_image.load_image(image_path)[0]\n",
        "        # clip_vision = clip_vision_loader.load_clip(\"clip_vision_h.safetensors\")[0]\n",
        "        # clip_vision_output = clip_vision_encode.encode(clip_vision, loaded_image, \"none\")[0]\n",
        "\n",
        "        width_int, height_int = image_width_height(loaded_image)\n",
        "\n",
        "        if change_resolution:\n",
        "            print(\"Changing Image Resolution...\")\n",
        "            loaded_image = image_scaler.upscale(\n",
        "                loaded_image,\n",
        "                \"lanczos\",\n",
        "                width,\n",
        "                height,\n",
        "                \"disabled\"\n",
        "            )[0]\n",
        "        else:\n",
        "            width = width_int\n",
        "            height = height_int\n",
        "\n",
        "        print(f\"Image width is {width} and height is {height}\")\n",
        "\n",
        "        print(\"Processing Video...\")\n",
        "        image_batch, fps = extract_frames(video_path)\n",
        "        image_batch, fps = select_every_n_frame_tensor(image_batch, fps, select_every_nth_frame, skip_first_frames, frames)\n",
        "        if image_batch is None:\n",
        "            raise ValueError(\"No frames extracted from video!\")\n",
        "\n",
        "        image_batch = image_scaler.upscale(\n",
        "                image_batch,\n",
        "                \"lanczos\",\n",
        "                width,\n",
        "                height,\n",
        "                \"disabled\"\n",
        "            )[0]\n",
        "\n",
        "        if use_controlnet_canny:\n",
        "            print(\"Extracting canny images from video...\")\n",
        "            image_batch = canny_edge.execute(image_batch, low_threshold, high_threshold, resolution)[0]\n",
        "        elif use_controlnet_depth:\n",
        "            print(\"Extracting depth images from video...\")\n",
        "            image_batch = depth_anything.execute(image_batch)[0]\n",
        "        else:\n",
        "            print(\"Extracting dwpose images from video...\")\n",
        "            result_dict = dw_pose_estimator.estimate_pose(image_batch)\n",
        "            image_batch = result_dict[\"result\"][0]\n",
        "            del result_dict\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "        print(\"Loading VAE...\")\n",
        "        vae = vae_loader.load_vae(vae_model)[0]\n",
        "\n",
        "        positive_out, negative_out, out_latent, trim_latent = wan_vace_to_video.encode(\n",
        "            positive, negative, vae, width, height, frames, 1, 1, image_batch, None, loaded_image\n",
        "        )\n",
        "\n",
        "        del image_batch\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        print(\"Loading Unet Model...\")\n",
        "        model = unet_loader.load_unet(dit_model)[0]\n",
        "\n",
        "        if use_causvid:\n",
        "            # if lora is not None:\n",
        "            print(\"Loading causvid Lora...\")\n",
        "            model = load_lora.load_lora_model_only(model, causvid_lora, causvid_lora_strength)[0]\n",
        "\n",
        "\n",
        "        if use_lora_1:\n",
        "            if lora_1 is not None:\n",
        "                print(\"Loading LoRA 1...\")\n",
        "                model = load_lora_1.load_lora_model_only(model, lora_1, LoRA_1_Strength)[0]\n",
        "\n",
        "        if use_lora_2:\n",
        "            if lora_2 is not None:\n",
        "                print(\"Loading LoRA 2...\")\n",
        "                model = load_lora_2.load_lora_model_only(model, lora_2, LoRA_2_Strength)[0]\n",
        "\n",
        "\n",
        "        model = model_sampling.patch(model, 8)[0]\n",
        "\n",
        "        clear_output()\n",
        "\n",
        "\n",
        "        base_name = \"output\"\n",
        "        if not overwrite:\n",
        "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            base_name += f\"_{timestamp}\"\n",
        "\n",
        "        print(\"Generating video...\")\n",
        "        sampled = ksampler.sample(\n",
        "            model=model,\n",
        "            seed=seed,\n",
        "            steps=steps,\n",
        "            cfg=cfg_scale,\n",
        "            sampler_name=sampler_name,\n",
        "            scheduler=scheduler,\n",
        "            positive=positive_out,\n",
        "            negative=negative_out,\n",
        "            latent_image=out_latent\n",
        "        )[0]\n",
        "\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        if remove_first_frame:\n",
        "            print(\"Trimming video latent...\")\n",
        "            sampled = trim_video_latent.op(sampled, trim_latent)[0]\n",
        "\n",
        "        # sampled = trim_video_latent.op(sampled, 1)[0]\n",
        "\n",
        "        try:\n",
        "            print(\"Decoding latents...\")\n",
        "            decoded = vae_decode.decode(vae, sampled)[0]\n",
        "\n",
        "            del vae\n",
        "            del sampled\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            if match_colors:\n",
        "                print(\"Matching color of video frames to Reference Image...\")\n",
        "                decoded = colormatch(loaded_image, decoded)[0]\n",
        "\n",
        "\n",
        "            output_path = \"\"\n",
        "            if frames == 1:\n",
        "                print(\"Single frame detected - saving as PNG image...\")\n",
        "                output_path = save_as_image(decoded[0], base_name)\n",
        "                # print(f\"Image saved as PNG: {output_path}\")\n",
        "\n",
        "                display(IPImage(filename=output_path))\n",
        "            else:\n",
        "                if output_format.lower() == \"webm\":\n",
        "                    print(\"Saving as WEBM...\")\n",
        "                    output_path = save_as_webm(\n",
        "                        decoded,\n",
        "                        base_name,\n",
        "                        fps=fps,\n",
        "                        codec=\"vp9\",\n",
        "                        quality=10\n",
        "                    )\n",
        "                elif output_format.lower() == \"mp4\":\n",
        "                    print(\"Saving as MP4...\")\n",
        "                    output_path = save_as_mp4(decoded, base_name, fps)\n",
        "                else:\n",
        "                    raise ValueError(f\"Unsupported output format: {output_format}\")\n",
        "\n",
        "                # print(f\"Video saved as {output_format.upper()}: {output_path}\")\n",
        "\n",
        "                display_video(output_path)\n",
        "\n",
        "                global vid_15fps\n",
        "\n",
        "                vid_15fps = output_path\n",
        "\n",
        "                del decoded\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during decoding/saving: {str(e)}\")\n",
        "            raise\n",
        "        finally:\n",
        "            clear_memory()\n",
        "\n",
        "def display_video(video_path):\n",
        "    from IPython.display import HTML\n",
        "    from base64 import b64encode\n",
        "\n",
        "    video_data = open(video_path,'rb').read()\n",
        "\n",
        "    # Determine MIME type based on file extension\n",
        "    if video_path.lower().endswith('.mp4'):\n",
        "        mime_type = \"video/mp4\"\n",
        "    elif video_path.lower().endswith('.webm'):\n",
        "        mime_type = \"video/webm\"\n",
        "    elif video_path.lower().endswith('.webp'):\n",
        "        mime_type = \"image/webp\"\n",
        "    else:\n",
        "        mime_type = \"video/mp4\"  # default\n",
        "\n",
        "    data_url = f\"data:{mime_type};base64,\" + b64encode(video_data).decode()\n",
        "\n",
        "    display(HTML(f\"\"\"\n",
        "    <video width=512 controls autoplay loop>\n",
        "        <source src=\"{data_url}\" type=\"{mime_type}\">\n",
        "    </video>\n",
        "    \"\"\"))\n",
        "\n",
        "print(\"✅ Environment Setup Complete!\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wspT60jB9xXa"
      },
      "outputs": [],
      "source": [
        "# @title Upload Image\n",
        "%cd /content/ComfyUI\n",
        "file_uploaded = upload_image()\n",
        "display_upload = True # @param {type:\"boolean\"}\n",
        "if display_upload:\n",
        "    if file_uploaded.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "        display(IPImage(filename=file_uploaded))\n",
        "    else:\n",
        "        print(\"The image format cannot be displayed.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Upload Control Video\n",
        "\n",
        "%cd /content/ComfyUI\n",
        "vid_uploaded = upload_file()\n",
        "display_upload = True # @param {type:\"boolean\"}\n",
        "if display_upload:\n",
        "    try:\n",
        "        if vid_uploaded.lower().endswith(('.mp4', '.mov', '.webm', '.avi', '.mkv')):\n",
        "            display_video(vid_uploaded)\n",
        "        else:\n",
        "            print(\"The uploaded file format is not supported for display.\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error displaying the uploaded file: {e}\")"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "DCeNkWKJTrBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NOrFgFfORA-F"
      },
      "outputs": [],
      "source": [
        "# @title Generate Video\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "positive_prompt = \"A beautiful couple dancing\" # @param {\"type\":\"string\"}\n",
        "negative_prompt = \"bad quality, blurry, messy, chaotic\" # @param {\"type\":\"string\"}\n",
        "# @markdown ---\n",
        "# @markdown ### Image Settings\n",
        "change_resolution=True # @param {type:\"boolean\"}\n",
        "new_width = 432 # @param {\"type\":\"number\"}\n",
        "new_height = 768 # @param {\"type\":\"number\"}\n",
        "# @markdown ---\n",
        "# @markdown ### Video Settings\n",
        "select_every_nth_frame = 6 # @param {\"type\":\"integer\"}\n",
        "skip_first_frames = 0 # @param {\"type\":\"integer\"}\n",
        "# max_output_frames = 0 # @param {\"type\":\"integer\"}\n",
        "max_frames = 21 # @param {\"type\":\"integer\", \"min\":1, \"max\":120}\n",
        "max_frames = max_frames+2\n",
        "# fps = 16 # @param {\"type\":\"integer\", \"min\":1, \"max\":60}\n",
        "fps = 15\n",
        "remove_first_frame=True # @param {type:\"boolean\"}\n",
        "match_colors=True # @param {type:\"boolean\"}\n",
        "FRAME_MULTIPLIER = 6 # @param {\"type\":\"number\"}\n",
        "vid_fps = 30 # @param {\"type\":\"number\"}\n",
        "output_format = \"mp4\" # @param [\"mp4\", \"webm\"]\n",
        "overwrite=True # @param {type:\"boolean\"}\n",
        "# @markdown ---\n",
        "# @markdown ### Sampler Settings\n",
        "seed = 0 # @param {\"type\":\"integer\"}\n",
        "steps = 4 # @param {\"type\":\"integer\", \"min\":1, \"max\":100}\n",
        "cfg_scale = 1 # @param {\"type\":\"number\", \"min\":1, \"max\":20}\n",
        "sampler_name=\"euler_ancestral\" # @param [\"uni_pc\", \"uni_pc_bh2\", \"ddim\",\"euler\", \"euler_cfg_pp\", \"euler_ancestral\", \"euler_ancestral_cfg_pp\", \"heun\", \"heunpp2\",\"dpm_2\", \"dpm_2_ancestral\",\"lms\", \"dpm_fast\", \"dpm_adaptive\", \"dpmpp_2s_ancestral\", \"dpmpp_2s_ancestral_cfg_pp\", \"dpmpp_sde\", \"dpmpp_sde_gpu\",\"dpmpp_2m\", \"dpmpp_2m_cfg_pp\", \"dpmpp_2m_sde\", \"dpmpp_2m_sde_gpu\", \"dpmpp_3m_sde\", \"dpmpp_3m_sde_gpu\", \"ddpm\", \"lcm\",\"ipndm\", \"ipndm_v\", \"deis\", \"res_multistep\", \"res_multistep_cfg_pp\", \"res_multistep_ancestral\", \"res_multistep_ancestral_cfg_pp\",\"gradient_estimation\", \"er_sde\", \"seeds_2\", \"seeds_3\"]\n",
        "scheduler=\"simple\" # @param [\"simple\",\"normal\",\"karras\",\"exponential\",\"sgm_uniform\",\"ddim_uniform\",\"beta\",\"linear_quadratic\",\"kl_optimal\"]\n",
        "# @markdown ---\n",
        "# @markdown ### LoRA Settings\n",
        "use_causvid_lora=True # @param {type:\"boolean\"}\n",
        "causvid_lora_strength = 0.8 # @param {\"type\":\"slider\",\"min\":-1,\"max\":1,\"step\":0.01}\n",
        "use_lora_1=False # @param {type:\"boolean\"}\n",
        "LoRA_1_Strength=1 # @param {\"type\":\"slider\",\"min\":-1,\"max\":1,\"step\":0.01}\n",
        "use_lora_2=False # @param {type:\"boolean\"}\n",
        "LoRA_2_Strength=1 # @param {\"type\":\"slider\",\"min\":-1,\"max\":1,\"step\":0.01}\n",
        "# @markdown ---\n",
        "# @markdown ### ControlNets Settings\n",
        "use_canny=False # @param {type:\"boolean\"}\n",
        "canny_low_threshold=100 # @param {\"type\":\"slider\",\"min\":0,\"max\":255,\"step\":1}\n",
        "canny_high_threshold=200 # @param {\"type\":\"slider\",\"min\":0,\"max\":255,\"step\":1}\n",
        "canny_resolution=512 # @param {\"type\":\"slider\",\"min\":64,\"max\":16384,\"step\":64}\n",
        "use_depth=False # @param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "%cd /content/ComfyUI\n",
        "\n",
        "import random\n",
        "seed = seed if seed != 0 else random.randint(0, 2**32 - 1)\n",
        "print(f\"Using seed: {seed}\")\n",
        "\n",
        "# with torch.inference_mode():\n",
        "generate_video(\n",
        "    image_path=file_uploaded,\n",
        "    video_path=vid_uploaded,\n",
        "    positive_prompt=positive_prompt,\n",
        "    negative_prompt=negative_prompt,\n",
        "    change_resolution=change_resolution,\n",
        "    width=new_width,\n",
        "    height=new_height,\n",
        "    select_every_nth=select_every_nth_frame,\n",
        "    skip_first_frames=skip_first_frames,\n",
        "    seed=seed,\n",
        "    use_causvid=use_causvid_lora,\n",
        "    causvid_lora_strength=causvid_lora_strength,\n",
        "    steps=steps,\n",
        "    cfg_scale=cfg_scale,\n",
        "    sampler_name=sampler_name,\n",
        "    scheduler=scheduler,\n",
        "    frames=max_frames,\n",
        "    fps=fps,\n",
        "    remove_first_frame=remove_first_frame,\n",
        "    match_colors=match_colors,\n",
        "    output_format=output_format,\n",
        "    overwrite=overwrite,\n",
        "    use_lora_1=use_lora_1,\n",
        "    LoRA_1_Strength=LoRA_1_Strength,\n",
        "    use_lora_2=use_lora_2,\n",
        "    LoRA_2_Strength=LoRA_2_Strength,\n",
        "    use_controlnet_canny=use_canny,\n",
        "    low_threshold=canny_low_threshold,\n",
        "    high_threshold=canny_high_threshold,\n",
        "    resolution=canny_resolution,\n",
        "    use_controlnet_depth=use_depth\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "duration = end_time - start_time\n",
        "mins, secs = divmod(duration, 60)\n",
        "print(f\"✅ Generation completed in {int(mins)} min {secs:.2f} sec\")\n",
        "\n",
        "clear_memory()\n",
        "\n",
        "print(f\"Converting video to {vid_fps} fps...\")\n",
        "\n",
        "%cd /content/Practical-RIFE\n",
        "\n",
        "# Suppress ALSA errors\n",
        "os.environ[\"XDG_RUNTIME_DIR\"] = \"/tmp\"\n",
        "os.environ[\"SDL_AUDIODRIVER\"] = \"dummy\"\n",
        "\n",
        "# Disable warnings from ffmpeg about missing audio\n",
        "os.environ[\"PYGAME_HIDE_SUPPORT_PROMPT\"] = \"1\"\n",
        "os.environ[\"FFMPEG_LOGLEVEL\"] = \"quiet\"\n",
        "\n",
        "!python3 inference_video.py --multi={FRAME_MULTIPLIER} --fps={vid_fps} --video={vid_15fps} --scale={1}\n",
        "video_folder = \"/content/ComfyUI/output/\"\n",
        "\n",
        "# Find the latest MP4 file\n",
        "video_files = glob.glob(os.path.join(video_folder, \"*.mp4\"))\n",
        "\n",
        "if video_files:\n",
        "    latest_video = max(video_files, key=os.path.getctime)\n",
        "    !ffmpeg -i \"{latest_video}\" -vcodec libx264 -crf 18 -preset fast output_converted.mp4 -loglevel error -y\n",
        "\n",
        "    print(f\"Displaying video: {latest_video}\")\n",
        "    display(outVid(\"output_converted.mp4\", embed=True))\n",
        "    # displayVid(outVid(latest_video, embed=True))\n",
        "else:\n",
        "    print(\"❌ No video found in output/\")\n",
        "\n",
        "del video_files\n",
        "\n",
        "end_time = time.time()\n",
        "duration = end_time - start_time\n",
        "mins, secs = divmod(duration, 60)\n",
        "print(f\"✅ Generation completed in {int(mins)} min {secs:.2f} sec\")\n",
        "\n",
        "clear_memory()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}