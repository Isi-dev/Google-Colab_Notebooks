{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2lupAO0HdyH"
      },
      "source": [
        "# **InfiniteTalk & MultiTalk with WAN2.1**\n",
        "-  This notebook is based on ComfyUI_WanVideoWrapper by Kijai : https://github.com/kijai/ComfyUI-WanVideoWrapper\n",
        "- You can use this notebook for audio driven image to video generation on the L4 or A100 GPU. Videos are generated at 25 fps. This means that a 5-second video requires 125 frames. The A100 is recommended for generations beyond 5 seconds.  \n",
        "- If using the L4, then I recommend you use a Q4_K_M GGUF Wan 2.1 Model or a lower quant and reduce the image resolution below 480p to avoid OOM errors. Generating a 17-second (425 frames) 432x768 video with the Q4_K_M GGUF model almost crashed the A100. You can enable 'use_block_swap' to manage VRAM for longer generations at the cost of increased generation time.\n",
        "- You can find models in these huggingface repos: (1) https://huggingface.co/Kijai/WanVideo_comfy/tree/main (2) https://huggingface.co/Comfy-Org/models (3) https://huggingface.co/MeiGen-AI/InfiniteTalk/tree/main (4) https://huggingface.co/city96/models\n",
        "- Make sure the dimensions of your image are divisible by 16.\n",
        "- Note that while multitalk uses one model for both single-person and multiple-people generation, infiniteTalk uses different models.\n",
        "- Github projects: InfiniteTalk -> https://github.com/MeiGen-AI/InfiniteTalk MultiTalk -> https://github.com/MeiGen-AI/MultiTalk\n",
        "- Notebook source: https://github.com/Isi-dev/Google-Colab_Notebooks\n",
        "- Premium notebooks I highly recommend: https://isinse.gumroad.com/\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "96675e45HYsu"
      },
      "outputs": [],
      "source": [
        "# Default links\n",
        "# wan21_model_download_url:\n",
        "# https://huggingface.co/city96/Wan2.1-I2V-14B-480P-gguf/resolve/main/wan2.1-i2v-14b-480p-Q4_K_M.gguf\n",
        "# speed_LoRA_download_url:\n",
        "# https://huggingface.co/Kijai/WanVideo_comfy/resolve/main/Lightx2v/lightx2v_I2V_14B_480p_cfg_step_distill_rank64_bf16.safetensors\n",
        "# infiniteTalk_url:\n",
        "# https://huggingface.co/Kijai/WanVideo_comfy/resolve/main/InfiniteTalk/Wan2_1-InfiniTetalk-Single_fp16.safetensors\n",
        "# multiTalk_url:\n",
        "# https://huggingface.co/Kijai/WanVideo_comfy/resolve/main/WanVideo_2_1_Multitalk_14B_fp8_e4m3fn.safetensors\n",
        "# infiniteTalk_url (for multiple speakers):\n",
        "# https://huggingface.co/Kijai/WanVideo_comfy/resolve/main/InfiniteTalk/Wan2_1-InfiniteTalk-Multi_fp16.safetensors\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# @markdown # 💥1. Prepare Environment\n",
        "# !pip install --upgrade --quiet torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "# !pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0\n",
        "!pip install torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0\n",
        "%cd /content\n",
        "from IPython.display import clear_output\n",
        "# !pip install -q torchsde einops diffusers accelerate xformers==0.0.29.post2 triton==3.2.0 sageattention\n",
        "!pip install -q torchsde einops diffusers accelerate xformers==0.0.32.post1 triton==3.4 sageattention\n",
        "clear_output()\n",
        "!pip install av spandrel albumentations onnx opencv-python color-matcher segment_anything ultralytics onnxruntime\n",
        "clear_output()\n",
        "!pip install onnxruntime-gpu -y\n",
        "clear_output()\n",
        "!git clone --branch ComfyUI_v0.3.47 https://github.com/Isi-dev/ComfyUI\n",
        "clear_output()\n",
        "%cd /content/ComfyUI/custom_nodes\n",
        "# !git clone --branch forHidream  https://github.com/Isi-dev/ComfyUI_GGUF.git\n",
        "# clear_output()\n",
        "# !git clone --branch kjnv1.1.3 https://github.com/Isi-dev/ComfyUI_KJNodes.git\n",
        "# clear_output()\n",
        "!git clone https://github.com/Isi-dev/ComfyUI_WanVideoWrapper\n",
        "!git clone https://github.com/Isi-dev/audio_separation_nodes_comfyui\n",
        "# %cd /content/ComfyUI/custom_nodes/ComfyUI_GGUF\n",
        "# !pip install -r requirements.txt\n",
        "clear_output()\n",
        "# %cd /content/ComfyUI/custom_nodes/ComfyUI_KJNodes\n",
        "# !pip install -r requirements.txt\n",
        "%cd /content/ComfyUI/custom_nodes/ComfyUI_WanVideoWrapper\n",
        "!pip install -r requirements.txt\n",
        "%cd /content/ComfyUI/custom_nodes/audio_separation_nodes_comfyui\n",
        "!pip install -r requirements.txt\n",
        "clear_output()\n",
        "\n",
        "\n",
        "%cd /content/ComfyUI\n",
        "!apt -y install -qq aria2 ffmpeg\n",
        "clear_output()\n",
        "\n",
        "\n",
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import gc\n",
        "import sys\n",
        "import random\n",
        "import imageio\n",
        "import subprocess\n",
        "import shutil\n",
        "from google.colab import files\n",
        "from IPython.display import display, HTML, Image as IPImage\n",
        "sys.path.insert(0, '/content/ComfyUI')\n",
        "\n",
        "from comfy import model_management\n",
        "\n",
        "from nodes import (\n",
        "    CheckpointLoaderSimple,\n",
        "    CLIPLoader,\n",
        "    CLIPTextEncode,\n",
        "    VAEDecode,\n",
        "    VAELoader,\n",
        "    KSampler,\n",
        "    KSamplerAdvanced,\n",
        "    UNETLoader,\n",
        "    LoadImage,\n",
        "    SaveImage,\n",
        "    CLIPVisionLoader,\n",
        "    CLIPVisionEncode,\n",
        "    LoraLoaderModelOnly,\n",
        "    ImageScale\n",
        ")\n",
        "\n",
        "# from custom_nodes.ComfyUI_GGUF.nodes import UnetLoaderGGUF\n",
        "# from custom_nodes.ComfyUI_KJNodes.nodes.model_optimization_nodes import (\n",
        "#     WanVideoTeaCacheKJ,\n",
        "#     PathchSageAttentionKJ,\n",
        "#     WanVideoNAG,\n",
        "#     SkipLayerGuidanceWanVideo\n",
        "# )\n",
        "\n",
        "from custom_nodes.ComfyUI_WanVideoWrapper.multitalk.nodes import (\n",
        "    MultiTalkModelLoader,\n",
        "    MultiTalkWav2VecEmbeds,\n",
        "    WanVideoImageToVideoMultiTalk\n",
        ")\n",
        "\n",
        "from custom_nodes.ComfyUI_WanVideoWrapper.nodes import (\n",
        "    WanVideoSampler,\n",
        "    WanVideoContextOptions,\n",
        "    WanVideoTextEmbedBridge,\n",
        "    WanVideoDecode,\n",
        "    WanVideoClipVisionEncode\n",
        ")\n",
        "\n",
        "from custom_nodes.ComfyUI_WanVideoWrapper.nodes_model_loading import (\n",
        "    WanVideoModelLoader,\n",
        "    WanVideoVAELoader,\n",
        "    WanVideoLoraSelect,\n",
        "    WanVideoBlockSwap\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "from custom_nodes.ComfyUI_WanVideoWrapper.fantasytalking.nodes import DownloadAndLoadWav2VecModel\n",
        "\n",
        "from comfy_extras.nodes_audio import LoadAudio\n",
        "from custom_nodes.audio_separation_nodes_comfyui.src.separation import AudioSeparation\n",
        "from custom_nodes.audio_separation_nodes_comfyui.src.crop import AudioCrop\n",
        "\n",
        "# from comfy_extras.nodes_model_advanced import ModelSamplingSD3\n",
        "from comfy_extras.nodes_images import SaveAnimatedWEBP\n",
        "from comfy_extras.nodes_video import SaveWEBM\n",
        "# from comfy_extras.nodes_wan import WanImageToVideo\n",
        "# from comfy_extras.nodes_wan import WanFirstLastFrameToVideo\n",
        "# from comfy_extras.nodes_upscale_model import UpscaleModelLoader\n",
        "\n",
        "\n",
        "def download_with_aria2c(link, folder=\"/content/ComfyUI/models/loras\"):\n",
        "    import os\n",
        "\n",
        "    filename = link.split(\"/\")[-1]\n",
        "    command = f\"aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {link} -d {folder} -o {filename}\"\n",
        "\n",
        "    print(\"Executing download command:\")\n",
        "    print(command)\n",
        "\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "    get_ipython().system(command)\n",
        "\n",
        "    return filename\n",
        "\n",
        "\n",
        "\n",
        "def download_civitai_model(civitai_link, civitai_token, folder=\"/content/ComfyUI/models/loras\"):\n",
        "    import os\n",
        "    import time\n",
        "\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        model_id = civitai_link.split(\"/models/\")[1].split(\"?\")[0]\n",
        "    except IndexError:\n",
        "        raise ValueError(\"Invalid Civitai URL format. Please use a link like: https://civitai.com/api/download/models/1523247?...\")\n",
        "\n",
        "    civitai_url = f\"https://civitai.com/api/download/models/{model_id}?type=Model&format=SafeTensor\"\n",
        "    if civitai_token:\n",
        "        civitai_url += f\"&token={civitai_token}\"\n",
        "\n",
        "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"model_{timestamp}.safetensors\"\n",
        "\n",
        "    full_path = os.path.join(folder, filename)\n",
        "\n",
        "    download_command = f\"wget --max-redirect=10 --show-progress \\\"{civitai_url}\\\" -O \\\"{full_path}\\\"\"\n",
        "    print(\"Downloading from Civitai...\")\n",
        "\n",
        "    os.system(download_command)\n",
        "\n",
        "    local_path = os.path.join(folder, filename)\n",
        "    if os.path.exists(local_path) and os.path.getsize(local_path) > 0:\n",
        "        print(f\"LoRA downloaded successfully: {local_path}\")\n",
        "    else:\n",
        "        print(f\"❌ LoRA download failed or file is empty: {local_path}\")\n",
        "\n",
        "    return filename\n",
        "\n",
        "def download_lora(link, folder=\"/content/ComfyUI/models/loras\", civitai_token=None):\n",
        "    \"\"\"\n",
        "    Download a model file, automatically detecting if it's a Civitai link or huggingface download.\n",
        "\n",
        "    Args:\n",
        "        link: The download URL (either huggingface or Civitai)\n",
        "        folder: Destination folder for the download\n",
        "        civitai_token: Optional token for Civitai downloads (required if link is from Civitai)\n",
        "\n",
        "    Returns:\n",
        "        The filename of the downloaded model\n",
        "    \"\"\"\n",
        "    if \"civitai.com\" in link.lower():\n",
        "        if not civitai_token:\n",
        "            raise ValueError(\"Civitai token is required for Civitai downloads\")\n",
        "        return download_civitai_model(link, civitai_token, folder)\n",
        "    else:\n",
        "        return download_with_aria2c(link, folder)\n",
        "\n",
        "\n",
        "\n",
        "def model_download(url: str, dest_dir: str, filename: str = None, silent: bool = True) -> bool:\n",
        "    \"\"\"\n",
        "    Colab-optimized download with aria2c\n",
        "\n",
        "    Args:\n",
        "        url: Download URL\n",
        "        dest_dir: Target directory (will be created if needed)\n",
        "        filename: Optional output filename (defaults to URL filename)\n",
        "        silent: If True, suppresses all output (except errors)\n",
        "\n",
        "    Returns:\n",
        "        bool: True if successful, False if failed\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create destination directory\n",
        "        Path(dest_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Set filename if not specified\n",
        "        if filename is None:\n",
        "            filename = url.split('/')[-1].split('?')[0]  # Remove URL parameters\n",
        "\n",
        "        # Build command\n",
        "        cmd = [\n",
        "            'aria2c',\n",
        "            '--console-log-level=error',\n",
        "            '-c', '-x', '16', '-s', '16', '-k', '1M',\n",
        "            '-d', dest_dir,\n",
        "            '-o', filename,\n",
        "            url\n",
        "        ]\n",
        "\n",
        "        # Add silent flags if requested\n",
        "        if silent:\n",
        "            cmd.extend(['--summary-interval=0', '--quiet'])\n",
        "            print(f\"Downloading {filename}...\", end=' ', flush=True)\n",
        "\n",
        "        # Run download\n",
        "        result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
        "\n",
        "        if silent:\n",
        "            print(\"Done!\")\n",
        "        else:\n",
        "            print(f\"Downloaded {filename} to {dest_dir}\")\n",
        "        return filename\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        error = e.stderr.strip() or \"Unknown error\"\n",
        "        print(f\"\\nError downloading {filename}: {error}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "# model_quant = \"Q4_K_M\" # @param [\"Q4_K_M\", \"Q5_K_M\", \"Q6_K\", \"Q8_0\"]\n",
        "# lightx2v_rank = \"128\" # @param [\"32\", \"64\", \"128\"]\n",
        "lightx2v_rank = \"32\"\n",
        "\n",
        "# use_preferred_wanModels = True # @param {type:\"boolean\"}\n",
        "use_preferred_wanModels = False\n",
        "# high_noise_model_download_url = \"https://huggingface.co/bullerwins/Wan2.2-I2V-A14B-GGUF/resolve/main/wan2.2_i2v_high_noise_14B_Q4_K_M.gguf\"# @param {\"type\":\"string\"}\n",
        "# low_noise_model_download_url = \"https://huggingface.co/bullerwins/Wan2.2-I2V-A14B-GGUF/resolve/main/wan2.2_i2v_low_noise_14B_Q4_K_M.gguf\"# @param {\"type\":\"string\"}\n",
        "\n",
        "# low_noise_model_download_url = \"https://huggingface.co/Isi99999/Wan2.1BasedModels/resolve/main/wan2.1-i2v-14b-480p-Q4_K_M.gguf\"\n",
        "\n",
        "# use_preferred_speedup_LoRAs = True # @param {type:\"boolean\"}\n",
        "use_preferred_speedup_LoRAs = False\n",
        "# high_noise_speed_LoRA_download_url = \"https://huggingface.co/Kijai/WanVideo_comfy/resolve/main/Wan22-Lightning/Wan2.2-Lightning_I2V-A14B-4steps-lora_HIGH_fp16.safetensors\"# @param {\"type\":\"string\"}\n",
        "# low_noise_speed_LoRA_download_url = \"https://huggingface.co/Kijai/WanVideo_comfy/resolve/main/Wan22-Lightning/Wan2.2-Lightning_I2V-A14B-4steps-lora_LOW_fp16.safetensors\"# @param {\"type\":\"string\"}\n",
        "\n",
        "wan21_model_download_url = \"https://huggingface.co/city96/Wan2.1-I2V-14B-480P-gguf/resolve/main/wan2.1-i2v-14b-480p-Q4_K_M.gguf\"# @param {\"type\":\"string\"}\n",
        "\n",
        "speed_LoRA_download_url = \"https://huggingface.co/Kijai/WanVideo_comfy/resolve/main/Lightx2v/lightx2v_I2V_14B_480p_cfg_step_distill_rank64_bf16.safetensors\" # @param {\"type\":\"string\"}\n",
        "\n",
        "infiniteTalk_url = \"https://huggingface.co/Kijai/WanVideo_comfy/resolve/main/InfiniteTalk/Wan2_1-InfiniteTalk-Multi_fp16.safetensors\" # @param {\"type\":\"string\"}\n",
        "\n",
        "use_multiTalk_instead = True # @param {type:\"boolean\"}\n",
        "\n",
        "multiTalk_url = \"https://huggingface.co/Kijai/WanVideo_comfy/resolve/main/WanVideo_2_1_Multitalk_14B_fp8_e4m3fn.safetensors\" # @param {\"type\":\"string\"}\n",
        "\n",
        "download_loRA_1 = False\n",
        "lora_1_download_url = \"Put your loRA here\"\n",
        "download_loRA_2 = False\n",
        "lora_2_download_url = \"Put your loRA here\"\n",
        "\n",
        "download_loRA_3 = False\n",
        "lora_3_download_url = \"https://huggingface.co/Remade-AI/Rotate/resolve/main/rotate_20_epochs.safetensors\"\n",
        "\n",
        "token_if_civitai_url = \"Put your civitai token here\"\n",
        "\n",
        "lora_1 = None\n",
        "if download_loRA_1:\n",
        "    lora_1 = download_lora(lora_1_download_url, civitai_token=token_if_civitai_url)\n",
        "# Validate loRA file extension\n",
        "valid_extensions = {'.safetensors', '.ckpt', '.pt', '.pth', '.sft'}\n",
        "if lora_1:\n",
        "    if not any(lora_1.lower().endswith(ext) for ext in valid_extensions):\n",
        "        print(f\"❌ Invalid LoRA format: {lora_1}\")\n",
        "        lora_1 = None\n",
        "    else:\n",
        "        clear_output()\n",
        "        print(\"loRA 1 downloaded succesfully!\")\n",
        "\n",
        "lora_2 = None\n",
        "if download_loRA_2:\n",
        "    lora_2 = download_lora(lora_2_download_url, civitai_token=token_if_civitai_url)\n",
        "if lora_2:\n",
        "    if not any(lora_2.lower().endswith(ext) for ext in valid_extensions):\n",
        "        print(f\"❌ Invalid LoRA format: {lora_2}\")\n",
        "        lora_2 = None\n",
        "    else:\n",
        "        clear_output()\n",
        "        print(\"loRA 2 downloaded succesfully!\")\n",
        "\n",
        "lora_3 = None\n",
        "if download_loRA_3:\n",
        "    lora_3 = download_lora(lora_3_download_url, civitai_token=token_if_civitai_url)\n",
        "if lora_3:\n",
        "    if not any(lora_3.lower().endswith(ext) for ext in valid_extensions):\n",
        "        print(f\"❌ Invalid LoRA format: {lora_3}\")\n",
        "        lora_3 = None\n",
        "    else:\n",
        "        clear_output()\n",
        "        print(\"loRA 3 downloaded succesfully!\")\n",
        "\n",
        "\n",
        "# if use_preferred_wanModels:\n",
        "#     dit_model = model_download(high_noise_model_download_url, \"/content/ComfyUI/models/diffusion_models\")\n",
        "#     dit_model2 = model_download(low_noise_model_download_url, \"/content/ComfyUI/models/diffusion_models\")\n",
        "\n",
        "# else:\n",
        "#     # if model_quant == \"Q4_K_M\":\n",
        "#     #     dit_model = model_download(\"https://huggingface.co/Isi99999/Wan2.2BasedModels/resolve/main/wan2.2_i2v_high_noise_14B_Q4_K_M.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
        "#     #     dit_model2 = model_download(\"https://huggingface.co/Isi99999/Wan2.2BasedModels/resolve/main/wan2.2_i2v_low_noise_14B_Q4_K_M.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
        "#     # elif model_quant == \"Q5_K_M\":\n",
        "#     #     dit_model = model_download(\"https://huggingface.co/Isi99999/Wan2.2BasedModels/resolve/main/wan2.2_i2v_high_noise_14B_Q5_K_M.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
        "#     #     dit_model2 = model_download(\"https://huggingface.co/Isi99999/Wan2.2BasedModels/resolve/main/wan2.2_i2v_low_noise_14B_Q5_K_M.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
        "#     # elif model_quant == \"Q6_K\":\n",
        "#     #     dit_model = model_download(\"https://huggingface.co/Isi99999/Wan2.2BasedModels/resolve/main/wan2.2_i2v_high_noise_14B_Q6_K.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
        "#     #     dit_model2 = model_download(\"https://huggingface.co/Isi99999/Wan2.2BasedModels/resolve/main/wan2.2_i2v_low_noise_14B_Q6_K.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
        "#     # else:\n",
        "#     #     dit_model = model_download(\"https://huggingface.co/Isi99999/Wan2.2BasedModels/resolve/main/wan2.2_i2v_high_noise_14B_Q8_0.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
        "#     #     dit_model2 = model_download(\"https://huggingface.co/Isi99999/Wan2.2BasedModels/resolve/main/wan2.2_i2v_low_noise_14B_Q8_0.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
        "\n",
        "#     if model_quant == \"Q4_K_M\":\n",
        "#         dit_model = model_download(\"https://huggingface.co/Isi99999/Wan2.1BasedModels/resolve/main/wan2.1-i2v-14b-480p-Q4_K_M.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
        "#     elif model_quant == \"Q5_K_M\":\n",
        "#         dit_model = model_download(\"https://huggingface.co/Isi99999/Wan2.1BasedModels/resolve/main/wan2.1-i2v-14b-480p-Q5_K_M.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
        "#     elif model_quant == \"Q6_K\":\n",
        "#         dit_model = model_download(\"https://huggingface.co/Isi99999/Wan2.1BasedModels/resolve/main/wan2.1-i2v-14b-480p-Q6_K.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
        "#     elif model_quant == \"Q4_0\":\n",
        "#         dit_model = model_download(\"https://huggingface.co/city96/Wan2.1-I2V-14B-480P-gguf/resolve/main/wan2.1-i2v-14b-480p-Q4_0.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
        "#     else:\n",
        "#         dit_model = model_download(\"https://huggingface.co/Isi99999/Wan2.1BasedModels/resolve/main/wan2.1-i2v-14b-480p-Q8_0.gguf\", \"/content/ComfyUI/models/diffusion_models\")\n",
        "\n",
        "\n",
        "\n",
        "clear_output()\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Isi99999/Wan_Extras/resolve/main/umt5_xxl_fp8_e4m3fn_scaled.safetensors -d /content/ComfyUI/models/text_encoders -o umt5_xxl_fp8_e4m3fn_scaled.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Isi99999/Wan_Extras/resolve/main/wan_2.1_vae.safetensors -d /content/ComfyUI/models/vae -o wan_2.1_vae.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Isi99999/Wan_Extras/resolve/main/clip_vision_h.safetensors -d /content/ComfyUI/models/clip_vision -o clip_vision_h.safetensors\n",
        "clear_output()\n",
        "\n",
        "dit_model = model_download(wan21_model_download_url, \"/content/ComfyUI/models/diffusion_models\")\n",
        "\n",
        "lightx2v_lora_lowNoise = model_download(speed_LoRA_download_url, \"/content/ComfyUI/models/loras\")\n",
        "\n",
        "# if use_preferred_speedup_LoRAs:\n",
        "#     lightx2v_lora = model_download(high_noise_speed_LoRA_download_url, \"/content/ComfyUI/models/loras\")\n",
        "#     lightx2v_lora_lowNoise = model_download(low_noise_speed_LoRA_download_url, \"/content/ComfyUI/models/loras\")\n",
        "# else:\n",
        "#     if lightx2v_rank == \"32\":\n",
        "#         lightx2v_lora_lowNoise = model_download(\"https://huggingface.co/Isi99999/Wan2.1BasedModels/resolve/main/lightx2v_I2V_14B_480p_cfg_step_distill_rank32_bf16.safetensors\", \"/content/ComfyUI/models/loras\")\n",
        "#         # lightx2v_lora = model_download(\"https://huggingface.co/Isi99999/Wan2.1BasedModels/resolve/main/lightx2v_T2V_14B_cfg_step_distill_v2_lora_rank32_bf16.safetensors\", \"/content/ComfyUI/models/loras\")\n",
        "#     elif lightx2v_rank == \"64\":\n",
        "#         lightx2v_lora_lowNoise = model_download(\"https://huggingface.co/Isi99999/Wan2.1BasedModels/resolve/main/lightx2v_T2V_14B_cfg_step_distill_v2_lora_rank64_bf16.safetensors\", \"/content/ComfyUI/models/loras\")\n",
        "#     else:\n",
        "#         lightx2v_lora_lowNoise = model_download(\"https://huggingface.co/Isi99999/Wan2.1BasedModels/resolve/main/lightx2v_T2V_14B_cfg_step_distill_v2_lora_rank128_bf16.safetensors\", \"/content/ComfyUI/models/loras\")\n",
        "\n",
        "\n",
        "if use_multiTalk_instead:\n",
        "    multitalkModel  = model_download(multiTalk_url, \"/content/ComfyUI/models/diffusion_models\")\n",
        "else:\n",
        "    multitalkModel  = model_download(infiniteTalk_url, \"/content/ComfyUI/models/diffusion_models\")\n",
        "\n",
        "w2v_model = model_download(\"https://huggingface.co/TencentGameMate/chinese-wav2vec2-base/resolve/main/chinese-wav2vec2-base-fairseq-ckpt.pt\", \"/content/ComfyUI/models/transformers\")\n",
        "# pyt_model = model_download(\"https://huggingface.co/TencentGameMate/chinese-wav2vec2-base/resolve/main/pytorch_model.bin\", \"/content/ComfyUI/models/transformers\")\n",
        "\n",
        "def upload_file():\n",
        "    \"\"\"Handle file upload (image or video) and return paths.\"\"\"\n",
        "    os.makedirs('/content/ComfyUI/input', exist_ok=True)\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    paths = []\n",
        "    for filename in uploaded.keys():\n",
        "        src_path = f'/content/ComfyUI/{filename}'\n",
        "        dest_path = f'/content/ComfyUI/input/{filename}'\n",
        "        shutil.move(src_path, dest_path)\n",
        "        paths.append(dest_path)\n",
        "        print(f\"File saved to: {dest_path}\")\n",
        "\n",
        "    return paths[0] if paths else None\n",
        "\n",
        "\n",
        "def upload_fileAny(target_dir: str = '/content/ComfyUI/input', file_type: str = 'any') -> str:\n",
        "    \"\"\"\n",
        "    Handle file uploads in Colab and store in specified directory\n",
        "\n",
        "    Args:\n",
        "        target_dir: Where to store uploaded files\n",
        "        file_type: Filter for specific file types ('image', 'audio', or 'any')\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the uploaded file, or None if failed\n",
        "    \"\"\"\n",
        "    from google.colab import files\n",
        "    import os\n",
        "    import shutil\n",
        "\n",
        "    # Create target directory if needed\n",
        "    os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "    # Upload file\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if not uploaded:\n",
        "        print(\"No file was uploaded\")\n",
        "        return None\n",
        "\n",
        "    # Get the first uploaded file (we'll handle one file at a time)\n",
        "    filename = next(iter(uploaded.keys()))\n",
        "    src_path = os.path.join('/content/ComfyUI', filename)\n",
        "    dest_path = os.path.join(target_dir, filename)\n",
        "\n",
        "    # Verify file type if requested\n",
        "    if file_type.lower() != 'any':\n",
        "        ext = os.path.splitext(filename)[1].lower()\n",
        "        if file_type.lower() == 'image' and ext not in ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp']:\n",
        "            print(f\"Error: {filename} is not an image file\")\n",
        "            return None\n",
        "        elif file_type.lower() == 'audio' and ext not in ['.mp3', '.wav', '.ogg', '.flac', '.aac', '.m4a']:\n",
        "            print(f\"Error: {filename} is not an audio file\")\n",
        "            return None\n",
        "\n",
        "    try:\n",
        "        shutil.move(src_path, dest_path)\n",
        "        print(f\"File saved to: {dest_path}\")\n",
        "        return dest_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error moving file: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def upload_fileInt():\n",
        "    \"\"\"Handle file upload (image or video) and return paths.\"\"\"\n",
        "    os.makedirs('/content/ComfyUI/output', exist_ok=True)\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    paths = []\n",
        "    for filename in uploaded.keys():\n",
        "        src_path = f'/content/ComfyUI/{filename}'\n",
        "        dest_path = f'/content/ComfyUI/output/{filename}'\n",
        "        shutil.move(src_path, dest_path)\n",
        "        paths.append(dest_path)\n",
        "        print(f\"File saved to: {dest_path}\")\n",
        "\n",
        "    return paths[0] if paths else None\n",
        "\n",
        "def extract_frames(video_path, max_frames=None):\n",
        "    \"\"\"Extract frames from video and return as a batch tensor.\"\"\"\n",
        "    vidcap = cv2.VideoCapture(video_path)\n",
        "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "    frames = []\n",
        "\n",
        "    while True:\n",
        "        success, frame = vidcap.read()\n",
        "        if not success or (max_frames and len(frames) >= max_frames):\n",
        "            break\n",
        "\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frame = torch.from_numpy(frame).float() / 255.0\n",
        "        frames.append(frame)\n",
        "\n",
        "    if not frames:\n",
        "        return None, fps\n",
        "\n",
        "    # Stack frames into a batch tensor: (N, H, W, 3)\n",
        "    batch = torch.stack(frames, dim=0)\n",
        "    # print(f\"Extracted {len(frames)} frames (shape: {batch.shape})\")\n",
        "    return batch, fps\n",
        "\n",
        "\n",
        "def select_every_n_frame_tensor(\n",
        "    frames_tensor: torch.Tensor,\n",
        "    fps: float,\n",
        "    n: int,\n",
        "    skip_first: int = 0,\n",
        "    max_output_frames: int = 0\n",
        "):\n",
        "    if frames_tensor is None or frames_tensor.ndim != 4:\n",
        "        raise ValueError(\"frames_tensor must be a 4D tensor of shape (N, H, W, C)\")\n",
        "    if n < 1:\n",
        "        raise ValueError(\"n must be >= 1\")\n",
        "\n",
        "    total_frames = frames_tensor.shape[0]\n",
        "\n",
        "    if skip_first >= total_frames:\n",
        "        print(\"No frames available after skipping.\")\n",
        "        return None, 0.0\n",
        "\n",
        "    frames_to_use = frames_tensor[skip_first:]\n",
        "\n",
        "    # Select every nth frame\n",
        "    selected_frames = frames_to_use[::n]\n",
        "\n",
        "    # Cap output if needed\n",
        "    if max_output_frames > 0 and selected_frames.shape[0] > max_output_frames:\n",
        "        selected_frames = selected_frames[:max_output_frames]\n",
        "\n",
        "    adjusted_fps = fps / n\n",
        "\n",
        "    if max_output_frames:\n",
        "        print(f\"Frame cap: {max_output_frames} -> Final output: {selected_frames.shape[0]} frames\")\n",
        "    # print(f\"Adjusted FPS: {adjusted_fps:.2f}  -> Final output: {selected_frames.shape[0]} frames\")\n",
        "\n",
        "    return selected_frames, adjusted_fps\n",
        "\n",
        "def swapT(pa, f, s):\n",
        "    if pa == f:\n",
        "        pa = s\n",
        "    return pa\n",
        "\n",
        "\n",
        "\n",
        "def image_width_height(image):\n",
        "    if image.ndim == 4:\n",
        "        _, height, width, _ = image.shape\n",
        "    elif image.ndim == 3:\n",
        "        height, width, _ = image.shape\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported image shape: {image.shape}\")\n",
        "    return width, height\n",
        "\n",
        "def clear_memory():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()\n",
        "    for obj in list(globals().values()):\n",
        "        if torch.is_tensor(obj) or (hasattr(obj, \"data\") and torch.is_tensor(obj.data)):\n",
        "            del obj\n",
        "    gc.collect()\n",
        "\n",
        "# def save_as_mp4(images, filename_prefix, fps, output_dir=\"/content/ComfyUI/output\"):\n",
        "#     os.makedirs(output_dir, exist_ok=True)\n",
        "#     output_path = f\"{output_dir}/{filename_prefix}.mp4\"\n",
        "\n",
        "#     frames = [(img.cpu().numpy() * 255).astype(np.uint8) for img in images]\n",
        "\n",
        "#     with imageio.get_writer(output_path, fps=fps) as writer:\n",
        "#         for frame in frames:\n",
        "#             writer.append_data(frame)\n",
        "\n",
        "#     return output_path\n",
        "\n",
        "def save_as_mp4(images, filename_prefix, fps=25, audio_path=None, output_dir=\"/content/ComfyUI/output\"):\n",
        "    \"\"\"\n",
        "    Save images as MP4 video with optional audio\n",
        "\n",
        "    Args:\n",
        "        images: List of image tensors or numpy arrays\n",
        "        filename_prefix: Output filename without extension\n",
        "        fps: Frames per second\n",
        "        audio_path: Path to audio file (optional)\n",
        "        output_dir: Output directory\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the generated MP4 file\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Convert images to uint8 numpy arrays\n",
        "    frames = [(img.cpu().numpy() * 255).astype(np.uint8) if hasattr(img, 'cpu')\n",
        "             else (img * 255).astype(np.uint8) for img in images]\n",
        "\n",
        "    # Temporary video path without audio\n",
        "    temp_video_path = f\"{output_dir}/{filename_prefix}_temp.mp4\"\n",
        "    final_video_path = f\"{output_dir}/{filename_prefix}.mp4\"\n",
        "\n",
        "    # Save video without audio first\n",
        "    with imageio.get_writer(temp_video_path, fps=fps) as writer:\n",
        "        for frame in frames:\n",
        "            writer.append_data(frame)\n",
        "\n",
        "    # If audio path is provided, merge with video\n",
        "    if audio_path and os.path.exists(audio_path):\n",
        "        try:\n",
        "            # Use ffmpeg to merge audio and video\n",
        "            cmd = [\n",
        "                'ffmpeg',\n",
        "                '-y',  # Overwrite without asking\n",
        "                '-i', temp_video_path,\n",
        "                '-i', audio_path,\n",
        "                '-c:v', 'copy',  # Copy video stream without re-encoding\n",
        "                '-c:a', 'aac',   # Encode audio to AAC\n",
        "                '-shortest',     # Match duration of the shorter input\n",
        "                final_video_path\n",
        "            ]\n",
        "            subprocess.run(cmd, check=True, capture_output=True)\n",
        "\n",
        "            # Remove temporary file\n",
        "            os.remove(temp_video_path)\n",
        "\n",
        "            print(f\"Video with audio saved to: {final_video_path}\")\n",
        "            return final_video_path\n",
        "\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"Error adding audio: {e.stderr.decode()}\")\n",
        "            os.rename(temp_video_path, final_video_path)\n",
        "            return final_video_path\n",
        "    else:\n",
        "        os.rename(temp_video_path, final_video_path)\n",
        "        print(f\"Video saved to: {final_video_path}\")\n",
        "        return final_video_path\n",
        "\n",
        "def save_as_mp4U(images, filename_prefix, fps, output_dir=\"/content/ComfyUI/output\"):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_path = f\"{output_dir}/{filename_prefix}.mp4\"\n",
        "\n",
        "    frames = []\n",
        "    for i, img in enumerate(images):\n",
        "        try:\n",
        "\n",
        "            if isinstance(img, torch.Tensor):\n",
        "                img = img.cpu().numpy()\n",
        "\n",
        "            # print(f\"Frame {i} initial shape: {img.shape}, dtype: {img.dtype}, max: {img.max()}\")  # Debug\n",
        "\n",
        "\n",
        "            if img.max() <= 1.0:\n",
        "                img = (img * 255).astype(np.uint8)\n",
        "            else:\n",
        "                img = img.astype(np.uint8)\n",
        "\n",
        "\n",
        "            if len(img.shape) == 4:  # Batch dimension? (N, C, H, W)\n",
        "                img = img[0]  # Take first image in batch\n",
        "\n",
        "            if len(img.shape) == 3:\n",
        "                if img.shape[0] in (1, 3, 4):  # CHW format\n",
        "                    img = np.transpose(img, (1, 2, 0))\n",
        "                elif img.shape[2] > 4:  # Too many channels\n",
        "                    img = img[:, :, :3]\n",
        "            elif len(img.shape) == 2:\n",
        "                img = np.expand_dims(img, axis=-1)\n",
        "\n",
        "            # print(f\"Frame {i} processed shape: {img.shape}\")  # Debug\n",
        "\n",
        "            # Final validation\n",
        "            if len(img.shape) != 3 or img.shape[2] not in (1, 3, 4):\n",
        "                raise ValueError(f\"Invalid frame shape after processing: {img.shape}\")\n",
        "\n",
        "            frames.append(img)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing frame {i}: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    try:\n",
        "        with imageio.get_writer(output_path, fps=fps) as writer:\n",
        "            for i, frame in enumerate(frames):\n",
        "                # print(f\"Writing frame {i} with shape: {frame.shape}\")  # Debug\n",
        "                writer.append_data(frame)\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing video: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "    return output_path\n",
        "\n",
        "def save_as_webp(images, filename_prefix, fps, quality=90, lossless=False, method=4, output_dir=\"/content/ComfyUI/output\"):\n",
        "    \"\"\"Save images as animated WEBP using imageio.\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_path = f\"{output_dir}/{filename_prefix}.webp\"\n",
        "\n",
        "\n",
        "    frames = [(img.cpu().numpy() * 255).astype(np.uint8) for img in images]\n",
        "\n",
        "\n",
        "    kwargs = {\n",
        "        'fps': int(fps),\n",
        "        'quality': int(quality),\n",
        "        'lossless': bool(lossless),\n",
        "        'method': int(method)\n",
        "    }\n",
        "\n",
        "    with imageio.get_writer(\n",
        "        output_path,\n",
        "        format='WEBP',\n",
        "        mode='I',\n",
        "        **kwargs\n",
        "    ) as writer:\n",
        "        for frame in frames:\n",
        "            writer.append_data(frame)\n",
        "\n",
        "    return output_path\n",
        "\n",
        "def save_as_webm(images, filename_prefix, fps, codec=\"vp9\", quality=32, output_dir=\"/content/ComfyUI/output\"):\n",
        "    \"\"\"Save images as WEBM using imageio.\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_path = f\"{output_dir}/{filename_prefix}.webm\"\n",
        "\n",
        "\n",
        "    frames = [(img.cpu().numpy() * 255).astype(np.uint8) for img in images]\n",
        "\n",
        "\n",
        "    kwargs = {\n",
        "        'fps': int(fps),\n",
        "        'quality': int(quality),\n",
        "        'codec': str(codec),\n",
        "        'output_params': ['-crf', str(int(quality))]\n",
        "    }\n",
        "\n",
        "    with imageio.get_writer(\n",
        "        output_path,\n",
        "        format='FFMPEG',\n",
        "        mode='I',\n",
        "        **kwargs\n",
        "    ) as writer:\n",
        "        for frame in frames:\n",
        "            writer.append_data(frame)\n",
        "\n",
        "    return output_path\n",
        "\n",
        "def save_as_image(image, filename_prefix, output_dir=\"/content/ComfyUI/output\"):\n",
        "    \"\"\"Save single frame as PNG image.\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_path = f\"{output_dir}/{filename_prefix}.png\"\n",
        "\n",
        "    frame = (image.cpu().numpy() * 255).astype(np.uint8)\n",
        "\n",
        "    Image.fromarray(frame).save(output_path)\n",
        "\n",
        "    return output_path\n",
        "\n",
        "def save_as_image2(image, filename_prefix, output_dir=\"/content/ComfyUI/output\"):\n",
        "    \"\"\"Save single frame as PNG image.\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_path = f\"{output_dir}/{filename_prefix}.png\"\n",
        "\n",
        "    if isinstance(image, torch.Tensor):\n",
        "        image = image.cpu().numpy()\n",
        "    if image.ndim == 4:  # Batch dimension\n",
        "        image = image[0]\n",
        "    if image.shape[0] == 3:  # CHW to HWC\n",
        "        image = np.transpose(image, (1, 2, 0))\n",
        "    image = (image * 255).astype(np.uint8)\n",
        "\n",
        "    Image.fromarray(image).save(output_path)\n",
        "    return output_path\n",
        "\n",
        "\n",
        "def upload_image():\n",
        "    \"\"\"Handle image upload in Colab and store in /content/ComfyUI/input/\"\"\"\n",
        "    from google.colab import files\n",
        "    import os\n",
        "    import shutil\n",
        "\n",
        "    os.makedirs('/content/ComfyUI/input', exist_ok=True)\n",
        "\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # Move each uploaded file to ComfyUI input directory\n",
        "    for filename in uploaded.keys():\n",
        "        src_path = f'/content/ComfyUI/{filename}'\n",
        "        dest_path = f'/content/ComfyUI/input/{filename}'\n",
        "\n",
        "        shutil.move(src_path, dest_path)\n",
        "        # print(f\"Image saved to: {dest_path}\")\n",
        "        return dest_path\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "\n",
        "output_path =\"\"\n",
        "\n",
        "# file_uploaded = None\n",
        "# file_uploaded2 = None\n",
        "\n",
        "def generate_video(\n",
        "    image_path: str = None,\n",
        "    image_path2: str = None,\n",
        "    audio_path: str = None,\n",
        "    audio_path2: str = None,\n",
        "    audio_path3: str = None,\n",
        "    audio_path4: str = None,\n",
        "    LoRA_Strength: float = 1.00,\n",
        "    rel_l1_thresh: float = 0.275,\n",
        "    start_percent: float = 0.1,\n",
        "    end_percent: float = 1.0,\n",
        "    positive_prompt: str = \"a cute anime girl with massive fennec ears and a big fluffy tail wearing a maid outfit turning around\",\n",
        "    prompt_assist: str = \"walking to viewers\",\n",
        "    negative_prompt: str = \"色调艳丽，过曝，静态，细节模糊不清，字幕，风格，作品，画作，画面，静止，整体发灰，最差质量，低质量，JPEG压缩残留，丑陋的，残缺的，多余的手指，画得不好的手部，画得不好的脸部，畸形的，毁容的，形态畸形的肢体，手指融合，静止不动的画面，杂乱的背景，三条腿，背景人很多，倒着走\",\n",
        "    width: int = 832,\n",
        "    height: int = 480,\n",
        "    custom_audio_duration: int = 30,\n",
        "    seed: int = 82628696717253,\n",
        "    steps: int = 20,\n",
        "    cfg_scale: float = 1.0,\n",
        "    sampler_name: str = \"uni_pc\",\n",
        "    scheduler: str = \"simple\",\n",
        "    # frames: int = 33,\n",
        "    fps: int = 16,\n",
        "    output_format: str = \"mp4\",\n",
        "    overwrite: bool = False,\n",
        "    use_lora: bool = True,\n",
        "    use_lora2: bool = True,\n",
        "    LoRA_Strength2: float = 1.00,\n",
        "    use_lora3: bool = True,\n",
        "    LoRA_Strength3: float = 1.00,\n",
        "    use_lightx2v: bool = False,\n",
        "    lightx2v_Strength: float = 0.80,\n",
        "    lightx2v_steps: int = 4,\n",
        "    use_pusa: bool = False,\n",
        "    pusa_Strength: float = 1.2,\n",
        "    pusa_steps: int = 6,\n",
        "    use_sage_attention: bool = True,\n",
        "    enable_flow_shift: bool = True,\n",
        "    shift: float = 8.0,\n",
        "    enable_flow_shift2: bool = True,\n",
        "    shift2: float = 8.0,\n",
        "    end_step1: int = 10,\n",
        "    use_one_model: bool = True,\n",
        "    use_block_swap: bool = True,\n",
        "    blocks_to_swap: int = 20\n",
        "\n",
        "):\n",
        "\n",
        "    with torch.inference_mode():\n",
        "\n",
        "        # Initialize nodes\n",
        "        # unet_loader = UnetLoaderGGUF()\n",
        "        load_audio = LoadAudio()\n",
        "        audio_separation = AudioSeparation()\n",
        "        audio_crop = AudioCrop()\n",
        "        load_wav2vec = DownloadAndLoadWav2VecModel()\n",
        "        wan_model_loader = WanVideoModelLoader()\n",
        "        wan_lora_select = WanVideoLoraSelect()\n",
        "        wan_vae_loader = WanVideoVAELoader()\n",
        "        wan_vae_decoder = WanVideoDecode()\n",
        "        wan_text_embed_bridge = WanVideoTextEmbedBridge()\n",
        "        wan_clip_vision = WanVideoClipVisionEncode()\n",
        "        multitalk_loader = MultiTalkModelLoader()\n",
        "        multitalk_wav2vec = MultiTalkWav2VecEmbeds()\n",
        "        multitalk_img2vid = WanVideoImageToVideoMultiTalk()\n",
        "        wan_sampler = WanVideoSampler()\n",
        "        wan_context_options = WanVideoContextOptions()\n",
        "        block_swapper = WanVideoBlockSwap()\n",
        "\n",
        "        # pathch_sage_attention = PathchSageAttentionKJ()\n",
        "        # wan_video_nag = WanVideoNAG()\n",
        "        # teacache = WanVideoTeaCacheKJ()\n",
        "        # model_sampling = ModelSamplingSD3()\n",
        "        clip_loader = CLIPLoader()\n",
        "        clip_encode_positive = CLIPTextEncode()\n",
        "        clip_encode_negative = CLIPTextEncode()\n",
        "        # vae_loader = VAELoader()\n",
        "        clip_vision_loader = CLIPVisionLoader()\n",
        "        # clip_vision_encode = CLIPVisionEncode()\n",
        "        load_image = LoadImage()\n",
        "        # wan_image_to_video = WanFirstLastFrameToVideo()\n",
        "        # ksampler = KSamplerAdvanced()\n",
        "        # vae_decode = VAEDecode()\n",
        "        save_webp = SaveAnimatedWEBP()\n",
        "        save_webm = SaveWEBM()\n",
        "        # pAssLora = LoraLoaderModelOnly()\n",
        "        # load_lora = LoraLoaderModelOnly()\n",
        "        # load_lora2 = LoraLoaderModelOnly()\n",
        "        # load_lora3 = LoraLoaderModelOnly()\n",
        "        load_lightx2v_lora = LoraLoaderModelOnly()\n",
        "        load_pusa_lora = LoraLoaderModelOnly()\n",
        "        image_scaler = ImageScale()\n",
        "\n",
        "        end_step1in = end_step1\n",
        "\n",
        "        print(\"Loading Text_Encoder...\")\n",
        "        clip = clip_loader.load_clip(\"umt5_xxl_fp8_e4m3fn_scaled.safetensors\", \"wan\", \"default\")[0]\n",
        "\n",
        "        positive = clip_encode_positive.encode(clip, positive_prompt)[0]\n",
        "        negative = clip_encode_negative.encode(clip, negative_prompt)[0]\n",
        "\n",
        "        del clip\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        text_embeds = wan_text_embed_bridge.process(positive, negative)[0]\n",
        "\n",
        "        print(\"Loading vision_Encoder...\")\n",
        "        clip_vision = clip_vision_loader.load_clip(\"clip_vision_h.safetensors\")[0]\n",
        "\n",
        "        if image_path is None:\n",
        "            print(\"Please upload an image file to avoid errors:\")\n",
        "            image_path = upload_image()\n",
        "        if image_path is None:\n",
        "            raise ValueError(\"No image uploaded!\")\n",
        "\n",
        "        if image_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            loaded_image = load_image.load_image(image_path)[0]\n",
        "\n",
        "            width_int, height_int = image_width_height(loaded_image)\n",
        "\n",
        "            if height == 0:\n",
        "                height = int(width * height_int / width_int)\n",
        "\n",
        "            print(f\"Image resolution is {width_int}x{height_int}\")\n",
        "            print(f\"Scaling image to {width}x{height}...\")\n",
        "            loaded_image = image_scaler.upscale(\n",
        "                loaded_image,\n",
        "                \"lanczos\",\n",
        "                width,\n",
        "                height,\n",
        "                \"disabled\"\n",
        "            )[0]\n",
        "\n",
        "        else:\n",
        "            framesVid, fpsVid = extract_frames(image_path)\n",
        "            width_int, height_int = image_width_height(framesVid[0])\n",
        "            print(f\"Video resolution is {width_int}x{height_int}\")\n",
        "            print(f\"Scaling video to {width}x{height}...\")\n",
        "            loaded_image = image_scaler.upscale(\n",
        "                framesVid,\n",
        "                \"lanczos\",\n",
        "                width,\n",
        "                height,\n",
        "                \"disabled\"\n",
        "            )[0]\n",
        "\n",
        "\n",
        "        print(\"Processing with clip vision...\")\n",
        "        clip_vision_output = wan_clip_vision.process(\n",
        "            clip_vision=clip_vision,\n",
        "            image_1=loaded_image,\n",
        "            strength_1=1.0,\n",
        "            strength_2=1.0,\n",
        "            force_offload=False,\n",
        "            crop=\"disabled\",\n",
        "            combine_embeds=\"average\",\n",
        "            image_2=None,\n",
        "            negative_image=None,\n",
        "            tiles=0,\n",
        "            ratio=0.5\n",
        "        )[0]\n",
        "\n",
        "\n",
        "        if audio_path is None:\n",
        "            print(\"Please upload an audio file:\")\n",
        "            audio_path = upload_fileAny(file_type='audio')\n",
        "        if audio_path is None:\n",
        "            raise ValueError(\"No audio uploaded!\")\n",
        "\n",
        "        # loaded_audios[\"audio1\"] = load_audio.load(audio_path)[0]\n",
        "        # audio_duration = loaded_audios[\"audio1\"][\"waveform\"].shape[-1] / loaded_audios[\"audio1\"][\"sample_rate\"]\n",
        "\n",
        "\n",
        "        # for i in range(2, 5):\n",
        "        #     audio_path = globals().get(f\"audio_path{i}\")\n",
        "        #     if audio_path and isinstance(audio_path, str):\n",
        "        #         loaded_audios[f\"audio{i}\"] = load_audio.load(audio_path)[0]\n",
        "        #         audio_duration += loaded_audios[f\"audio{i}\"][\"waveform\"].shape[-1] / loaded_audios[f\"audio{i}\"][\"sample_rate\"]\n",
        "        audio_paths = [audio_path, audio_path2, audio_path3, audio_path4]\n",
        "        loaded_audios = []\n",
        "        for path in audio_paths:\n",
        "            if path is not None:\n",
        "                loaded_audios.append(load_audio.load(path)[0])\n",
        "                # print(\"Added audio for combination\")\n",
        "            else:\n",
        "                loaded_audios.append(None)\n",
        "        non_none_audios = [a for a in loaded_audios if a is not None]\n",
        "        if len(non_none_audios) > 2:\n",
        "            # print(\"Combining remaining audios\")\n",
        "            combined_waveform = torch.cat([a[\"waveform\"] for a in non_none_audios], dim=-1)\n",
        "            sample_rate = non_none_audios[0][\"sample_rate\"]\n",
        "            combined_audio = {\"waveform\": combined_waveform, \"sample_rate\": sample_rate}\n",
        "        else:\n",
        "            combined_audio = non_none_audios[0]\n",
        "\n",
        "        audio_duration = combined_audio[\"waveform\"].shape[-1] / combined_audio[\"sample_rate\"]\n",
        "\n",
        "        print(f\"Input audio duration is {audio_duration} seconds, chosen audio duration is: {custom_audio_duration} seconds.\")\n",
        "\n",
        "        if custom_audio_duration < audio_duration:\n",
        "            print(\"Reducing audio length...\")\n",
        "            audio_duration = int(custom_audio_duration)\n",
        "            combined_audio = audio_crop.main(combined_audio, \"00\", f\"{audio_duration}\")[0]\n",
        "\n",
        "        # output_audio_path = \"/content/cropped_audio.wav\"\n",
        "        # waveform = combined_audio[\"waveform\"].squeeze(0)  # [channels, samples]\n",
        "        # torchaudio.save(output_audio_path, waveform, combined_audio[\"sample_rate\"])\n",
        "\n",
        "        if image_path2 is None:\n",
        "            loaded_image2 = None\n",
        "            clip_vision_output2 = None\n",
        "        else:\n",
        "            loaded_image2 = load_image.load_image(image_path2)[0]\n",
        "\n",
        "            width_int, height_int = image_width_height(loaded_image2)\n",
        "\n",
        "            if height == 0:\n",
        "                height = int(width * height_int / width_int)\n",
        "\n",
        "            print(f\"Second Image resolution is {width_int}x{height_int}\")\n",
        "            print(f\"Scaling Second image to {width}x{height}...\")\n",
        "            loaded_image2 = image_scaler.upscale(\n",
        "                loaded_image2,\n",
        "                \"lanczos\",\n",
        "                width,\n",
        "                height,\n",
        "                \"disabled\"\n",
        "            )[0]\n",
        "            # clip_vision_output2 = clip_vision_encode.encode(clip_vision, loaded_image2, \"none\")[0]\n",
        "\n",
        "            # clip_vision_output2 = wan_clip_vision.process(\n",
        "            #     clip_vision=clip_vision,\n",
        "            #     image_1=loaded_image2,\n",
        "            #     strength_1=1.0,\n",
        "            #     strength_2=1.0,\n",
        "            #     force_offload=False,\n",
        "            #     crop=\"disabled\",\n",
        "            #     combine_embeds=\"average\",\n",
        "            #     image_2=None,\n",
        "            #     negative_image=None,\n",
        "            #     tiles=0,\n",
        "            #     ratio=0.5\n",
        "            # )[0]\n",
        "\n",
        "        # del clip_vision\n",
        "        # torch.cuda.empty_cache()\n",
        "        # gc.collect()\n",
        "\n",
        "\n",
        "        # if custom_audio_duration < audio_duration:\n",
        "\n",
        "        #     print(\"reducing audio length...\")\n",
        "        #     audio_duration = int(custom_audio_duration)\n",
        "        #     loaded_audio = audio_crop.main(loaded_audio, \"00\", f\"{audio_duration}\")[0]\n",
        "\n",
        "        #     output_audio_path = \"/content/cropped_audio.wav\"\n",
        "        #     waveform = loaded_audio[\"waveform\"].squeeze(0)  # now [channels, samples]\n",
        "        #     torchaudio.save(output_audio_path, waveform, loaded_audio[\"sample_rate\"])\n",
        "\n",
        "        # else:\n",
        "        #     output_audio_path = audio_path\n",
        "\n",
        "        frames = max(1*fps, int(audio_duration * fps))\n",
        "        if frames % 2 == 0:\n",
        "            frames += 1\n",
        "\n",
        "        print(f\"Audio duration is now {audio_duration} seconds, and frames is: {frames}.\")\n",
        "\n",
        "        # print(\"separating vocals...\")\n",
        "        # separated_vocals = audio_separation.main(loaded_audio,\"linear\", 10.0, 0.1)[3]\n",
        "\n",
        "        print(\"Loading wav2vec...\")\n",
        "        wav2vec_model = load_wav2vec.loadmodel(\"TencentGameMate/chinese-wav2vec2-base\", \"fp16\", \"main_device\")[0]\n",
        "\n",
        "        # print(f\"Loaded audios: {loaded_audios}\")\n",
        "        print(\"Embedding wav2vec...\")\n",
        "        wav2vec_embeds, combined_audio, actual_num_frames = multitalk_wav2vec.process(\n",
        "            wav2vec_model,\n",
        "            True,\n",
        "            fps,\n",
        "            frames,\n",
        "            loaded_audios[0],\n",
        "            1,\n",
        "            1,\n",
        "            \"para\",\n",
        "            loaded_audios[1],\n",
        "            loaded_audios[2],\n",
        "            loaded_audios[3]\n",
        "        )\n",
        "\n",
        "        output_audio_path = \"/content/cropped_audio.wav\"\n",
        "        waveform = combined_audio[\"waveform\"].squeeze(0)  # [channels, samples]\n",
        "        torchaudio.save(output_audio_path, waveform, combined_audio[\"sample_rate\"])\n",
        "\n",
        "        del wav2vec_model\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "\n",
        "        print(\"Loading VAE...\")\n",
        "        vae = wan_vae_loader.loadmodel(\"wan_2.1_vae.safetensors\", \"fp16\")[0]\n",
        "\n",
        "        # print(\"embedding Image...\")\n",
        "        image_embeds = multitalk_img2vid.process(vae, width, height, frames, fps, False, 'mkl', loaded_image, False, clip_vision_output)[0]\n",
        "\n",
        "\n",
        "        # positive_out, negative_out, latent = wan_image_to_video.encode(\n",
        "        #     positive, negative, vae, width, height, frames, 1, loaded_image, loaded_image2, clip_vision_output, clip_vision_output2\n",
        "        # )\n",
        "\n",
        "        usedSteps = steps\n",
        "\n",
        "        if use_multiTalk_instead:\n",
        "            print(\"Loading multiTalk Model...\")\n",
        "        else:\n",
        "            print(\"Loading infiniteTalk Model...\")\n",
        "\n",
        "        multitalk_model = multitalk_loader.loadmodel(multitalkModel, \"fp16\")[0]\n",
        "\n",
        "        if use_lightx2v:\n",
        "            print(\"Loading speed LoRA...\")\n",
        "            wan_speed_lora = wan_lora_select.getlorapath(lightx2v_lora_lowNoise, pusa_Strength, None, {}, None, False, False)[0]\n",
        "            # model = load_pusa_lora.load_lora_model_only(model, lightx2v_lora_lowNoise, pusa_Strength)[0]\n",
        "            usedSteps=lightx2v_steps\n",
        "        end_step1in = -1\n",
        "\n",
        "        clear_output()\n",
        "\n",
        "        print(\"Loading Model...\")\n",
        "\n",
        "        if use_block_swap:\n",
        "            block_swap_args = block_swapper.setargs(\n",
        "                blocks_to_swap=blocks_to_swap,\n",
        "                offload_img_emb=False,\n",
        "                offload_txt_emb=False,\n",
        "                use_non_blocking=True,\n",
        "                vace_blocks_to_swap=0,\n",
        "                prefetch_blocks=0,\n",
        "                block_swap_debug=False\n",
        "            )[0]\n",
        "        else:\n",
        "            block_swap_args=None\n",
        "\n",
        "        model = wan_model_loader.loadmodel(\n",
        "            model=dit_model,\n",
        "            base_precision=\"fp16\",\n",
        "            load_device=\"offload_device\",\n",
        "            quantization=\"disabled\",\n",
        "            compile_args=None,\n",
        "            attention_mode=\"sageattn\",\n",
        "            block_swap_args=block_swap_args,\n",
        "            lora=wan_speed_lora,\n",
        "            vram_management_args=None,\n",
        "            vace_model=None,\n",
        "            fantasytalking_model=None,\n",
        "            multitalk_model=multitalk_model,\n",
        "            fantasyportrait_model=None\n",
        "        )[0]\n",
        "\n",
        "        del multitalk_model\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "        # model = unet_loader.load_unet(dit_model)[0]\n",
        "\n",
        "        # model = wan_video_nag.patch(model, negative, 11.0, 0.25, 2.5)[0]\n",
        "\n",
        "        # if enable_flow_shift:\n",
        "        #     model = model_sampling.patch(model, shift)[0]\n",
        "\n",
        "        # if prompt_assist != \"none\":\n",
        "        #     if prompt_assist == \"walking to viewers\":\n",
        "        #         print(\"Loading walking to camera LoRA...\")\n",
        "        #         model = pAssLora.load_lora_model_only(model, walkingToViewersL, 1)[0]\n",
        "        #     if prompt_assist == \"walking from behind\":\n",
        "        #         print(\"Loading walking from camera LoRA...\")\n",
        "        #         model = pAssLora.load_lora_model_only(model, walkingFromBehindL, 1)[0]\n",
        "        #     if prompt_assist == \"b3ll13-d8nc3r\":\n",
        "        #         print(\"Loading dancing LoRA...\")\n",
        "        #         model = pAssLora.load_lora_model_only(model, dancingL, 1)[0]\n",
        "\n",
        "        # if use_lora and lora_1 is not None:\n",
        "        #     print(\"Loading LoRA...\")\n",
        "        #     model = load_lora.load_lora_model_only(model, lora_1, LoRA_Strength)[0]\n",
        "\n",
        "        # if use_lora2 and lora_2 is not None:\n",
        "        #     print(\"Loading LoRA 2...\")\n",
        "        #     model = load_lora2.load_lora_model_only(model, lora_2, LoRA_Strength2)[0]\n",
        "\n",
        "        # if use_lora3 and lora_3 is not None:\n",
        "        #     print(\"Loading LoRA 3...\")\n",
        "        #     model = load_lora3.load_lora_model_only(model, lora_3, LoRA_Strength3)[0]\n",
        "\n",
        "        # if use_causvid:\n",
        "        #     print(\"Loading causvid LoRA...\")\n",
        "        #     model = load_causvid_lora.load_lora_model_only(model, causvid_lora, causvid_Strength)[0]\n",
        "        #     usedSteps=causvid_steps\n",
        "\n",
        "        # if use_lightx2v:\n",
        "        #     if use_one_model:\n",
        "        #         print(\"Loading speed LoRA...\")\n",
        "        #         model = load_pusa_lora.load_lora_model_only(model, lightx2v_lora_lowNoise, pusa_Strength)[0]\n",
        "        #         usedSteps=lightx2v_steps\n",
        "        #         end_step1in = -1\n",
        "\n",
        "        #     else:\n",
        "        #         print(\"Loading high noise LoRA...\")\n",
        "        #         model = load_lightx2v_lora.load_lora_model_only(model, lightx2v_lora, lightx2v_Strength)[0]\n",
        "        #         usedSteps=lightx2v_steps\n",
        "\n",
        "        # if use_pusa:\n",
        "        #     print(\"Loading pusav1 LoRA...\")\n",
        "        #     model = load_pusa_lora.load_lora_model_only(model, pusa_lora, pusa_Strength)[0]\n",
        "        #     usedSteps=pusa_steps\n",
        "\n",
        "        # if use_sage_attention:\n",
        "        #     model = pathch_sage_attention.patch(model, \"auto\")[0]\n",
        "\n",
        "        # if rel_l1_thresh > 0:\n",
        "        #     print(\"Setting Teacache...\")\n",
        "        #     model = teacache.patch_teacache(model, rel_l1_thresh, start_percent, end_percent, \"main_device\", \"14B\")[0]\n",
        "\n",
        "        clear_output()\n",
        "\n",
        "        if use_one_model:\n",
        "            print(\"Generating video...\")\n",
        "        else:\n",
        "            print(\"Generating video with high noise model...\")\n",
        "        # sampled = ksampler.sample(\n",
        "        #     model=model,\n",
        "        #     add_noise=\"enable\",\n",
        "        #     noise_seed=seed,\n",
        "        #     steps=usedSteps,\n",
        "        #     cfg=cfg_scale,\n",
        "        #     sampler_name=sampler_name,\n",
        "        #     scheduler=scheduler,\n",
        "        #     positive=positive_out,\n",
        "        #     negative=negative_out,\n",
        "        #     latent_image=latent,\n",
        "        #     start_at_step=0,\n",
        "        #     end_at_step=end_step1,\n",
        "        #     return_with_leftover_noise=\"enable\"\n",
        "        # )[0]\n",
        "\n",
        "        # context_options = wan_context_options.process(\n",
        "        #     context_schedule=\"uniform_standard\",\n",
        "        #     context_frames=frames,\n",
        "        #     context_stride=4,\n",
        "        #     context_overlap=16,\n",
        "        #     freenoise=True,\n",
        "        #     verbose=False,\n",
        "        #     image_cond_start_step=6,\n",
        "        #     image_cond_window_count=2,\n",
        "        #     vae=None,\n",
        "        #     fuse_method=\"linear\",\n",
        "        #     reference_latent=None\n",
        "        # )[0]\n",
        "\n",
        "\n",
        "        sampled = wan_sampler.process(\n",
        "            model=model,\n",
        "            image_embeds=image_embeds,\n",
        "            shift=8,\n",
        "            steps=usedSteps,\n",
        "            cfg=cfg_scale,\n",
        "            seed=seed,\n",
        "            scheduler=sampler_name,\n",
        "            riflex_freq_index=0,\n",
        "            text_embeds=text_embeds,\n",
        "            force_offload=True,\n",
        "            samples=None,\n",
        "            feta_args=None,\n",
        "            denoise_strength=1.0,\n",
        "            context_options=None,\n",
        "            cache_args=None,\n",
        "            teacache_args=None,\n",
        "            flowedit_args=None,\n",
        "            batched_cfg=False,\n",
        "            slg_args=None,\n",
        "            rope_function=\"default\",\n",
        "            loop_args=None,\n",
        "            experimental_args=None,\n",
        "            sigmas=None,\n",
        "            unianimate_poses=None,\n",
        "            fantasytalking_embeds=None,\n",
        "            uni3c_embeds=None,\n",
        "            multitalk_embeds=wav2vec_embeds,\n",
        "            freeinit_args=None,\n",
        "            start_step=0,\n",
        "            end_step=end_step1in,\n",
        "            add_noise_to_samples=False\n",
        "        )[0]\n",
        "\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        if use_one_model is False:\n",
        "            multitalk_model = multitalk_loader.loadmodel(multitalkModel, \"fp16\")[0]\n",
        "\n",
        "            print(\"Loading low noise Model...\")\n",
        "            model = wan_model_loader.loadmodel(\n",
        "                model=dit_model2,\n",
        "                base_precision=\"fp16\",\n",
        "                load_device=\"main_device\",\n",
        "                quantization=\"disabled\",\n",
        "                compile_args=None,\n",
        "                attention_mode=\"sageattn\",\n",
        "                block_swap_args=None,\n",
        "                lora=None,\n",
        "                vram_management_args=None,\n",
        "                vace_model=None,\n",
        "                fantasytalking_model=None,\n",
        "                multitalk_model=multitalk_model,\n",
        "                fantasyportrait_model=None\n",
        "            )[0]\n",
        "\n",
        "            del multitalk_model\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            # model = unet_loader.load_unet(dit_model2)[0]\n",
        "\n",
        "\n",
        "\n",
        "            # model = wan_video_nag.patch(model, negative, 11.0, 0.25, 2.5)[0]\n",
        "\n",
        "            # if enable_flow_shift2:\n",
        "            #     model = model_sampling.patch(model, shift2)[0]\n",
        "\n",
        "            # if prompt_assist != \"none\":\n",
        "            #     if prompt_assist == \"walking to viewers\":\n",
        "            #         print(\"Loading walking to camera LoRA...\")\n",
        "            #         model = pAssLora.load_lora_model_only(model, walkingToViewersL, 1)[0]\n",
        "            #     if prompt_assist == \"walking from behind\":\n",
        "            #         print(\"Loading walking from camera LoRA...\")\n",
        "            #         model = pAssLora.load_lora_model_only(model, walkingFromBehindL, 1)[0]\n",
        "            #     if prompt_assist == \"b3ll13-d8nc3r\":\n",
        "            #         print(\"Loading dancing LoRA...\")\n",
        "            #         model = pAssLora.load_lora_model_only(model, dancingL, 1)[0]\n",
        "\n",
        "            # if use_lora and lora_1 is not None:\n",
        "            #     print(\"Loading LoRA...\")\n",
        "            #     model = load_lora.load_lora_model_only(model, lora_1, LoRA_Strength)[0]\n",
        "\n",
        "            # if use_lora2 and lora_2 is not None:\n",
        "            #     print(\"Loading LoRA 2...\")\n",
        "            #     model = load_lora2.load_lora_model_only(model, lora_2, LoRA_Strength2)[0]\n",
        "\n",
        "            # if use_lora3 and lora_3 is not None:\n",
        "            #     print(\"Loading LoRA 3...\")\n",
        "            #     model = load_lora3.load_lora_model_only(model, lora_3, LoRA_Strength3)[0]\n",
        "\n",
        "            # if use_causvid:\n",
        "            #     print(\"Loading causvid LoRA...\")\n",
        "            #     model = load_causvid_lora.load_lora_model_only(model, causvid_lora, causvid_Strength)[0]\n",
        "            #     usedSteps=causvid_steps\n",
        "\n",
        "            # if use_lightx2v:\n",
        "            #     print(\"Loading lightx2v LoRA...\")\n",
        "            #     model = load_lightx2v_lora.load_lora_model_only(model, lightx2v_lora, lightx2v_Strength)[0]\n",
        "            #     usedSteps=lightx2v_steps\n",
        "\n",
        "            if use_pusa:\n",
        "                print(\"Loading low noise LoRA...\")\n",
        "                model = load_pusa_lora.load_lora_model_only(model, lightx2v_lora_lowNoise, pusa_Strength)[0]\n",
        "                usedSteps=lightx2v_steps\n",
        "\n",
        "            # if use_sage_attention:\n",
        "            #     model = pathch_sage_attention.patch(model, \"auto\")[0]\n",
        "\n",
        "            # if rel_l1_thresh > 0:\n",
        "            #     print(\"Setting Teacache...\")\n",
        "            #     model = teacache.patch_teacache(model, rel_l1_thresh, start_percent, end_percent, \"main_device\", \"14B\")[0]\n",
        "\n",
        "            # clear_output()\n",
        "\n",
        "            print(\"Generating video with low noise model...\")\n",
        "            # sampled = ksampler.sample(\n",
        "            #     model=model,\n",
        "            #     add_noise=\"disable\",\n",
        "            #     noise_seed=seed,\n",
        "            #     steps=usedSteps,\n",
        "            #     cfg=cfg_scale,\n",
        "            #     sampler_name=sampler_name,\n",
        "            #     scheduler=scheduler,\n",
        "            #     positive=positive_out,\n",
        "            #     negative=negative_out,\n",
        "            #     latent_image=sampled,\n",
        "            #     start_at_step=end_step1,\n",
        "            #     end_at_step=10000,\n",
        "            #     return_with_leftover_noise=\"disable\"\n",
        "            # )[0]\n",
        "\n",
        "            print(sampled.keys())\n",
        "\n",
        "            sampled = wan_sampler.process(\n",
        "                model=model,\n",
        "                image_embeds=image_embeds,\n",
        "                shift=8,\n",
        "                steps=usedSteps,\n",
        "                cfg=cfg_scale,\n",
        "                seed=seed,\n",
        "                scheduler=sampler_name,\n",
        "                riflex_freq_index=0,\n",
        "                text_embeds=text_embeds,\n",
        "                force_offload=True,\n",
        "                samples=sampled,\n",
        "                feta_args=None,\n",
        "                denoise_strength=1.0,\n",
        "                context_options=None,\n",
        "                cache_args=None,\n",
        "                teacache_args=None,\n",
        "                flowedit_args=None,\n",
        "                batched_cfg=False,\n",
        "                slg_args=None,\n",
        "                rope_function=\"default\",\n",
        "                loop_args=None,\n",
        "                experimental_args=None,\n",
        "                sigmas=None,\n",
        "                unianimate_poses=None,\n",
        "                fantasytalking_embeds=None,\n",
        "                uni3c_embeds=None,\n",
        "                multitalk_embeds=wav2vec_embeds,\n",
        "                freeinit_args=None,\n",
        "                start_step=end_step1,\n",
        "                end_step=-1,\n",
        "                add_noise_to_samples=False\n",
        "            )[0]\n",
        "\n",
        "            del model\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "\n",
        "        try:\n",
        "            print(\"Decoding latents...\")\n",
        "            decoded = wan_vae_decoder.decode(\n",
        "                vae=vae,\n",
        "                samples=sampled,\n",
        "                enable_vae_tiling=False,\n",
        "                tile_x=272,\n",
        "                tile_y=272,\n",
        "                tile_stride_x=144,\n",
        "                tile_stride_y=128,\n",
        "                normalization=\"default\")[0]\n",
        "\n",
        "            # decoded = vae_decode.decode(vae, sampled)[0]\n",
        "\n",
        "            del vae\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            global output_path\n",
        "            import datetime\n",
        "            base_name = \"ComfyUI\"\n",
        "            if not overwrite:\n",
        "                timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "                base_name += f\"_{timestamp}\"\n",
        "            if frames == 1:\n",
        "                print(\"Single frame detected - saving as PNG image...\")\n",
        "                output_path = save_as_image(decoded[0], \"ComfyUI\")\n",
        "                # print(f\"Image saved as PNG: {output_path}\")\n",
        "\n",
        "                display(IPImage(filename=output_path))\n",
        "            else:\n",
        "                if output_format.lower() == \"webm\":\n",
        "                    print(\"Saving as WEBM...\")\n",
        "                    output_path = save_as_webm(\n",
        "                        decoded,\n",
        "                        base_name,\n",
        "                        fps=fps,\n",
        "                        codec=\"vp9\",\n",
        "                        quality=10\n",
        "                    )\n",
        "                elif output_format.lower() == \"mp4\":\n",
        "                    print(\"Saving as MP4...\")\n",
        "                    # output_path1 = save_as_mp4(decoded, base_name, fps)\n",
        "                    output_path = save_as_mp4(\n",
        "                        images=decoded,\n",
        "                        filename_prefix=base_name,\n",
        "                        fps=fps,\n",
        "                        audio_path=output_audio_path\n",
        "                    )\n",
        "\n",
        "                    output_path2 = save_as_mp4(decoded, \"ComfyUI\", fps)\n",
        "                else:\n",
        "                    raise ValueError(f\"Unsupported output format: {output_format}\")\n",
        "\n",
        "                # print(f\"Video saved as {output_format.upper()}: {output_path}\")\n",
        "\n",
        "                display_video(output_path)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during decoding/saving: {str(e)}\")\n",
        "            raise\n",
        "        finally:\n",
        "            clear_memory()\n",
        "\n",
        "\n",
        "\n",
        "def display_video(video_path):\n",
        "    from IPython.display import HTML\n",
        "    from base64 import b64encode\n",
        "\n",
        "    video_data = open(video_path,'rb').read()\n",
        "\n",
        "    # Determine MIME type based on file extension\n",
        "    if video_path.lower().endswith('.mp4'):\n",
        "        mime_type = \"video/mp4\"\n",
        "    elif video_path.lower().endswith('.webm'):\n",
        "        mime_type = \"video/webm\"\n",
        "    elif video_path.lower().endswith('.webp'):\n",
        "        mime_type = \"image/webp\"\n",
        "    else:\n",
        "        mime_type = \"video/mp4\"  # default\n",
        "\n",
        "    data_url = f\"data:{mime_type};base64,\" + b64encode(video_data).decode()\n",
        "\n",
        "    display(HTML(f\"\"\"\n",
        "    <video width=512 controls autoplay loop>\n",
        "        <source src=\"{data_url}\" type=\"{mime_type}\">\n",
        "    </video>\n",
        "    \"\"\"))\n",
        "\n",
        "clear_output()\n",
        "\n",
        "print(\"✅ Environment Setup Complete!\")\n",
        "\n",
        "\n",
        "# @markdown ---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zWqdWqkdNxtM"
      },
      "outputs": [],
      "source": [
        "\n",
        "# @markdown # 💥2. Upload Image (png, jpg, jpeg)/Video\n",
        "file_uploaded = upload_file()\n",
        "display_upload = False # @param {type:\"boolean\"}\n",
        "if display_upload:\n",
        "    if file_uploaded.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "        display(IPImage(filename=file_uploaded))\n",
        "    else:\n",
        "        print(\"If Image, then Image format cannnot be displayed.\")\n",
        "        display_video(file_uploaded)\n",
        "# @markdown ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_sQbJ_p_kPv3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# @markdown # 💥3. Upload Audio\n",
        "audio_uploaded = upload_fileAny(file_type='audio')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown # Upload Audio 2 (Optional)\n",
        "audio_uploaded2 = upload_fileAny(file_type='audio')\n",
        "# @markdown ---"
      ],
      "metadata": {
        "cellView": "form",
        "id": "HRLekaVd1xMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "fcsJjujta1K9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# @markdown # 💥4. Generate Video\n",
        "import time\n",
        "start_time = time.time()\n",
        "# @markdown ### Video Settings\n",
        "# use_image1_as_first_last = False # @param {type:\"boolean\"}\n",
        "# disable_image2 = False # @param {type:\"boolean\"}\n",
        "positive_prompt = \"The woman and man take turns talking to each other\" # @param {\"type\":\"string\"}\n",
        "prompt_assist = \"none\"\n",
        "# prompt_assist = swapT(prompt_assist, \"walking to camera\", \"walking to viewers\")\n",
        "# prompt_assist = swapT(prompt_assist, \"walking from camera\", \"walking from behind\")\n",
        "# prompt_assist = swapT(prompt_assist, \"swaying\", \"b3ll13-d8nc3r\")\n",
        "# positive_prompt = f\"{positive_prompt} {prompt_assist}.\" if prompt_assist != \"none\" else positive_prompt\n",
        "# positive_prompt = f\"{positive_prompt} Turn this image into {prompt_assist} style.\" if prompt_assist != \"none\" else positive_prompt\n",
        "\n",
        "negative_prompt = \"bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards\" # @param {\"type\":\"string\"}\n",
        "width = 400 # @param {\"type\":\"number\"}\n",
        "height = 704 # @param {\"type\":\"number\"}\n",
        "reduce_audio_duration_to = 20 # @param {\"type\":\"number\"}\n",
        "seed = 2335434353 # @param {\"type\":\"integer\"}\n",
        "high_noise_steps = 2\n",
        "steps = 4 # @param {\"type\":\"integer\", \"min\":1, \"max\":50}\n",
        "cfg_scale = 1 # @param {\"type\":\"number\", \"min\":1, \"max\":20}\n",
        "scheduler = \"flowmatch_distill\" # @param [\"flowmatch_distill\",\"uni_pc\", \"uni_pc_bh2\", \"ddim\",\"euler\", \"euler_cfg_pp\", \"euler_ancestral\", \"euler_ancestral_cfg_pp\", \"heun\", \"heunpp2\",\"dpm_2\", \"dpm_2_ancestral\",\"lms\", \"dpm_fast\", \"dpm_adaptive\", \"dpmpp_2s_ancestral\", \"dpmpp_2s_ancestral_cfg_pp\", \"dpmpp_sde\", \"dpmpp_sde_gpu\",\"dpmpp_2m\", \"dpmpp_2m_cfg_pp\", \"dpmpp_2m_sde\", \"dpmpp_2m_sde_gpu\", \"dpmpp_3m_sde\", \"dpmpp_3m_sde_gpu\", \"ddpm\", \"lcm\",\"ipndm\", \"ipndm_v\", \"deis\", \"res_multistep\", \"res_multistep_cfg_pp\", \"res_multistep_ancestral\", \"res_multistep_ancestral_cfg_pp\",\"gradient_estimation\", \"er_sde\", \"seeds_2\", \"seeds_3\"]\n",
        "sampler_name = \"simple\"\n",
        "\n",
        "frames = 81\n",
        "# fps = 16 # @param {\"type\":\"integer\", \"min\":1, \"max\":60}\n",
        "fps = 25\n",
        "# output_format = \"mp4\" # @param [\"mp4\", \"webm\"]\n",
        "output_format = \"mp4\"\n",
        "overwrite_previous_video = False # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown ### Model Configuration\n",
        "use_block_swap = False # @param {type:\"boolean\"}\n",
        "blocks_to_swap = 20 # @param {\"type\":\"number\"}\n",
        "use_sage_attention = True # @param {type:\"boolean\"}\n",
        "# use_sage_attention = True\n",
        "use_flow_shift = True\n",
        "flow_shift = 11\n",
        "flow_shift2 = 11\n",
        "use_one_model = True\n",
        "\n",
        "# use_causvid = False # @param {type:\"boolean\"}\n",
        "# causvid_Strength = 0.8 # @param {\"type\":\"slider\",\"min\":-100,\"max\":100,\"step\":0.01}\n",
        "# causvid_steps = 4 # @param {\"type\":\"integer\", \"min\":1, \"max\":20}\n",
        "use_high_noise_speed_LoRA = True\n",
        "high_noise_speed_LoRA_Strength = 1\n",
        "# lightx2v_steps = 4 # @param {\"type\":\"integer\", \"min\":1, \"max\":20}\n",
        "use_speed_LoRA = True # @param {type:\"boolean\"}\n",
        "speed_LoRA_Strength = 1 # @param {\"type\":\"slider\",\"min\":-10,\"max\":10,\"step\":0.01}\n",
        "# pusav1_steps = 6 # @param {\"type\":\"integer\", \"min\":1, \"max\":20}\n",
        "\n",
        "use_lora = False\n",
        "LoRA_Strength = 1.5\n",
        "use_lora2 = False\n",
        "LoRA_Strength2 = 1.0\n",
        "use_lora3 = False #\n",
        "LoRA_Strength3 = 1.0\n",
        "\n",
        "rel_l1_thresh = 0\n",
        "start_percent = 0.2\n",
        "end_percent = 1.0\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "import random\n",
        "seed = seed if seed != 0 else random.randint(0, 2**32 - 1)\n",
        "print(f\"Using seed: {seed}\")\n",
        "\n",
        "second_image = None\n",
        "\n",
        "audio_driver  = globals().get(\"audio_uploaded\", None)\n",
        "audio_driver2 = globals().get(\"audio_uploaded2\", None)\n",
        "audio_driver3 = globals().get(\"audio_uploaded3\", None)\n",
        "audio_driver4 = globals().get(\"audio_uploaded4\", None)\n",
        "\n",
        "# with torch.inference_mode():\n",
        "generate_video(\n",
        "    image_path=file_uploaded,\n",
        "    image_path2=second_image,\n",
        "    audio_path = audio_driver,\n",
        "    audio_path2 = audio_driver2,\n",
        "    audio_path3 = audio_driver3,\n",
        "    audio_path4 = audio_driver4,\n",
        "    LoRA_Strength=LoRA_Strength,\n",
        "    rel_l1_thresh=rel_l1_thresh,\n",
        "    start_percent=start_percent,\n",
        "    end_percent = end_percent,\n",
        "    positive_prompt=positive_prompt,\n",
        "    prompt_assist=prompt_assist,\n",
        "    negative_prompt=negative_prompt,\n",
        "    width=width,\n",
        "    height=height,\n",
        "    custom_audio_duration=reduce_audio_duration_to,\n",
        "    seed=seed,\n",
        "    steps=steps,\n",
        "    cfg_scale=cfg_scale,\n",
        "    sampler_name=scheduler,\n",
        "    scheduler=sampler_name,\n",
        "    # frames=frames,\n",
        "    fps=fps,\n",
        "    output_format=output_format,\n",
        "    overwrite=overwrite_previous_video,\n",
        "    use_lora = use_lora,\n",
        "    use_lora2=use_lora2,\n",
        "    LoRA_Strength2=LoRA_Strength2,\n",
        "    use_lora3=use_lora3,\n",
        "    LoRA_Strength3=LoRA_Strength3,\n",
        "    # use_causvid=use_causvid,\n",
        "    # causvid_Strength=causvid_Strength,\n",
        "    # causvid_steps=causvid_steps,\n",
        "    use_lightx2v=use_high_noise_speed_LoRA,\n",
        "    lightx2v_Strength=high_noise_speed_LoRA_Strength,\n",
        "    lightx2v_steps=steps,\n",
        "    use_pusa=use_speed_LoRA,\n",
        "    pusa_Strength=speed_LoRA_Strength,\n",
        "    pusa_steps=steps,\n",
        "    use_sage_attention = use_sage_attention,\n",
        "    enable_flow_shift = use_flow_shift,\n",
        "    shift = flow_shift,\n",
        "    enable_flow_shift2 = use_flow_shift,\n",
        "    shift2 = flow_shift2,\n",
        "    end_step1 = high_noise_steps,\n",
        "    use_one_model = use_one_model,\n",
        "    use_block_swap = use_block_swap,\n",
        "    blocks_to_swap = blocks_to_swap\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "duration = end_time - start_time\n",
        "mins, secs = divmod(duration, 60)\n",
        "print(f\"Seed: {seed}\")\n",
        "# print(f\"prompt: {positive_prompt}\")\n",
        "print(f\"✅ Generation completed in {int(mins)} min {secs:.2f} sec\")\n",
        "\n",
        "clear_memory()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}